commit,file,added_code,deleted_code,hash_clean,cve_id,hash,repo_url,rel_type,score,extraction_status,text_norm,cluster_kmeans,cluster_hac
anything-llm_3c88aec034934bcbad30c5ef1cab62cbbdb98e64,cloud-deployments/digitalocean/terraform/user_data.tp1,"#!/bin/bash
# check output of userdata script with sudo tail -f /var/log/cloud-init-output.log

sudo apt-get update
sudo apt-get install -y docker.io
sudo usermod -a -G docker ubuntu

curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

sudo systemctl enable docker
sudo systemctl start docker

sudo apt-get install -y git

git clone https://github.com/Mintplex-Labs/anything-llm.git /home/anything-llm
cd /home/anything-llm/docker

cat >> .env << END
${env_content}
UID=""1000""
GID=""1000""
NO_DEBUG=""true""
END

echo ""Set .env file""

cd ../frontend
sudo rm -rf .env.production

sudo cat >> .env.production << END
GENERATE_SOURCEMAP=true
VITE_API_BASE=""/api""
END

echo ""Set .env.production file""

cd ../docker
sudo docker-compose up -d --build
echo ""Container ID: $(sudo docker ps --latest --quiet)""

sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) mkdir -p /app/server/storage /app/server/storage/documents /app/server/storage/vector-cache /app/server/storage/lancedb
echo ""Placeholder folders in storage created.""

sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) touch /app/server/storage/anythingllm.db
echo ""SQLite DB placeholder set.""

sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) chown -R anythingllm:anythingllm /app/collector /app/server
echo ""File permissions corrected.""

export ONLINE=$(curl -Is http://localhost:3001/api/ping | head -n 1|cut -d$' ' -f2)
echo ""Health check: $ONLINE""

if [ ""$ONLINE"" = 200 ]; then
  echo ""Running migrations..."" && curl -Is http://localhost:3001/api/migrate | head -n 1 | cut -d$' ' -f2
fi

echo ""Setup complete! AnythingLLM instance is now online!""",,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,CVE-2023-4897,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"  VAR VAR-VAR VAR VAR VAR-VAR VAR -VAR VAR.VAR VAR VAR -VAR -VAR VAR VAR VAR -VAR VAR: VAR VAR +VAR /VAR/VAR/VAR/VAR-VAR VAR VAR -VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR-VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR-VAR VAR -VAR VAR VAR VAR VAR: VAR /VAR/VAR-VAR/VAR VAR >> .VAR << VAR ${VAR} VAR=""1000"" VAR=""1000"" VAR=""VAR"" VAR VAR ""VAR .VAR VAR"" VAR ../VAR VAR VAR -VAR .VAR.VAR VAR VAR >> .VAR.VAR << VAR VAR=VAR VAR=""/VAR"" VAR VAR ""VAR .VAR.VAR VAR"" VAR ../VAR VAR VAR-VAR VAR -VAR --VAR VAR ""VAR VAR: $(VAR VAR VAR --VAR --VAR)"" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR /VAR/VAR/VAR /VAR/VAR/VAR/VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR/VAR VAR ""VAR VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR /VAR/VAR/VAR/VAR.VAR VAR ""VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR VAR:VAR /VAR/VAR /VAR/VAR VAR ""VAR VAR VAR."" VAR VAR=$(VAR -VAR VAR: VAR ""VAR VAR: $VAR"" VAR [ ""$VAR"" = 200 ]; VAR VAR ""VAR VAR..."" && VAR -VAR VAR: VAR VAR ""VAR VAR! VAR VAR VAR VAR VAR!"" ",14,9
anything-llm_3c88aec034934bcbad30c5ef1cab62cbbdb98e64,cloud-deployments/gcp/deployment/gcp_deploy_anything_llm.yaml,"resources:
  - name: anything-llm-instance
    type: compute.v1.instance
    properties:
      zone: us-central1-a
      machineType: zones/us-central1-a/machineTypes/n1-standard-1
      disks:
        - deviceName: boot
          type: PERSISTENT
          boot: true
          autoDelete: true
          initializeParams:
            sourceImage: projects/ubuntu-os-cloud/global/images/family/ubuntu-2004-lts
            diskSizeGb: 10
      networkInterfaces:
        - network: global/networks/default
          accessConfigs:
            - name: External NAT
              type: ONE_TO_ONE_NAT
      metadata:
        items:
          - key: startup-script
            value: |
              #!/bin/bash
              # check output of userdata script with sudo tail -f /var/log/cloud-init-output.log

              sudo apt-get update
              sudo apt-get install -y docker.io
              sudo usermod -a -G docker ubuntu

              curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
              sudo chmod +x /usr/local/bin/docker-compose
              sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

              sudo systemctl enable docker
              sudo systemctl start docker

              sudo apt-get install -y git

              git clone https://github.com/Mintplex-Labs/anything-llm.git /home/anything-llm
              cd /home/anything-llm/docker

              cat >> .env << END
              !SUB::USER::CONTENT!
              UID=""1000""
              GID=""1000""
              NO_DEBUG=""true""
              END

              echo ""Set .env file""

              cd ../frontend
              sudo rm -rf .env.production

              sudo cat >> .env.production << END
              GENERATE_SOURCEMAP=true
              VITE_API_BASE=""/api""
              END

              echo ""Set .env.production file""

              cd ../docker
              sudo docker-compose up -d --build
              echo ""Container ID: $(sudo docker ps --latest --quiet)""

              sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) mkdir -p /app/server/storage /app/server/storage/documents /app/server/storage/vector-cache /app/server/storage/lancedb
              echo ""Placeholder folders in storage created.""

              sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) touch /app/server/storage/anythingllm.db
              echo ""SQLite DB placeholder set.""

              sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) chown -R anythingllm:anythingllm /app/collector /app/server
              echo ""File permissions corrected.""

              export ONLINE=$(curl -Is http://localhost:3001/api/ping | head -n 1|cut -d$' ' -f2)
              echo ""Health check: $ONLINE""

              if [ ""$ONLINE"" = 200 ]; then
                echo ""Running migrations..."" && curl -Is http://localhost:3001/api/migrate | head -n 1 | cut -d$' ' -f2
              fi

              echo ""Setup complete! AnythingLLM instance is now online!""
",,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,CVE-2023-4897,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR: - VAR: VAR-VAR-VAR VAR: VAR.VAR.VAR VAR: VAR: VAR-VAR-VAR VAR: VAR/VAR-VAR-VAR/VAR/VAR-VAR-1 VAR: - VAR: VAR VAR: VAR VAR: VAR VAR: VAR VAR: VAR: VAR/VAR-VAR-VAR/VAR/VAR/VAR/VAR-2004-VAR VAR: 10 VAR: - VAR: VAR/VAR/VAR VAR: - VAR: VAR VAR VAR: VAR VAR: VAR: - VAR: VAR-VAR VAR: |   VAR VAR-VAR VAR VAR VAR-VAR VAR -VAR VAR.VAR VAR VAR -VAR -VAR VAR VAR VAR -VAR VAR: VAR VAR +VAR /VAR/VAR/VAR/VAR-VAR VAR VAR -VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR-VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR-VAR VAR -VAR VAR VAR VAR VAR: VAR /VAR/VAR-VAR/VAR VAR >> .VAR << VAR !VAR::VAR::VAR! VAR=""1000"" VAR=""1000"" VAR=""VAR"" VAR VAR ""VAR .VAR VAR"" VAR ../VAR VAR VAR -VAR .VAR.VAR VAR VAR >> .VAR.VAR << VAR VAR=VAR VAR=""/VAR"" VAR VAR ""VAR .VAR.VAR VAR"" VAR ../VAR VAR VAR-VAR VAR -VAR --VAR VAR ""VAR VAR: $(VAR VAR VAR --VAR --VAR)"" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR /VAR/VAR/VAR /VAR/VAR/VAR/VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR/VAR VAR ""VAR VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR /VAR/VAR/VAR/VAR.VAR VAR ""VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR VAR:VAR /VAR/VAR /VAR/VAR VAR ""VAR VAR VAR."" VAR VAR=$(VAR -VAR VAR: VAR ""VAR VAR: $VAR"" VAR [ ""$VAR"" = 200 ]; VAR VAR ""VAR VAR..."" && VAR -VAR VAR: VAR VAR ""VAR VAR! VAR VAR VAR VAR VAR!"" ",14,9
anything-llm_3c88aec034934bcbad30c5ef1cab62cbbdb98e64,docker/.env.example,"SERVER_PORT=3001
CACHE_VECTORS=""true""
# JWT_SECRET=""my-random-string-for-seeding"" # Only needed if AUTH_TOKEN is set. Please generate random string at least 12 chars long.

###########################################
######## LLM API SElECTION ################
###########################################
LLM_PROVIDER='openai'
# OPEN_AI_KEY=
OPEN_MODEL_PREF='gpt-3.5-turbo'

# LLM_PROVIDER='azure'
# AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_KEY=
# OPEN_MODEL_PREF='my-gpt35-deployment' # This is the ""deployment"" on Azure you want to use. Not the base model.
# EMBEDDING_MODEL_PREF='embedder-model' # This is the ""deployment"" on Azure you want to use for embeddings. Not the base model. Valid base model is text-embedding-ada-002


###########################################
######## Vector Database Selection ########
###########################################
# Enable all below if you are using vector database: Chroma.
# VECTOR_DB=""chroma""
# CHROMA_ENDPOINT='http://host.docker.internal:8000'

# Enable all below if you are using vector database: Pinecone.
VECTOR_DB=""pinecone""
PINECONE_ENVIRONMENT=
PINECONE_API_KEY=
PINECONE_INDEX=

# Enable all below if you are using vector database: LanceDB.
# VECTOR_DB=""lancedb""

# Enable all below if you are using vector database: Weaviate.
# VECTOR_DB=""weaviate""
# WEAVIATE_ENDPOINT=""http://localhost:8080""
# WEAVIATE_API_KEY=

# Enable all below if you are using vector database: Qdrant.
# VECTOR_DB=""qdrant""
# QDRANT_ENDPOINT=""http://localhost:6333""
# QDRANT_API_KEY=

# CLOUD DEPLOYMENT VARIRABLES ONLY
# AUTH_TOKEN=""hunter2"" # This is the password to your application if remote hosting.
# NO_DEBUG=""true""
STORAGE_DIR=""./server/storage""
GOOGLE_APIS_KEY=
UID='1000'
GID='1000'",,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,CVE-2023-4897,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR=3001 VAR=""VAR""     VAR='VAR'  VAR='VAR-3.5-VAR'             VAR=""VAR"" VAR= VAR= VAR=              VAR=""./VAR/VAR"" VAR= VAR='1000' VAR='1000' ",14,9
anything-llm_3c88aec034934bcbad30c5ef1cab62cbbdb98e64,docker/HOW_TO_USE_DOCKER.md,"# How to use Dockerized Anything LLM

Use the Dockerized version of AnythingLLM for a much faster and complete startup of AnythingLLM.

## Requirements
- Install [Docker](https://www.docker.com/) on your computer or machine.

## How to install
- `git clone` this repo and `cd anything-llm` to get to the root directory.
- `cd docker/`
- `cp .env.example .env` to create the `.env` file.
- Edit `.env` file and update the variables
- `docker-compose up -d --build` to build the image - this will take a few moments.

Your docker host will show the image as online once the build process is completed. This will build the app to `http://localhost:3001`.

## How to use the user interface
- To access the full application, visit `http://localhost:3001` in your browser.

## How to add files to my system
- Upload files from the UI in your Workspace settings

- To run the collector scripts to grab external data (articles, URLs, etc.)
  - `docker exec -it --workdir=/app/collector anything-llm python main.py`

- To run the collector watch script to process files from the hotdir
  - `docker exec -it --workdir=/app/collector anything-llm python watch.py`
  - Upload [compliant files](../collector/hotdir/__HOTDIR__.md) to `./collector/hotdir` and they will be processed and made available in the UI.

## How to update and rebuild the ENV?
- Update the `./docker/.env` and run `docker-compose up -d --build` to rebuild with new environments.

## About UID and GID in the ENV
- The UID and GID are set to 1000 by default. This is the default user in the Docker container and on most host operating systems. If there is a mismatch between your host user UID and GID and what is set in the `.env` file, you may experience permission issues.

## ⚠️ Vector DB support ⚠️
Out of the box, all vector databases are supported. Any vector databases requiring special configuration are listed below.

### Using local ChromaDB with Dockerized AnythingLLM
- Ensure in your `./docker/.env` file that you have
```
#./docker/.env
...other configs

VECTOR_DB=""chroma""
CHROMA_ENDPOINT='http://host.docker.internal:8000' # Allow docker to look on host port, not container.

...other configs

```

## Common questions and fixes

### API is not working, cannot login, LLM is ""offline""?
You are likely running the docker container on a remote machine like EC2 or some other instance where the reachable URL
is not `http://localhost:3001` and instead is something like `http://193.xx.xx.xx:3001` - in this case all you need to do is add the following to your `frontend/.env.production` before running `docker-compose up -d --build`
```
# frontend/.env.production
GENERATE_SOURCEMAP=false
VITE_API_BASE=""http://<YOUR_REACHABLE_IP_ADDRESS>:3001/api""
```
For example, if the docker instance is available on `192.186.1.222` your `VITE_API_BASE` would look like `VITE_API_BASE=""http://192.186.1.222:3001/api""` in `frontend/.env.production`.

### Still not working?
[Ask for help on Discord](https://discord.gg/6UyHPeGZAC)

",,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,CVE-2023-4897,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED," VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR.  - VAR [VAR](VAR:  - `VAR VAR` VAR VAR VAR `VAR VAR-VAR` VAR VAR VAR VAR VAR VAR. - `VAR VAR/` - `VAR .VAR.VAR .VAR` VAR VAR VAR `.VAR` VAR. - VAR `.VAR` VAR VAR VAR VAR VAR - `VAR-VAR VAR -VAR --VAR` VAR VAR VAR VAR - VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR `VAR:  - VAR VAR VAR VAR VAR, VAR `VAR:  - VAR VAR VAR VAR VAR VAR VAR VAR VAR - VAR VAR VAR VAR VAR VAR VAR VAR VAR (VAR, VAR, VAR.) - `VAR VAR -VAR --VAR=/VAR/VAR VAR-VAR VAR VAR.VAR` - VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR - `VAR VAR -VAR --VAR=/VAR/VAR VAR-VAR VAR VAR.VAR` - VAR [VAR VAR](../VAR/VAR/VAR.VAR) VAR `./VAR/VAR` VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR.  - VAR VAR `./VAR/.VAR` VAR VAR `VAR-VAR VAR -VAR --VAR` VAR VAR VAR VAR VAR.  - VAR VAR VAR VAR VAR VAR VAR 1000 VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR `.VAR` VAR, VAR VAR VAR VAR VAR.  VAR VAR VAR VAR, VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR.  - VAR VAR VAR `./VAR/.VAR` VAR VAR VAR VAR ```  ...VAR VAR VAR=""VAR"" VAR='VAR: ...VAR VAR ```   VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR `VAR: ```  VAR=VAR VAR=""VAR: ``` VAR VAR, VAR VAR VAR VAR VAR VAR VAR `192.186.1.222` VAR `VAR` VAR VAR VAR `VAR=""VAR:  [VAR VAR VAR VAR VAR](VAR: ",14,9
anything-llm_3c88aec034934bcbad30c5ef1cab62cbbdb98e64,server/utils/vectorDbProviders/chroma/index.js,"const { ChromaClient } = require(""chromadb"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");

const Chroma = {
  name: ""Chroma"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""chroma"")
      throw new Error(""Chroma::Invalid ENV settings"");

    const client = new ChromaClient({
      path: process.env.CHROMA_ENDPOINT, // if not set will fallback to localhost:8000
    });

    const isAlive = await client.heartbeat();
    if (!isAlive)
      throw new Error(
        ""ChromaDB::Invalid Heartbeat received - is the instance online?""
      );
    return { client };
  },
  heartbeat: async function () {
    const { client } = await this.connect();
    return { heartbeat: await client.heartbeat() };
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const collections = await client.listCollections();
    var totalVectors = 0;
    for (const collectionObj of collections) {
      const collection = await client
        .getCollection({ name: collectionObj.name })
        .catch(() => null);
      if (!collection) continue;
      totalVectors += await collection.count();
    }
    return totalVectors;
  },
  namespaceCount: async function (_namespace = null) {
    const { client } = await this.connect();
    const namespace = await this.namespace(client, _namespace);
    return namespace?.vectorCount || 0;
  },
  similarityResponse: async function (client, namespace, queryVector) {
    const collection = await client.getCollection({ name: namespace });
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const response = await collection.query({
      queryEmbeddings: queryVector,
      nResults: 4,
    });
    response.ids[0].forEach((_, i) => {
      result.contextTexts.push(response.documents[0][i]);
      result.sourceDocuments.push(response.metadatas[0][i]);
    });

    return result;
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client
      .getCollection({ name: namespace })
      .catch(() => null);
    if (!collection) return null;

    return {
      ...collection,
      vectorCount: await collection.count(),
    };
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    return await this.namespaceExists(client, namespace);
  },
  namespaceExists: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client
      .getCollection({ name: namespace })
      .catch((e) => {
        console.error(""ChromaDB::namespaceExists"", e.message);
        return null;
      });
    return !!collection;
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    await client.deleteCollection({ name: namespace });
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const collection = await client.getOrCreateCollection({
          name: namespace,
          metadata: { ""hnsw:space"": ""cosine"" },
        });
        const { chunks } = cacheResult;
        const documentVectors = [];

        for (const chunk of chunks) {
          const submission = {
            ids: [],
            embeddings: [],
            metadatas: [],
            documents: [],
          };

          // Before sending to Chroma and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const { id: _id, ...metadata } = chunk.metadata;
            documentVectors.push({ docId, vectorId: id });
            submission.ids.push(id);
            submission.embeddings.push(chunk.values);
            submission.metadatas.push(metadata);
            submission.documents.push(metadata.text);
          });

          const additionResult = await collection.add(submission);
          if (!additionResult)
            throw new Error(""Error embedding into ChromaDB"", additionResult);
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `Chroma.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);
      const submission = {
        ids: [],
        embeddings: [],
        metadatas: [],
        documents: [],
      };

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            values: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            metadata: { ...metadata, text: textChunks[i] },
          };

          submission.ids.push(vectorRecord.id);
          submission.embeddings.push(vectorRecord.values);
          submission.metadatas.push(metadata);
          submission.documents.push(textChunks[i]);

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      const { client } = await this.connect();
      const collection = await client.getOrCreateCollection({
        name: namespace,
        metadata: { ""hnsw:space"": ""cosine"" },
      });

      if (vectors.length > 0) {
        const chunks = [];

        console.log(""Inserting vectorized chunks into Chroma collection."");
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        const additionResult = await collection.add(submission);
        if (!additionResult)
          throw new Error(""Error embedding into ChromaDB"", additionResult);

        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) return;
    const collection = await client.getCollection({
      name: namespace,
    });

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    const vectorIds = knownDocuments.map((doc) => doc.vectorId);
    await collection.delete({ ids: vectorIds });

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    // When we roll out own response we have separate metadata and texts,
    // so for source collection we need to combine them.
    const sources = sourceDocuments.map((metadata, i) => {
      return { metadata: { ...metadata, text: contextTexts[i] } };
    });
    return {
      response: responseText,
      sources: this.curateSources(sources),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    // When we roll out own response we have separate metadata and texts,
    // so for source collection we need to combine them.
    const sources = sourceDocuments.map((metadata, i) => {
      return { metadata: { ...metadata, text: contextTexts[i] } };
    });
    return {
      response: responseText,
      sources: this.curateSources(sources),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    const details = await this.namespace(client, namespace);
    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${namespace} was deleted along with ${details?.vectorCount} vectors.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    await client.reset();
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      const { metadata = {} } = source;
      if (Object.keys(metadata).length > 0) {
        documents.push({
          ...metadata,
          ...(source.hasOwnProperty(""pageContent"")
            ? { text: source.pageContent }
            : {}),
        });
      }
    }

    return documents;
  },
};

module.exports.Chroma = Chroma;",,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,CVE-2023-4897,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR } = VAR(""VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR({ VAR: VAR.VAR.VAR,  }); VAR VAR = VAR VAR.VAR(); VAR (!VAR) VAR VAR VAR( ""VAR::VAR VAR VAR - VAR VAR VAR VAR?"" ); VAR { VAR }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR { VAR: VAR VAR.VAR() }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR VAR = VAR VAR .VAR({ VAR: VAR.VAR }) .VAR(() => VAR); VAR (!VAR) VAR; VAR += VAR VAR.VAR(); } VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR?.VAR || 0; }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = VAR VAR.VAR({ VAR: VAR }); VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR({ VAR: VAR, VAR: 4, }); VAR.VAR[0].VAR((VAR, VAR) => { VAR.VAR.VAR(VAR.VAR[0][VAR]); VAR.VAR.VAR(VAR.VAR[0][VAR]); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR .VAR({ VAR: VAR }) .VAR(() => VAR); VAR (!VAR) VAR VAR; VAR { ...VAR, VAR: VAR VAR.VAR(), }; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR VAR.VAR(VAR, VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR .VAR({ VAR: VAR }) .VAR((VAR) => { VAR.VAR(""VAR::VAR"", VAR.VAR); VAR VAR; }); VAR !!VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR({ VAR: VAR }); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR({ VAR: VAR, VAR: { ""VAR:VAR"": ""VAR"" }, }); VAR { VAR } = VAR; VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR VAR = { VAR: [], VAR: [], VAR: [], VAR: [], };   VAR.VAR((VAR) => { VAR VAR = VAR(); VAR { VAR: VAR, ...VAR } = VAR.VAR; VAR.VAR({ VAR, VAR: VAR }); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR.VAR); }); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR"", VAR); } VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], VAR: [], VAR: [], }; VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR[VAR]); VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR({ VAR: VAR, VAR: { ""VAR:VAR"": ""VAR"" }, }); VAR (VAR.VAR > 0) { VAR VAR = []; VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR"", VAR); VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR({ VAR: VAR, }); VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR({ VAR: VAR }); VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, });   VAR VAR = VAR.VAR((VAR, VAR) => { VAR { VAR: { ...VAR, VAR: VAR[VAR] } }; }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, });   VAR VAR = VAR.VAR((VAR, VAR) => { VAR { VAR: { ...VAR, VAR: VAR[VAR] } }; }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR VAR VAR ${VAR?.VAR} VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(); VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR { VAR = {} } = VAR; VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, ...(VAR.VAR(""VAR"") ? { VAR: VAR.VAR } : {}), }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_3c88aec034934bcbad30c5ef1cab62cbbdb98e64,server/utils/vectorDbProviders/lance/index.js,"const lancedb = require(""vectordb"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { OpenAIEmbeddings } = require(""langchain/embeddings/openai"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { chatPrompt } = require(""../../chats"");

const LanceDb = {
  uri: `${
    !!process.env.STORAGE_DIR ? `${process.env.STORAGE_DIR}/` : ""./storage/""
  }lancedb`,
  name: ""LanceDb"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""lancedb"")
      throw new Error(""LanceDB::Invalid ENV settings"");

    const client = await lancedb.connect(this.uri);
    return { client };
  },
  heartbeat: async function () {
    await this.connect();
    return { heartbeat: Number(new Date()) };
  },
  tables: async function () {
    const fs = require(""fs"");
    const { client } = await this.connect();
    const dirs = fs.readdirSync(client.uri);
    return dirs.map((folder) => folder.replace("".lance"", """"));
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const tables = await this.tables();
    let count = 0;
    for (const tableName of tables) {
      const table = await client.openTable(tableName);
      count += await table.countRows();
    }
    return count;
  },
  namespaceCount: async function (_namespace = null) {
    const { client } = await this.connect();
    const exists = await this.namespaceExists(client, _namespace);
    if (!exists) return 0;

    const table = await client.openTable(_namespace);
    return (await table.countRows()) || 0;
  },
  embedder: function () {
    return new OpenAIEmbeddings({ openAIApiKey: process.env.OPEN_AI_KEY });
  },
  similarityResponse: async function (client, namespace, queryVector) {
    const collection = await client.openTable(namespace);
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const response = await collection
      .search(queryVector)
      .metricType(""cosine"")
      .limit(5)
      .execute();

    response.forEach((item) => {
      const { vector: _, ...rest } = item;
      result.contextTexts.push(rest.text);
      result.sourceDocuments.push(rest);
    });

    return result;
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client.openTable(namespace).catch(() => false);
    if (!collection) return null;

    return {
      ...collection,
    };
  },
  updateOrCreateCollection: async function (client, data = [], namespace) {
    const hasNamespace = await this.hasNamespace(namespace);
    if (hasNamespace) {
      const collection = await client.openTable(namespace);
      await collection.add(data);
      return true;
    }

    await client.createTable(namespace, data);
    return true;
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    const exists = await this.namespaceExists(client, namespace);
    return exists;
  },
  namespaceExists: async function (_client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collections = await this.tables();
    return collections.includes(namespace);
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    const fs = require(""fs"");
    fs.rm(`${client.uri}/${namespace}.lance`, { recursive: true }, () => null);
    return true;
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { client } = await this.connect();
    const exists = await this.namespaceExists(client, namespace);
    if (!exists) {
      console.error(
        `LanceDB:deleteDocumentFromNamespace - namespace ${namespace} does not exist.`
      );
      return;
    }

    const { DocumentVectors } = require(""../../../models/vectors"");
    const table = await client.openTable(namespace);
    const vectorIds = (await DocumentVectors.where(`docId = '${docId}'`)).map(
      (record) => record.vectorId
    );

    await table.delete(`id IN (${vectorIds.map((v) => `'${v}'`).join("","")})`);
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const { chunks } = cacheResult;
        const documentVectors = [];
        const submissions = [];

        for (const chunk of chunks) {
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const { id: _id, ...metadata } = chunk.metadata;
            documentVectors.push({ docId, vectorId: id });
            submissions.push({ id: id, vector: chunk.values, ...metadata });
          });
        }

        await this.updateOrCreateCollection(client, submissions, namespace);
        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `xyz.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const submissions = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            values: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            metadata: { ...metadata, text: textChunks[i] },
          };

          vectors.push(vectorRecord);
          submissions.push({
            id: vectorRecord.id,
            vector: vectorRecord.values,
            ...vectorRecord.metadata,
          });
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      if (vectors.length > 0) {
        const chunks = [];
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        console.log(""Inserting vectorized chunks into LanceDB collection."");
        const { client } = await this.connect();
        await this.updateOrCreateCollection(client, submissions, namespace);
        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${namespace} was deleted.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    const fs = require(""fs"");
    fs.rm(`${client.uri}`, { recursive: true }, () => null);
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      const { text, vector: _v, score: _s, ...metadata } = source;
      if (Object.keys(metadata).length > 0) {
        documents.push({ ...metadata, text });
      }
    }

    return documents;
  },
};

module.exports.LanceDb = LanceDb;",,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,CVE-2023-4897,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR VAR = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""VAR/VAR/VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: `${ !!VAR.VAR.VAR ? `${VAR.VAR.VAR}/` : ""./VAR/"" }VAR`, VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR.VAR(VAR.VAR); VAR { VAR }; }, VAR: VAR VAR () { VAR VAR.VAR(); VAR { VAR: VAR(VAR VAR()) }; }, VAR: VAR VAR () { VAR VAR = VAR(""VAR""); VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR.VAR(VAR.VAR); VAR VAR.VAR((VAR) => VAR.VAR("".VAR"", """")); }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR += VAR VAR.VAR(); } VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) VAR 0; VAR VAR = VAR VAR.VAR(VAR); VAR (VAR VAR.VAR()) || 0; }, VAR: VAR () { VAR VAR VAR({ VAR: VAR.VAR.VAR }); }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR .VAR(VAR) .VAR(""VAR"") .VAR(5) .VAR(); VAR.VAR((VAR) => { VAR { VAR: VAR, ...VAR } = VAR; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR).VAR(() => VAR); VAR (!VAR) VAR VAR; VAR { ...VAR, }; }, VAR: VAR VAR (VAR, VAR = [], VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR (VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR VAR.VAR(VAR); VAR VAR; } VAR VAR.VAR(VAR, VAR); VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(); VAR VAR.VAR(VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR = VAR(""VAR""); VAR.VAR(`${VAR.VAR}/${VAR}.VAR`, { VAR: VAR }, () => VAR); VAR VAR; }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) { VAR.VAR( `VAR:VAR - VAR ${VAR} VAR VAR VAR.` ); VAR; } VAR { VAR } = VAR(""../../../VAR/VAR""); VAR VAR = VAR VAR.VAR(VAR); VAR VAR = (VAR VAR.VAR(`VAR = '${VAR}'`)).VAR( (VAR) => VAR.VAR ); VAR VAR.VAR(`VAR VAR (${VAR.VAR((VAR) => `'${VAR}'`).VAR("","")})`); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR; VAR VAR = []; VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR.VAR((VAR) => { VAR VAR = VAR(); VAR { VAR: VAR, ...VAR } = VAR.VAR; VAR.VAR({ VAR, VAR: VAR }); VAR.VAR({ VAR: VAR, VAR: VAR.VAR, ...VAR }); }); } VAR VAR.VAR(VAR, VAR, VAR); VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR(VAR); VAR.VAR({ VAR: VAR.VAR, VAR: VAR.VAR, ...VAR.VAR, }); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR (VAR.VAR > 0) { VAR VAR = []; VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR, VAR, VAR); VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR(""VAR""); VAR.VAR(`${VAR.VAR}`, { VAR: VAR }, () => VAR); VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR { VAR, VAR: VAR, VAR: VAR, ...VAR } = VAR; VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, VAR }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_3c88aec034934bcbad30c5ef1cab62cbbdb98e64,server/utils/vectorDbProviders/pinecone/index.js,"const { PineconeClient } = require(""@pinecone-database/pinecone"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");

const Pinecone = {
  name: ""Pinecone"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""pinecone"")
      throw new Error(""Pinecone::Invalid ENV settings"");

    const client = new PineconeClient();
    await client.init({
      apiKey: process.env.PINECONE_API_KEY,
      environment: process.env.PINECONE_ENVIRONMENT,
    });
    const pineconeIndex = client.Index(process.env.PINECONE_INDEX);
    const { status } = await client.describeIndex({
      indexName: process.env.PINECONE_INDEX,
    });

    if (!status.ready) throw new Error(""Pinecode::Index not ready."");
    return { client, pineconeIndex, indexName: process.env.PINECONE_INDEX };
  },
  totalIndicies: async function () {
    const { pineconeIndex } = await this.connect();
    const { namespaces } = await pineconeIndex.describeIndexStats1();
    return Object.values(namespaces).reduce(
      (a, b) => a + (b?.vectorCount || 0),
      0
    );
  },
  namespaceCount: async function (_namespace = null) {
    const { pineconeIndex } = await this.connect();
    const namespace = await this.namespace(pineconeIndex, _namespace);
    return namespace?.vectorCount || 0;
  },
  similarityResponse: async function (index, namespace, queryVector) {
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };
    const response = await index.query({
      queryRequest: {
        namespace,
        vector: queryVector,
        topK: 4,
        includeMetadata: true,
      },
    });

    response.matches.forEach((match) => {
      result.contextTexts.push(match.metadata.text);
      result.sourceDocuments.push(match);
    });

    return result;
  },
  namespace: async function (index, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const { namespaces } = await index.describeIndexStats1();
    return namespaces.hasOwnProperty(namespace) ? namespaces[namespace] : null;
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { pineconeIndex } = await this.connect();
    return await this.namespaceExists(pineconeIndex, namespace);
  },
  namespaceExists: async function (index, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const { namespaces } = await index.describeIndexStats1();
    return namespaces.hasOwnProperty(namespace);
  },
  deleteVectorsInNamespace: async function (index, namespace = null) {
    await index.delete1({ namespace, deleteAll: true });
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { pineconeIndex } = await this.connect();
        const { chunks } = cacheResult;
        const documentVectors = [];

        for (const chunk of chunks) {
          // Before sending to Pinecone and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          const newChunks = chunk.map((chunk) => {
            const id = uuidv4();
            documentVectors.push({ docId, vectorId: id });
            return { ...chunk, id };
          });

          // Push chunks with new ids to pinecone.
          await pineconeIndex.upsert({
            upsertRequest: {
              vectors: [...newChunks],
              namespace,
            },
          });
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `PineconeStore.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L167
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            values: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            metadata: { ...metadata, text: textChunks[i] },
          };

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      if (vectors.length > 0) {
        const chunks = [];
        const { pineconeIndex } = await this.connect();
        console.log(""Inserting vectorized chunks into Pinecone."");
        for (const chunk of toChunks(vectors, 100)) {
          chunks.push(chunk);
          await pineconeIndex.upsert({
            upsertRequest: {
              vectors: [...chunk],
              namespace,
            },
          });
        }
        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace))) return;

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    const vectorIds = knownDocuments.map((doc) => doc.vectorId);
    for (const batchOfVectorIds of toChunks(vectorIds, 1000)) {
      await pineconeIndex.delete1({
        ids: batchOfVectorIds,
        namespace,
      });
    }

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(pineconeIndex, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    const details = await this.namespace(pineconeIndex, namespace);
    await this.deleteVectorsInNamespace(pineconeIndex, namespace);
    return {
      message: `Namespace ${namespace} was deleted along with ${details.vectorCount} vectors.`,
    };
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      pineconeIndex,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
     Context:
     ${contextTexts
       .map((text, i) => {
         return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
       })
       .join("""")}`,
    };

    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace)))
      throw new Error(
        ""Invalid namespace - has it been collected and seeded yet?""
      );

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      pineconeIndex,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };

    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      const { metadata = {} } = source;
      if (Object.keys(metadata).length > 0) {
        documents.push({
          ...metadata,
          ...(source.hasOwnProperty(""pageContent"")
            ? { text: source.pageContent }
            : {}),
        });
      }
    }

    return documents;
  },
};

module.exports.Pinecone = Pinecone;",,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,CVE-2023-4897,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR } = VAR(""@VAR-VAR/VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR(); VAR VAR.VAR({ VAR: VAR.VAR.VAR, VAR: VAR.VAR.VAR, }); VAR VAR = VAR.VAR(VAR.VAR.VAR); VAR { VAR } = VAR VAR.VAR({ VAR: VAR.VAR.VAR, }); VAR (!VAR.VAR) VAR VAR VAR(""VAR::VAR VAR VAR.""); VAR { VAR, VAR, VAR: VAR.VAR.VAR }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR).VAR( (VAR, VAR) => VAR + (VAR?.VAR || 0), 0 ); }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR?.VAR || 0; }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR({ VAR: { VAR, VAR: VAR, VAR: 4, VAR: VAR, }, }); VAR.VAR.VAR((VAR) => { VAR.VAR.VAR(VAR.VAR.VAR); VAR.VAR.VAR(VAR); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR) ? VAR[VAR] : VAR; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR VAR.VAR(VAR, VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR({ VAR, VAR: VAR }); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR; VAR VAR = []; VAR (VAR VAR VAR VAR) {   VAR VAR = VAR.VAR((VAR) => { VAR VAR = VAR(); VAR.VAR({ VAR, VAR: VAR }); VAR { ...VAR, VAR }; });  VAR VAR.VAR({ VAR: { VAR: [...VAR], VAR, }, }); } VAR VAR.VAR(VAR); VAR VAR; }      VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR (VAR.VAR > 0) { VAR VAR = []; VAR { VAR } = VAR VAR.VAR(); VAR.VAR(""VAR VAR VAR VAR VAR.""); VAR (VAR VAR VAR VAR(VAR, 100)) { VAR.VAR(VAR); VAR VAR.VAR({ VAR: { VAR: [...VAR], VAR, }, }); } VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR (VAR VAR VAR VAR(VAR, 1000)) { VAR VAR.VAR({ VAR: VAR, VAR, }); } VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR VAR VAR ${VAR.VAR} VAR.`, }; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR( ""VAR VAR - VAR VAR VAR VAR VAR VAR VAR?"" ); VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR { VAR = {} } = VAR; VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, ...(VAR.VAR(""VAR"") ? { VAR: VAR.VAR } : {}), }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_3c88aec034934bcbad30c5ef1cab62cbbdb98e64,server/utils/vectorDbProviders/qdrant/index.js,"const { QdrantClient } = require(""@qdrant/js-client-rest"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");

const QDrant = {
  name: ""QDrant"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""qdrant"")
      throw new Error(""QDrant::Invalid ENV settings"");

    const client = new QdrantClient({
      url: process.env.QDRANT_ENDPOINT,
      ...(process.env.QDRANT_API_KEY
        ? { apiKey: process.env.QDRANT_API_KEY }
        : {}),
    });

    const isAlive = (await client.api(""cluster"")?.clusterStatus())?.ok || false;
    if (!isAlive)
      throw new Error(
        ""QDrant::Invalid Heartbeat received - is the instance online?""
      );

    return { client };
  },
  heartbeat: async function () {
    await this.connect();
    return { heartbeat: Number(new Date()) };
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const { collections } = await client.getCollections();
    var totalVectors = 0;
    for (const collection of collections) {
      if (!collection || !collection.name) continue;
      totalVectors +=
        (await this.namespace(client, collection.name))?.vectorCount || 0;
    }
    return totalVectors;
  },
  namespaceCount: async function (_namespace = null) {
    const { client } = await this.connect();
    const namespace = await this.namespace(client, _namespace);
    return namespace?.vectorCount || 0;
  },
  similarityResponse: async function (_client, namespace, queryVector) {
    const { client } = await this.connect();
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const responses = await client.search(namespace, {
      vector: queryVector,
      limit: 4,
    });

    responses.forEach((response) => {
      result.contextTexts.push(response?.payload?.text || """");
      result.sourceDocuments.push({
        ...(response?.payload || {}),
        id: response.id,
      });
    });

    return result;
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client.getCollection(namespace).catch(() => null);
    if (!collection) return null;

    return {
      name: namespace,
      ...collection,
      vectorCount: collection.vectors_count,
    };
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    return await this.namespaceExists(client, namespace);
  },
  namespaceExists: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client.getCollection(namespace).catch((e) => {
      console.error(""QDrant::namespaceExists"", e.message);
      return null;
    });
    return !!collection;
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    await client.deleteCollection(namespace);
    return true;
  },
  getOrCreateCollection: async function (client, namespace) {
    if (await this.namespaceExists(client, namespace)) {
      return await client.getCollection(namespace);
    }
    await client.createCollection(namespace, {
      vectors: {
        size: 1536, //TODO: Fixed to OpenAI models - when other embeddings exist make variable.
        distance: ""Cosine"",
      },
    });
    return await client.getCollection(namespace);
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const collection = await this.getOrCreateCollection(client, namespace);
        if (!collection)
          throw new Error(""Failed to create new QDrant collection!"", {
            namespace,
          });

        const { chunks } = cacheResult;
        const documentVectors = [];

        for (const chunk of chunks) {
          const submission = {
            ids: [],
            vectors: [],
            payloads: [],
          };

          // Before sending to Qdrant and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const { id: _id, ...payload } = chunk.payload;
            documentVectors.push({ docId, vectorId: id });
            submission.ids.push(id);
            submission.vectors.push(chunk.vector);
            submission.payloads.push(payload);
          });

          const additionResult = await client.upsert(namespace, {
            wait: true,
            batch: { ...submission },
          });
          if (additionResult?.status !== ""completed"")
            throw new Error(""Error embedding into QDrant"", additionResult);
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `Qdrant.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);
      const submission = {
        ids: [],
        vectors: [],
        payloads: [],
      };

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            vector: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            payload: { ...metadata, text: textChunks[i] },
          };

          submission.ids.push(vectorRecord.id);
          submission.vectors.push(vectorRecord.vector);
          submission.payloads.push(vectorRecord.payload);

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      const { client } = await this.connect();
      const collection = await this.getOrCreateCollection(client, namespace);
      if (!collection)
        throw new Error(""Failed to create new QDrant collection!"", {
          namespace,
        });

      if (vectors.length > 0) {
        const chunks = [];

        console.log(""Inserting vectorized chunks into QDrant collection."");
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        const additionResult = await client.upsert(namespace, {
          wait: true,
          batch: {
            ids: submission.ids,
            vectors: submission.vectors,
            payloads: submission.payloads,
          },
        });
        if (additionResult?.status !== ""completed"")
          throw new Error(""Error embedding into QDrant"", additionResult);

        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) return;

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    const vectorIds = knownDocuments.map((doc) => doc.vectorId);
    await client.delete(namespace, {
      wait: true,
      points: vectorIds,
    });

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    const details = await this.namespace(client, namespace);
    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${namespace} was deleted along with ${details?.vectorCount} vectors.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    const response = await client.getCollections();
    for (const collection of response.collections) {
      await client.deleteCollection(collection.name);
    }
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      if (Object.keys(source).length > 0) {
        documents.push({
          ...source,
        });
      }
    }

    return documents;
  },
};

module.exports.QDrant = QDrant;",,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,CVE-2023-4897,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR } = VAR(""@VAR/VAR-VAR-VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR({ VAR: VAR.VAR.VAR, ...(VAR.VAR.VAR ? { VAR: VAR.VAR.VAR } : {}), }); VAR VAR = (VAR VAR.VAR(""VAR"")?.VAR())?.VAR || VAR; VAR (!VAR) VAR VAR VAR( ""VAR::VAR VAR VAR - VAR VAR VAR VAR?"" ); VAR { VAR }; }, VAR: VAR VAR () { VAR VAR.VAR(); VAR { VAR: VAR(VAR VAR()) }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR VAR.VAR(); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR (!VAR || !VAR.VAR) VAR; VAR += (VAR VAR.VAR(VAR, VAR.VAR))?.VAR || 0; } VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR?.VAR || 0; }, VAR: VAR VAR (VAR, VAR, VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR, VAR: 4, }); VAR.VAR((VAR) => { VAR.VAR.VAR(VAR?.VAR?.VAR || """"); VAR.VAR.VAR({ ...(VAR?.VAR || {}), VAR: VAR.VAR, }); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR).VAR(() => VAR); VAR (!VAR) VAR VAR; VAR { VAR: VAR, ...VAR, VAR: VAR.VAR, }; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR VAR.VAR(VAR, VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR).VAR((VAR) => { VAR.VAR(""VAR::VAR"", VAR.VAR); VAR VAR; }); VAR !!VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR, VAR) { VAR (VAR VAR.VAR(VAR, VAR)) { VAR VAR VAR.VAR(VAR); } VAR VAR.VAR(VAR, { VAR: { VAR: 1536,  VAR: ""VAR"", }, }); VAR VAR VAR.VAR(VAR); }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR!"", { VAR, }); VAR { VAR } = VAR; VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR VAR = { VAR: [], VAR: [], VAR: [], };   VAR.VAR((VAR) => { VAR VAR = VAR(); VAR { VAR: VAR, ...VAR } = VAR.VAR; VAR.VAR({ VAR, VAR: VAR }); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); }); VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR, VAR: { ...VAR }, }); VAR (VAR?.VAR !== ""VAR"") VAR VAR VAR(""VAR VAR VAR VAR"", VAR); } VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], VAR: [], }; VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR!"", { VAR, }); VAR (VAR.VAR > 0) { VAR VAR = []; VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR, VAR: { VAR: VAR.VAR, VAR: VAR.VAR, VAR: VAR.VAR, }, }); VAR (VAR?.VAR !== ""VAR"") VAR VAR VAR(""VAR VAR VAR VAR"", VAR); VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR, { VAR: VAR, VAR: VAR, }); VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR VAR VAR ${VAR?.VAR} VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(); VAR (VAR VAR VAR VAR.VAR) { VAR VAR.VAR(VAR.VAR); } VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_3c88aec034934bcbad30c5ef1cab62cbbdb98e64,server/utils/vectorDbProviders/weaviate/index.js,"const { default: weaviate } = require(""weaviate-ts-client"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");
const { camelCase } = require(""../../helpers/camelcase"");

const Weaviate = {
  name: ""Weaviate"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""weaviate"")
      throw new Error(""Weaviate::Invalid ENV settings"");

    const weaviateUrl = new URL(process.env.WEAVIATE_ENDPOINT);
    const options = {
      scheme: weaviateUrl.protocol?.replace("":"", """") || ""http"",
      host: weaviateUrl?.host,
      ...(process.env?.WEAVIATE_API_KEY?.length > 0
        ? { apiKey: new weaviate.ApiKey(process.env?.WEAVIATE_API_KEY) }
        : {}),
    };
    const client = weaviate.client(options);
    const isAlive = await await client.misc.liveChecker().do();
    if (!isAlive)
      throw new Error(
        ""Weaviate::Invalid Alive signal received - is the service online?""
      );
    return { client };
  },
  heartbeat: async function () {
    await this.connect();
    return { heartbeat: Number(new Date()) };
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const collectionNames = await this.allNamespaces(client);
    var totalVectors = 0;
    for (const name of collectionNames) {
      totalVectors += await this.namespaceCountWithClient(client, name);
    }
    return totalVectors;
  },
  namespaceCountWithClient: async function (client, namespace) {
    try {
      const response = await client.graphql
        .aggregate()
        .withClassName(camelCase(namespace))
        .withFields(""meta { count }"")
        .do();
      return (
        response?.data?.Aggregate?.[camelCase(namespace)]?.[0]?.meta?.count || 0
      );
    } catch (e) {
      console.error(`Weaviate:namespaceCountWithClient`, e.message);
      return 0;
    }
  },
  namespaceCount: async function (namespace = null) {
    try {
      const { client } = await this.connect();
      const response = await client.graphql
        .aggregate()
        .withClassName(camelCase(namespace))
        .withFields(""meta { count }"")
        .do();

      return (
        response?.data?.Aggregate?.[camelCase(namespace)]?.[0]?.meta?.count || 0
      );
    } catch (e) {
      console.error(`Weaviate:namespaceCountWithClient`, e.message);
      return 0;
    }
  },
  similarityResponse: async function (client, namespace, queryVector) {
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const weaviateClass = await this.namespace(client, namespace);
    const fields = weaviateClass.properties.map((prop) => prop.name).join("" "");
    const queryResponse = await client.graphql
      .get()
      .withClassName(camelCase(namespace))
      .withFields(`${fields} _additional { id }`)
      .withNearVector({ vector: queryVector })
      .withLimit(4)
      .do();

    const responses = queryResponse?.data?.Get?.[camelCase(namespace)];
    responses.forEach((response) => {
      // In Weaviate we have to pluck id from _additional and spread it into the rest
      // of the properties.
      const {
        _additional: { id },
        ...rest
      } = response;
      result.contextTexts.push(rest.text);
      result.sourceDocuments.push({ ...rest, id });
    });

    return result;
  },
  allNamespaces: async function (client) {
    try {
      const { classes = [] } = await client.schema.getter().do();
      return classes.map((classObj) => classObj.class);
    } catch (e) {
      console.error(""Weaviate::AllNamespace"", e);
      return [];
    }
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    if (!(await this.namespaceExists(client, namespace))) return null;

    const weaviateClass = await client.schema
      .classGetter()
      .withClassName(camelCase(namespace))
      .do();

    return {
      ...weaviateClass,
      vectorCount: await this.namespaceCount(namespace),
    };
  },
  addVectors: async function (client, vectors = []) {
    const response = { success: true, errors: new Set([]) };
    const results = await client.batch
      .objectsBatcher()
      .withObjects(...vectors)
      .do();

    results.forEach((res) => {
      const { status, errors = [] } = res.result;
      if (status === ""SUCCESS"" || errors.length === 0) return;
      response.success = false;
      response.errors.add(errors.error?.[0]?.message || null);
    });

    response.errors = [...response.errors];
    return response;
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    const weaviateClasses = await this.allNamespaces(client);
    return weaviateClasses.includes(camelCase(namespace));
  },
  namespaceExists: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const weaviateClasses = await this.allNamespaces(client);
    return weaviateClasses.includes(camelCase(namespace));
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    await client.schema.classDeleter().withClassName(camelCase(namespace)).do();
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const {
        pageContent,
        docId,
        id: _id, // Weaviate will abort if `id` is present in properties
        ...metadata
      } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const weaviateClassExits = await this.hasNamespace(namespace);
        if (!weaviateClassExits) {
          await client.schema
            .classCreator()
            .withClass({
              class: camelCase(namespace),
              description: `Class created by AnythingLLM named ${camelCase(
                namespace
              )}`,
              vectorizer: ""none"",
            })
            .do();
        }

        const { chunks } = cacheResult;
        const documentVectors = [];
        const vectors = [];

        for (const chunk of chunks) {
          // Before sending to Weaviate and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const flattenedMetadata = this.flattenObjectForWeaviate(
              chunk.properties
            );
            documentVectors.push({ docId, vectorId: id });
            const vectorRecord = {
              id,
              class: camelCase(namespace),
              vector: chunk.vector || chunk.values || [],
              properties: { ...flattenedMetadata },
            };
            vectors.push(vectorRecord);
          });

          const { success: additionResult, errors = [] } =
            await this.addVectors(client, vectors);
          if (!additionResult) {
            console.error(""Weaviate::addVectors failed to insert"", errors);
            throw new Error(""Error embedding into Weaviate"");
          }
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `Chroma.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);
      const submission = {
        ids: [],
        vectors: [],
        properties: [],
      };

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const flattenedMetadata = this.flattenObjectForWeaviate(metadata);
          const vectorRecord = {
            class: camelCase(namespace),
            id: uuidv4(),
            vector: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/5485c4af50c063e257ad54f4393fa79e0aff6462/langchain/src/vectorstores/weaviate.ts#L133
            properties: { ...flattenedMetadata, text: textChunks[i] },
          };

          submission.ids.push(vectorRecord.id);
          submission.vectors.push(vectorRecord.values);
          submission.properties.push(metadata);

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      const { client } = await this.connect();
      const weaviateClassExits = await this.hasNamespace(namespace);
      if (!weaviateClassExits) {
        await client.schema
          .classCreator()
          .withClass({
            class: camelCase(namespace),
            description: `Class created by AnythingLLM named ${camelCase(
              namespace
            )}`,
            vectorizer: ""none"",
          })
          .do();
      }

      if (vectors.length > 0) {
        const chunks = [];
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        console.log(""Inserting vectorized chunks into Weaviate collection."");
        const { success: additionResult, errors = [] } = await this.addVectors(
          client,
          vectors
        );
        if (!additionResult) {
          console.error(""Weaviate::addVectors failed to insert"", errors);
          throw new Error(""Error embedding into Weaviate"");
        }
        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) return;

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    for (const doc of knownDocuments) {
      await client.data
        .deleter()
        .withClassName(camelCase(namespace))
        .withId(doc.vectorId)
        .do();
    }

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );

    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    const details = await this.namespace(client, namespace);
    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${camelCase(namespace)} was deleted along with ${
        details?.vectorCount
      } vectors.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    const weaviateClasses = await this.allNamespaces(client);
    for (const weaviateClass of weaviateClasses) {
      await client.schema.classDeleter().withClassName(weaviateClass).do();
    }
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      if (Object.keys(source).length > 0) {
        documents.push(source);
      }
    }

    return documents;
  },
  flattenObjectForWeaviate: function (obj = {}) {
    // Note this function is not generic, it is designed specifically for Weaviate
    // https://weaviate.io/developers/weaviate/config-refs/datatypes#introduction
    // Credit to LangchainJS
    // https://github.com/hwchase17/langchainjs/blob/5485c4af50c063e257ad54f4393fa79e0aff6462/langchain/src/vectorstores/weaviate.ts#L11C1-L50C3
    const flattenedObject = {};

    for (const key in obj) {
      if (!Object.hasOwn(obj, key)) {
        continue;
      }
      const value = obj[key];
      if (typeof obj[key] === ""object"" && !Array.isArray(value)) {
        const recursiveResult = this.flattenObjectForWeaviate(value);

        for (const deepKey in recursiveResult) {
          if (Object.hasOwn(obj, key)) {
            flattenedObject[`${key}_${deepKey}`] = recursiveResult[deepKey];
          }
        }
      } else if (Array.isArray(value)) {
        if (
          value.length > 0 &&
          typeof value[0] !== ""object"" &&
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          value.every((el) => typeof el === typeof value[0])
        ) {
          // Weaviate only supports arrays of primitive types,
          // where all elements are of the same type
          flattenedObject[key] = value;
        }
      } else {
        flattenedObject[key] = value;
      }
    }

    return flattenedObject;
  },
};

module.exports.Weaviate = Weaviate;",,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,CVE-2023-4897,3c88aec034934bcbad30c5ef1cab62cbbdb98e64,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR: VAR } = VAR(""VAR-VAR-VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR/VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR(VAR.VAR.VAR); VAR VAR = { VAR: VAR.VAR?.VAR("":"", """") || ""VAR"", VAR: VAR?.VAR, ...(VAR.VAR?.VAR?.VAR > 0 ? { VAR: VAR VAR.VAR(VAR.VAR?.VAR) } : {}), }; VAR VAR = VAR.VAR(VAR); VAR VAR = VAR VAR VAR.VAR.VAR().VAR(); VAR (!VAR) VAR VAR VAR( ""VAR::VAR VAR VAR VAR - VAR VAR VAR VAR?"" ); VAR { VAR }; }, VAR: VAR VAR () { VAR VAR.VAR(); VAR { VAR: VAR(VAR VAR()) }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR += VAR VAR.VAR(VAR, VAR); } VAR VAR; }, VAR: VAR VAR (VAR, VAR) { VAR { VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(""VAR { VAR }"") .VAR(); VAR ( VAR?.VAR?.VAR?.[VAR(VAR)]?.[0]?.VAR?.VAR || 0 ); } VAR (VAR) { VAR.VAR(`VAR:VAR`, VAR.VAR); VAR 0; } }, VAR: VAR VAR (VAR = VAR) { VAR { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(""VAR { VAR }"") .VAR(); VAR ( VAR?.VAR?.VAR?.[VAR(VAR)]?.[0]?.VAR?.VAR || 0 ); } VAR (VAR) { VAR.VAR(`VAR:VAR`, VAR.VAR); VAR 0; } }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR = VAR.VAR.VAR((VAR) => VAR.VAR).VAR("" ""); VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(`${VAR} VAR { VAR }`) .VAR({ VAR: VAR }) .VAR(4) .VAR(); VAR VAR = VAR?.VAR?.VAR?.[VAR(VAR)]; VAR.VAR((VAR) => {   VAR { VAR: { VAR }, ...VAR } = VAR; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR({ ...VAR, VAR }); }); VAR VAR; }, VAR: VAR VAR (VAR) { VAR { VAR { VAR = [] } = VAR VAR.VAR.VAR().VAR(); VAR VAR.VAR((VAR) => VAR.VAR); } VAR (VAR) { VAR.VAR(""VAR::VAR"", VAR); VAR []; } }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR; VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(); VAR { ...VAR, VAR: VAR VAR.VAR(VAR), }; }, VAR: VAR VAR (VAR, VAR = []) { VAR VAR = { VAR: VAR, VAR: VAR VAR([]) }; VAR VAR = VAR VAR.VAR .VAR() .VAR(...VAR) .VAR(); VAR.VAR((VAR) => { VAR { VAR, VAR = [] } = VAR.VAR; VAR (VAR === ""VAR"" || VAR.VAR === 0) VAR; VAR.VAR = VAR; VAR.VAR.VAR(VAR.VAR?.[0]?.VAR || VAR); }); VAR.VAR = [...VAR.VAR]; VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR VAR.VAR(VAR(VAR)); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR); VAR VAR.VAR(VAR(VAR)); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR.VAR().VAR(VAR(VAR)).VAR(); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, VAR: VAR,  ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) { VAR VAR.VAR .VAR() .VAR({ VAR: VAR(VAR), VAR: `VAR VAR VAR VAR VAR ${VAR( VAR )}`, VAR: ""VAR"", }) .VAR(); } VAR { VAR } = VAR; VAR VAR = []; VAR VAR = []; VAR (VAR VAR VAR VAR) {   VAR.VAR((VAR) => { VAR VAR = VAR(); VAR VAR = VAR.VAR( VAR.VAR ); VAR.VAR({ VAR, VAR: VAR }); VAR VAR = { VAR, VAR: VAR(VAR), VAR: VAR.VAR || VAR.VAR || [], VAR: { ...VAR }, }; VAR.VAR(VAR); }); VAR { VAR: VAR, VAR = [] } = VAR VAR.VAR(VAR, VAR); VAR (!VAR) { VAR.VAR(""VAR::VAR VAR VAR VAR"", VAR); VAR VAR VAR(""VAR VAR VAR VAR""); } } VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], VAR: [], }; VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = VAR.VAR(VAR); VAR VAR = { VAR: VAR(VAR), VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) { VAR VAR.VAR .VAR() .VAR({ VAR: VAR(VAR), VAR: `VAR VAR VAR VAR VAR ${VAR( VAR )}`, VAR: ""VAR"", }) .VAR(); } VAR (VAR.VAR > 0) { VAR VAR = []; VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR { VAR: VAR, VAR = [] } = VAR VAR.VAR( VAR, VAR ); VAR (!VAR) { VAR.VAR(""VAR::VAR VAR VAR VAR"", VAR); VAR VAR VAR(""VAR VAR VAR VAR""); } VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR (VAR VAR VAR VAR) { VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(VAR.VAR) .VAR(); } VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR(VAR)} VAR VAR VAR VAR ${ VAR?.VAR } VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR (VAR VAR VAR VAR) { VAR VAR.VAR.VAR().VAR(VAR).VAR(); } VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR(VAR); } } VAR VAR; }, VAR: VAR (VAR = {}) {     VAR VAR = {}; VAR (VAR VAR VAR VAR) { VAR (!VAR.VAR(VAR, VAR)) { VAR; } VAR VAR = VAR[VAR]; VAR (VAR VAR[VAR] === ""VAR"" && !VAR.VAR(VAR)) { VAR VAR = VAR.VAR(VAR); VAR (VAR VAR VAR VAR) { VAR (VAR.VAR(VAR, VAR)) { VAR[`${VAR}VAR${VAR}`] = VAR[VAR]; } } } VAR VAR (VAR.VAR(VAR)) { VAR ( VAR.VAR > 0 && VAR VAR[0] !== ""VAR"" &&  VAR.VAR((VAR) => VAR VAR === VAR VAR[0]) ) {   VAR[VAR] = VAR; } } VAR { VAR[VAR] = VAR; } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,cloud-deployments/digitalocean/terraform/user_data.tp1,"#!/bin/bash
# check output of userdata script with sudo tail -f /var/log/cloud-init-output.log

sudo apt-get update
sudo apt-get install -y docker.io
sudo usermod -a -G docker ubuntu

curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

sudo systemctl enable docker
sudo systemctl start docker

sudo apt-get install -y git

git clone https://github.com/Mintplex-Labs/anything-llm.git /home/anything-llm
cd /home/anything-llm/docker

cat >> .env << END
${env_content}
UID=""1000""
GID=""1000""
NO_DEBUG=""true""
END

echo ""Set .env file""

cd ../frontend
sudo rm -rf .env.production

sudo cat >> .env.production << END
GENERATE_SOURCEMAP=true
VITE_API_BASE=""/api""
END

echo ""Set .env.production file""

cd ../docker
sudo docker-compose up -d --build
echo ""Container ID: $(sudo docker ps --latest --quiet)""

sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) mkdir -p /app/server/storage /app/server/storage/documents /app/server/storage/vector-cache /app/server/storage/lancedb
echo ""Placeholder folders in storage created.""

sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) touch /app/server/storage/anythingllm.db
echo ""SQLite DB placeholder set.""

sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) chown -R anythingllm:anythingllm /app/collector /app/server
echo ""File permissions corrected.""

export ONLINE=$(curl -Is http://localhost:3001/api/ping | head -n 1|cut -d$' ' -f2)
echo ""Health check: $ONLINE""

if [ ""$ONLINE"" = 200 ]; then
  echo ""Running migrations..."" && curl -Is http://localhost:3001/api/migrate | head -n 1 | cut -d$' ' -f2
fi

echo ""Setup complete! AnythingLLM instance is now online!""",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4899,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"  VAR VAR-VAR VAR VAR VAR-VAR VAR -VAR VAR.VAR VAR VAR -VAR -VAR VAR VAR VAR -VAR VAR: VAR VAR +VAR /VAR/VAR/VAR/VAR-VAR VAR VAR -VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR-VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR-VAR VAR -VAR VAR VAR VAR VAR: VAR /VAR/VAR-VAR/VAR VAR >> .VAR << VAR ${VAR} VAR=""1000"" VAR=""1000"" VAR=""VAR"" VAR VAR ""VAR .VAR VAR"" VAR ../VAR VAR VAR -VAR .VAR.VAR VAR VAR >> .VAR.VAR << VAR VAR=VAR VAR=""/VAR"" VAR VAR ""VAR .VAR.VAR VAR"" VAR ../VAR VAR VAR-VAR VAR -VAR --VAR VAR ""VAR VAR: $(VAR VAR VAR --VAR --VAR)"" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR /VAR/VAR/VAR /VAR/VAR/VAR/VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR/VAR VAR ""VAR VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR /VAR/VAR/VAR/VAR.VAR VAR ""VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR VAR:VAR /VAR/VAR /VAR/VAR VAR ""VAR VAR VAR."" VAR VAR=$(VAR -VAR VAR: VAR ""VAR VAR: $VAR"" VAR [ ""$VAR"" = 200 ]; VAR VAR ""VAR VAR..."" && VAR -VAR VAR: VAR VAR ""VAR VAR! VAR VAR VAR VAR VAR!"" ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,cloud-deployments/digitalocean/terraform/user_data.tp1,"#!/bin/bash
# check output of userdata script with sudo tail -f /var/log/cloud-init-output.log

sudo apt-get update
sudo apt-get install -y docker.io
sudo usermod -a -G docker ubuntu

curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

sudo systemctl enable docker
sudo systemctl start docker

sudo apt-get install -y git

git clone https://github.com/Mintplex-Labs/anything-llm.git /home/anything-llm
cd /home/anything-llm/docker

cat >> .env << END
${env_content}
UID=""1000""
GID=""1000""
NO_DEBUG=""true""
END

echo ""Set .env file""

cd ../frontend
sudo rm -rf .env.production

sudo cat >> .env.production << END
GENERATE_SOURCEMAP=true
VITE_API_BASE=""/api""
END

echo ""Set .env.production file""

cd ../docker
sudo docker-compose up -d --build
echo ""Container ID: $(sudo docker ps --latest --quiet)""

sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) mkdir -p /app/server/storage /app/server/storage/documents /app/server/storage/vector-cache /app/server/storage/lancedb
echo ""Placeholder folders in storage created.""

sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) touch /app/server/storage/anythingllm.db
echo ""SQLite DB placeholder set.""

sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) chown -R anythingllm:anythingllm /app/collector /app/server
echo ""File permissions corrected.""

export ONLINE=$(curl -Is http://localhost:3001/api/ping | head -n 1|cut -d$' ' -f2)
echo ""Health check: $ONLINE""

if [ ""$ONLINE"" = 200 ]; then
  echo ""Running migrations..."" && curl -Is http://localhost:3001/api/migrate | head -n 1 | cut -d$' ' -f2
fi

echo ""Setup complete! AnythingLLM instance is now online!""",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4898,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"  VAR VAR-VAR VAR VAR VAR-VAR VAR -VAR VAR.VAR VAR VAR -VAR -VAR VAR VAR VAR -VAR VAR: VAR VAR +VAR /VAR/VAR/VAR/VAR-VAR VAR VAR -VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR-VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR-VAR VAR -VAR VAR VAR VAR VAR: VAR /VAR/VAR-VAR/VAR VAR >> .VAR << VAR ${VAR} VAR=""1000"" VAR=""1000"" VAR=""VAR"" VAR VAR ""VAR .VAR VAR"" VAR ../VAR VAR VAR -VAR .VAR.VAR VAR VAR >> .VAR.VAR << VAR VAR=VAR VAR=""/VAR"" VAR VAR ""VAR .VAR.VAR VAR"" VAR ../VAR VAR VAR-VAR VAR -VAR --VAR VAR ""VAR VAR: $(VAR VAR VAR --VAR --VAR)"" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR /VAR/VAR/VAR /VAR/VAR/VAR/VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR/VAR VAR ""VAR VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR /VAR/VAR/VAR/VAR.VAR VAR ""VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR VAR:VAR /VAR/VAR /VAR/VAR VAR ""VAR VAR VAR."" VAR VAR=$(VAR -VAR VAR: VAR ""VAR VAR: $VAR"" VAR [ ""$VAR"" = 200 ]; VAR VAR ""VAR VAR..."" && VAR -VAR VAR: VAR VAR ""VAR VAR! VAR VAR VAR VAR VAR!"" ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,cloud-deployments/gcp/deployment/gcp_deploy_anything_llm.yaml,"resources:
  - name: anything-llm-instance
    type: compute.v1.instance
    properties:
      zone: us-central1-a
      machineType: zones/us-central1-a/machineTypes/n1-standard-1
      disks:
        - deviceName: boot
          type: PERSISTENT
          boot: true
          autoDelete: true
          initializeParams:
            sourceImage: projects/ubuntu-os-cloud/global/images/family/ubuntu-2004-lts
            diskSizeGb: 10
      networkInterfaces:
        - network: global/networks/default
          accessConfigs:
            - name: External NAT
              type: ONE_TO_ONE_NAT
      metadata:
        items:
          - key: startup-script
            value: |
              #!/bin/bash
              # check output of userdata script with sudo tail -f /var/log/cloud-init-output.log

              sudo apt-get update
              sudo apt-get install -y docker.io
              sudo usermod -a -G docker ubuntu

              curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
              sudo chmod +x /usr/local/bin/docker-compose
              sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

              sudo systemctl enable docker
              sudo systemctl start docker

              sudo apt-get install -y git

              git clone https://github.com/Mintplex-Labs/anything-llm.git /home/anything-llm
              cd /home/anything-llm/docker

              cat >> .env << END
              !SUB::USER::CONTENT!
              UID=""1000""
              GID=""1000""
              NO_DEBUG=""true""
              END

              echo ""Set .env file""

              cd ../frontend
              sudo rm -rf .env.production

              sudo cat >> .env.production << END
              GENERATE_SOURCEMAP=true
              VITE_API_BASE=""/api""
              END

              echo ""Set .env.production file""

              cd ../docker
              sudo docker-compose up -d --build
              echo ""Container ID: $(sudo docker ps --latest --quiet)""

              sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) mkdir -p /app/server/storage /app/server/storage/documents /app/server/storage/vector-cache /app/server/storage/lancedb
              echo ""Placeholder folders in storage created.""

              sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) touch /app/server/storage/anythingllm.db
              echo ""SQLite DB placeholder set.""

              sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) chown -R anythingllm:anythingllm /app/collector /app/server
              echo ""File permissions corrected.""

              export ONLINE=$(curl -Is http://localhost:3001/api/ping | head -n 1|cut -d$' ' -f2)
              echo ""Health check: $ONLINE""

              if [ ""$ONLINE"" = 200 ]; then
                echo ""Running migrations..."" && curl -Is http://localhost:3001/api/migrate | head -n 1 | cut -d$' ' -f2
              fi

              echo ""Setup complete! AnythingLLM instance is now online!""
",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4899,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR: - VAR: VAR-VAR-VAR VAR: VAR.VAR.VAR VAR: VAR: VAR-VAR-VAR VAR: VAR/VAR-VAR-VAR/VAR/VAR-VAR-1 VAR: - VAR: VAR VAR: VAR VAR: VAR VAR: VAR VAR: VAR: VAR/VAR-VAR-VAR/VAR/VAR/VAR/VAR-2004-VAR VAR: 10 VAR: - VAR: VAR/VAR/VAR VAR: - VAR: VAR VAR VAR: VAR VAR: VAR: - VAR: VAR-VAR VAR: |   VAR VAR-VAR VAR VAR VAR-VAR VAR -VAR VAR.VAR VAR VAR -VAR -VAR VAR VAR VAR -VAR VAR: VAR VAR +VAR /VAR/VAR/VAR/VAR-VAR VAR VAR -VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR-VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR-VAR VAR -VAR VAR VAR VAR VAR: VAR /VAR/VAR-VAR/VAR VAR >> .VAR << VAR !VAR::VAR::VAR! VAR=""1000"" VAR=""1000"" VAR=""VAR"" VAR VAR ""VAR .VAR VAR"" VAR ../VAR VAR VAR -VAR .VAR.VAR VAR VAR >> .VAR.VAR << VAR VAR=VAR VAR=""/VAR"" VAR VAR ""VAR .VAR.VAR VAR"" VAR ../VAR VAR VAR-VAR VAR -VAR --VAR VAR ""VAR VAR: $(VAR VAR VAR --VAR --VAR)"" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR /VAR/VAR/VAR /VAR/VAR/VAR/VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR/VAR VAR ""VAR VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR /VAR/VAR/VAR/VAR.VAR VAR ""VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR VAR:VAR /VAR/VAR /VAR/VAR VAR ""VAR VAR VAR."" VAR VAR=$(VAR -VAR VAR: VAR ""VAR VAR: $VAR"" VAR [ ""$VAR"" = 200 ]; VAR VAR ""VAR VAR..."" && VAR -VAR VAR: VAR VAR ""VAR VAR! VAR VAR VAR VAR VAR!"" ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,cloud-deployments/gcp/deployment/gcp_deploy_anything_llm.yaml,"resources:
  - name: anything-llm-instance
    type: compute.v1.instance
    properties:
      zone: us-central1-a
      machineType: zones/us-central1-a/machineTypes/n1-standard-1
      disks:
        - deviceName: boot
          type: PERSISTENT
          boot: true
          autoDelete: true
          initializeParams:
            sourceImage: projects/ubuntu-os-cloud/global/images/family/ubuntu-2004-lts
            diskSizeGb: 10
      networkInterfaces:
        - network: global/networks/default
          accessConfigs:
            - name: External NAT
              type: ONE_TO_ONE_NAT
      metadata:
        items:
          - key: startup-script
            value: |
              #!/bin/bash
              # check output of userdata script with sudo tail -f /var/log/cloud-init-output.log

              sudo apt-get update
              sudo apt-get install -y docker.io
              sudo usermod -a -G docker ubuntu

              curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
              sudo chmod +x /usr/local/bin/docker-compose
              sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

              sudo systemctl enable docker
              sudo systemctl start docker

              sudo apt-get install -y git

              git clone https://github.com/Mintplex-Labs/anything-llm.git /home/anything-llm
              cd /home/anything-llm/docker

              cat >> .env << END
              !SUB::USER::CONTENT!
              UID=""1000""
              GID=""1000""
              NO_DEBUG=""true""
              END

              echo ""Set .env file""

              cd ../frontend
              sudo rm -rf .env.production

              sudo cat >> .env.production << END
              GENERATE_SOURCEMAP=true
              VITE_API_BASE=""/api""
              END

              echo ""Set .env.production file""

              cd ../docker
              sudo docker-compose up -d --build
              echo ""Container ID: $(sudo docker ps --latest --quiet)""

              sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) mkdir -p /app/server/storage /app/server/storage/documents /app/server/storage/vector-cache /app/server/storage/lancedb
              echo ""Placeholder folders in storage created.""

              sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) touch /app/server/storage/anythingllm.db
              echo ""SQLite DB placeholder set.""

              sudo docker container exec -u 0 -t $(sudo docker ps --latest --quiet) chown -R anythingllm:anythingllm /app/collector /app/server
              echo ""File permissions corrected.""

              export ONLINE=$(curl -Is http://localhost:3001/api/ping | head -n 1|cut -d$' ' -f2)
              echo ""Health check: $ONLINE""

              if [ ""$ONLINE"" = 200 ]; then
                echo ""Running migrations..."" && curl -Is http://localhost:3001/api/migrate | head -n 1 | cut -d$' ' -f2
              fi

              echo ""Setup complete! AnythingLLM instance is now online!""
",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4898,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR: - VAR: VAR-VAR-VAR VAR: VAR.VAR.VAR VAR: VAR: VAR-VAR-VAR VAR: VAR/VAR-VAR-VAR/VAR/VAR-VAR-1 VAR: - VAR: VAR VAR: VAR VAR: VAR VAR: VAR VAR: VAR: VAR/VAR-VAR-VAR/VAR/VAR/VAR/VAR-2004-VAR VAR: 10 VAR: - VAR: VAR/VAR/VAR VAR: - VAR: VAR VAR VAR: VAR VAR: VAR: - VAR: VAR-VAR VAR: |   VAR VAR-VAR VAR VAR VAR-VAR VAR -VAR VAR.VAR VAR VAR -VAR -VAR VAR VAR VAR -VAR VAR: VAR VAR +VAR /VAR/VAR/VAR/VAR-VAR VAR VAR -VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR-VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR-VAR VAR -VAR VAR VAR VAR VAR: VAR /VAR/VAR-VAR/VAR VAR >> .VAR << VAR !VAR::VAR::VAR! VAR=""1000"" VAR=""1000"" VAR=""VAR"" VAR VAR ""VAR .VAR VAR"" VAR ../VAR VAR VAR -VAR .VAR.VAR VAR VAR >> .VAR.VAR << VAR VAR=VAR VAR=""/VAR"" VAR VAR ""VAR .VAR.VAR VAR"" VAR ../VAR VAR VAR-VAR VAR -VAR --VAR VAR ""VAR VAR: $(VAR VAR VAR --VAR --VAR)"" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR /VAR/VAR/VAR /VAR/VAR/VAR/VAR /VAR/VAR/VAR/VAR-VAR /VAR/VAR/VAR/VAR VAR ""VAR VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR /VAR/VAR/VAR/VAR.VAR VAR ""VAR VAR VAR VAR."" VAR VAR VAR VAR -VAR 0 -VAR $(VAR VAR VAR --VAR --VAR) VAR -VAR VAR:VAR /VAR/VAR /VAR/VAR VAR ""VAR VAR VAR."" VAR VAR=$(VAR -VAR VAR: VAR ""VAR VAR: $VAR"" VAR [ ""$VAR"" = 200 ]; VAR VAR ""VAR VAR..."" && VAR -VAR VAR: VAR VAR ""VAR VAR! VAR VAR VAR VAR VAR!"" ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,docker/.env.example,"SERVER_PORT=3001
CACHE_VECTORS=""true""
# JWT_SECRET=""my-random-string-for-seeding"" # Only needed if AUTH_TOKEN is set. Please generate random string at least 12 chars long.

###########################################
######## LLM API SElECTION ################
###########################################
LLM_PROVIDER='openai'
# OPEN_AI_KEY=
OPEN_MODEL_PREF='gpt-3.5-turbo'

# LLM_PROVIDER='azure'
# AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_KEY=
# OPEN_MODEL_PREF='my-gpt35-deployment' # This is the ""deployment"" on Azure you want to use. Not the base model.
# EMBEDDING_MODEL_PREF='embedder-model' # This is the ""deployment"" on Azure you want to use for embeddings. Not the base model. Valid base model is text-embedding-ada-002


###########################################
######## Vector Database Selection ########
###########################################
# Enable all below if you are using vector database: Chroma.
# VECTOR_DB=""chroma""
# CHROMA_ENDPOINT='http://host.docker.internal:8000'

# Enable all below if you are using vector database: Pinecone.
VECTOR_DB=""pinecone""
PINECONE_ENVIRONMENT=
PINECONE_API_KEY=
PINECONE_INDEX=

# Enable all below if you are using vector database: LanceDB.
# VECTOR_DB=""lancedb""

# Enable all below if you are using vector database: Weaviate.
# VECTOR_DB=""weaviate""
# WEAVIATE_ENDPOINT=""http://localhost:8080""
# WEAVIATE_API_KEY=

# Enable all below if you are using vector database: Qdrant.
# VECTOR_DB=""qdrant""
# QDRANT_ENDPOINT=""http://localhost:6333""
# QDRANT_API_KEY=

# CLOUD DEPLOYMENT VARIRABLES ONLY
# AUTH_TOKEN=""hunter2"" # This is the password to your application if remote hosting.
# NO_DEBUG=""true""
STORAGE_DIR=""./server/storage""
GOOGLE_APIS_KEY=
UID='1000'
GID='1000'",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4899,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR=3001 VAR=""VAR""     VAR='VAR'  VAR='VAR-3.5-VAR'             VAR=""VAR"" VAR= VAR= VAR=              VAR=""./VAR/VAR"" VAR= VAR='1000' VAR='1000' ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,docker/.env.example,"SERVER_PORT=3001
CACHE_VECTORS=""true""
# JWT_SECRET=""my-random-string-for-seeding"" # Only needed if AUTH_TOKEN is set. Please generate random string at least 12 chars long.

###########################################
######## LLM API SElECTION ################
###########################################
LLM_PROVIDER='openai'
# OPEN_AI_KEY=
OPEN_MODEL_PREF='gpt-3.5-turbo'

# LLM_PROVIDER='azure'
# AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_KEY=
# OPEN_MODEL_PREF='my-gpt35-deployment' # This is the ""deployment"" on Azure you want to use. Not the base model.
# EMBEDDING_MODEL_PREF='embedder-model' # This is the ""deployment"" on Azure you want to use for embeddings. Not the base model. Valid base model is text-embedding-ada-002


###########################################
######## Vector Database Selection ########
###########################################
# Enable all below if you are using vector database: Chroma.
# VECTOR_DB=""chroma""
# CHROMA_ENDPOINT='http://host.docker.internal:8000'

# Enable all below if you are using vector database: Pinecone.
VECTOR_DB=""pinecone""
PINECONE_ENVIRONMENT=
PINECONE_API_KEY=
PINECONE_INDEX=

# Enable all below if you are using vector database: LanceDB.
# VECTOR_DB=""lancedb""

# Enable all below if you are using vector database: Weaviate.
# VECTOR_DB=""weaviate""
# WEAVIATE_ENDPOINT=""http://localhost:8080""
# WEAVIATE_API_KEY=

# Enable all below if you are using vector database: Qdrant.
# VECTOR_DB=""qdrant""
# QDRANT_ENDPOINT=""http://localhost:6333""
# QDRANT_API_KEY=

# CLOUD DEPLOYMENT VARIRABLES ONLY
# AUTH_TOKEN=""hunter2"" # This is the password to your application if remote hosting.
# NO_DEBUG=""true""
STORAGE_DIR=""./server/storage""
GOOGLE_APIS_KEY=
UID='1000'
GID='1000'",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4898,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR=3001 VAR=""VAR""     VAR='VAR'  VAR='VAR-3.5-VAR'             VAR=""VAR"" VAR= VAR= VAR=              VAR=""./VAR/VAR"" VAR= VAR='1000' VAR='1000' ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,docker/HOW_TO_USE_DOCKER.md,"# How to use Dockerized Anything LLM

Use the Dockerized version of AnythingLLM for a much faster and complete startup of AnythingLLM.

## Requirements
- Install [Docker](https://www.docker.com/) on your computer or machine.

## How to install
- `git clone` this repo and `cd anything-llm` to get to the root directory.
- `cd docker/`
- `cp .env.example .env` to create the `.env` file.
- Edit `.env` file and update the variables
- `docker-compose up -d --build` to build the image - this will take a few moments.

Your docker host will show the image as online once the build process is completed. This will build the app to `http://localhost:3001`.

## How to use the user interface
- To access the full application, visit `http://localhost:3001` in your browser.

## How to add files to my system
- Upload files from the UI in your Workspace settings

- To run the collector scripts to grab external data (articles, URLs, etc.)
  - `docker exec -it --workdir=/app/collector anything-llm python main.py`

- To run the collector watch script to process files from the hotdir
  - `docker exec -it --workdir=/app/collector anything-llm python watch.py`
  - Upload [compliant files](../collector/hotdir/__HOTDIR__.md) to `./collector/hotdir` and they will be processed and made available in the UI.

## How to update and rebuild the ENV?
- Update the `./docker/.env` and run `docker-compose up -d --build` to rebuild with new environments.

## About UID and GID in the ENV
- The UID and GID are set to 1000 by default. This is the default user in the Docker container and on most host operating systems. If there is a mismatch between your host user UID and GID and what is set in the `.env` file, you may experience permission issues.

## ⚠️ Vector DB support ⚠️
Out of the box, all vector databases are supported. Any vector databases requiring special configuration are listed below.

### Using local ChromaDB with Dockerized AnythingLLM
- Ensure in your `./docker/.env` file that you have
```
#./docker/.env
...other configs

VECTOR_DB=""chroma""
CHROMA_ENDPOINT='http://host.docker.internal:8000' # Allow docker to look on host port, not container.

...other configs

```

## Common questions and fixes

### API is not working, cannot login, LLM is ""offline""?
You are likely running the docker container on a remote machine like EC2 or some other instance where the reachable URL
is not `http://localhost:3001` and instead is something like `http://193.xx.xx.xx:3001` - in this case all you need to do is add the following to your `frontend/.env.production` before running `docker-compose up -d --build`
```
# frontend/.env.production
GENERATE_SOURCEMAP=false
VITE_API_BASE=""http://<YOUR_REACHABLE_IP_ADDRESS>:3001/api""
```
For example, if the docker instance is available on `192.186.1.222` your `VITE_API_BASE` would look like `VITE_API_BASE=""http://192.186.1.222:3001/api""` in `frontend/.env.production`.

### Still not working?
[Ask for help on Discord](https://discord.gg/6UyHPeGZAC)

",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4899,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED," VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR.  - VAR [VAR](VAR:  - `VAR VAR` VAR VAR VAR `VAR VAR-VAR` VAR VAR VAR VAR VAR VAR. - `VAR VAR/` - `VAR .VAR.VAR .VAR` VAR VAR VAR `.VAR` VAR. - VAR `.VAR` VAR VAR VAR VAR VAR - `VAR-VAR VAR -VAR --VAR` VAR VAR VAR VAR - VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR `VAR:  - VAR VAR VAR VAR VAR, VAR `VAR:  - VAR VAR VAR VAR VAR VAR VAR VAR VAR - VAR VAR VAR VAR VAR VAR VAR VAR VAR (VAR, VAR, VAR.) - `VAR VAR -VAR --VAR=/VAR/VAR VAR-VAR VAR VAR.VAR` - VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR - `VAR VAR -VAR --VAR=/VAR/VAR VAR-VAR VAR VAR.VAR` - VAR [VAR VAR](../VAR/VAR/VAR.VAR) VAR `./VAR/VAR` VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR.  - VAR VAR `./VAR/.VAR` VAR VAR `VAR-VAR VAR -VAR --VAR` VAR VAR VAR VAR VAR.  - VAR VAR VAR VAR VAR VAR VAR 1000 VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR `.VAR` VAR, VAR VAR VAR VAR VAR.  VAR VAR VAR VAR, VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR.  - VAR VAR VAR `./VAR/.VAR` VAR VAR VAR VAR ```  ...VAR VAR VAR=""VAR"" VAR='VAR: ...VAR VAR ```   VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR `VAR: ```  VAR=VAR VAR=""VAR: ``` VAR VAR, VAR VAR VAR VAR VAR VAR VAR `192.186.1.222` VAR `VAR` VAR VAR VAR `VAR=""VAR:  [VAR VAR VAR VAR VAR](VAR: ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,docker/HOW_TO_USE_DOCKER.md,"# How to use Dockerized Anything LLM

Use the Dockerized version of AnythingLLM for a much faster and complete startup of AnythingLLM.

## Requirements
- Install [Docker](https://www.docker.com/) on your computer or machine.

## How to install
- `git clone` this repo and `cd anything-llm` to get to the root directory.
- `cd docker/`
- `cp .env.example .env` to create the `.env` file.
- Edit `.env` file and update the variables
- `docker-compose up -d --build` to build the image - this will take a few moments.

Your docker host will show the image as online once the build process is completed. This will build the app to `http://localhost:3001`.

## How to use the user interface
- To access the full application, visit `http://localhost:3001` in your browser.

## How to add files to my system
- Upload files from the UI in your Workspace settings

- To run the collector scripts to grab external data (articles, URLs, etc.)
  - `docker exec -it --workdir=/app/collector anything-llm python main.py`

- To run the collector watch script to process files from the hotdir
  - `docker exec -it --workdir=/app/collector anything-llm python watch.py`
  - Upload [compliant files](../collector/hotdir/__HOTDIR__.md) to `./collector/hotdir` and they will be processed and made available in the UI.

## How to update and rebuild the ENV?
- Update the `./docker/.env` and run `docker-compose up -d --build` to rebuild with new environments.

## About UID and GID in the ENV
- The UID and GID are set to 1000 by default. This is the default user in the Docker container and on most host operating systems. If there is a mismatch between your host user UID and GID and what is set in the `.env` file, you may experience permission issues.

## ⚠️ Vector DB support ⚠️
Out of the box, all vector databases are supported. Any vector databases requiring special configuration are listed below.

### Using local ChromaDB with Dockerized AnythingLLM
- Ensure in your `./docker/.env` file that you have
```
#./docker/.env
...other configs

VECTOR_DB=""chroma""
CHROMA_ENDPOINT='http://host.docker.internal:8000' # Allow docker to look on host port, not container.

...other configs

```

## Common questions and fixes

### API is not working, cannot login, LLM is ""offline""?
You are likely running the docker container on a remote machine like EC2 or some other instance where the reachable URL
is not `http://localhost:3001` and instead is something like `http://193.xx.xx.xx:3001` - in this case all you need to do is add the following to your `frontend/.env.production` before running `docker-compose up -d --build`
```
# frontend/.env.production
GENERATE_SOURCEMAP=false
VITE_API_BASE=""http://<YOUR_REACHABLE_IP_ADDRESS>:3001/api""
```
For example, if the docker instance is available on `192.186.1.222` your `VITE_API_BASE` would look like `VITE_API_BASE=""http://192.186.1.222:3001/api""` in `frontend/.env.production`.

### Still not working?
[Ask for help on Discord](https://discord.gg/6UyHPeGZAC)

",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4898,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED," VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR.  - VAR [VAR](VAR:  - `VAR VAR` VAR VAR VAR `VAR VAR-VAR` VAR VAR VAR VAR VAR VAR. - `VAR VAR/` - `VAR .VAR.VAR .VAR` VAR VAR VAR `.VAR` VAR. - VAR `.VAR` VAR VAR VAR VAR VAR - `VAR-VAR VAR -VAR --VAR` VAR VAR VAR VAR - VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR `VAR:  - VAR VAR VAR VAR VAR, VAR `VAR:  - VAR VAR VAR VAR VAR VAR VAR VAR VAR - VAR VAR VAR VAR VAR VAR VAR VAR VAR (VAR, VAR, VAR.) - `VAR VAR -VAR --VAR=/VAR/VAR VAR-VAR VAR VAR.VAR` - VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR - `VAR VAR -VAR --VAR=/VAR/VAR VAR-VAR VAR VAR.VAR` - VAR [VAR VAR](../VAR/VAR/VAR.VAR) VAR `./VAR/VAR` VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR.  - VAR VAR `./VAR/.VAR` VAR VAR `VAR-VAR VAR -VAR --VAR` VAR VAR VAR VAR VAR.  - VAR VAR VAR VAR VAR VAR VAR 1000 VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR `.VAR` VAR, VAR VAR VAR VAR VAR.  VAR VAR VAR VAR, VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR.  - VAR VAR VAR `./VAR/.VAR` VAR VAR VAR VAR ```  ...VAR VAR VAR=""VAR"" VAR='VAR: ...VAR VAR ```   VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR `VAR: ```  VAR=VAR VAR=""VAR: ``` VAR VAR, VAR VAR VAR VAR VAR VAR VAR `192.186.1.222` VAR `VAR` VAR VAR VAR `VAR=""VAR:  [VAR VAR VAR VAR VAR](VAR: ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/chroma/index.js,"const { ChromaClient } = require(""chromadb"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");

const Chroma = {
  name: ""Chroma"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""chroma"")
      throw new Error(""Chroma::Invalid ENV settings"");

    const client = new ChromaClient({
      path: process.env.CHROMA_ENDPOINT, // if not set will fallback to localhost:8000
    });

    const isAlive = await client.heartbeat();
    if (!isAlive)
      throw new Error(
        ""ChromaDB::Invalid Heartbeat received - is the instance online?""
      );
    return { client };
  },
  heartbeat: async function () {
    const { client } = await this.connect();
    return { heartbeat: await client.heartbeat() };
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const collections = await client.listCollections();
    var totalVectors = 0;
    for (const collectionObj of collections) {
      const collection = await client
        .getCollection({ name: collectionObj.name })
        .catch(() => null);
      if (!collection) continue;
      totalVectors += await collection.count();
    }
    return totalVectors;
  },
  namespaceCount: async function (_namespace = null) {
    const { client } = await this.connect();
    const namespace = await this.namespace(client, _namespace);
    return namespace?.vectorCount || 0;
  },
  similarityResponse: async function (client, namespace, queryVector) {
    const collection = await client.getCollection({ name: namespace });
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const response = await collection.query({
      queryEmbeddings: queryVector,
      nResults: 4,
    });
    response.ids[0].forEach((_, i) => {
      result.contextTexts.push(response.documents[0][i]);
      result.sourceDocuments.push(response.metadatas[0][i]);
    });

    return result;
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client
      .getCollection({ name: namespace })
      .catch(() => null);
    if (!collection) return null;

    return {
      ...collection,
      vectorCount: await collection.count(),
    };
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    return await this.namespaceExists(client, namespace);
  },
  namespaceExists: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client
      .getCollection({ name: namespace })
      .catch((e) => {
        console.error(""ChromaDB::namespaceExists"", e.message);
        return null;
      });
    return !!collection;
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    await client.deleteCollection({ name: namespace });
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const collection = await client.getOrCreateCollection({
          name: namespace,
          metadata: { ""hnsw:space"": ""cosine"" },
        });
        const { chunks } = cacheResult;
        const documentVectors = [];

        for (const chunk of chunks) {
          const submission = {
            ids: [],
            embeddings: [],
            metadatas: [],
            documents: [],
          };

          // Before sending to Chroma and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const { id: _id, ...metadata } = chunk.metadata;
            documentVectors.push({ docId, vectorId: id });
            submission.ids.push(id);
            submission.embeddings.push(chunk.values);
            submission.metadatas.push(metadata);
            submission.documents.push(metadata.text);
          });

          const additionResult = await collection.add(submission);
          if (!additionResult)
            throw new Error(""Error embedding into ChromaDB"", additionResult);
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `Chroma.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);
      const submission = {
        ids: [],
        embeddings: [],
        metadatas: [],
        documents: [],
      };

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            values: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            metadata: { ...metadata, text: textChunks[i] },
          };

          submission.ids.push(vectorRecord.id);
          submission.embeddings.push(vectorRecord.values);
          submission.metadatas.push(metadata);
          submission.documents.push(textChunks[i]);

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      const { client } = await this.connect();
      const collection = await client.getOrCreateCollection({
        name: namespace,
        metadata: { ""hnsw:space"": ""cosine"" },
      });

      if (vectors.length > 0) {
        const chunks = [];

        console.log(""Inserting vectorized chunks into Chroma collection."");
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        const additionResult = await collection.add(submission);
        if (!additionResult)
          throw new Error(""Error embedding into ChromaDB"", additionResult);

        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) return;
    const collection = await client.getCollection({
      name: namespace,
    });

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    const vectorIds = knownDocuments.map((doc) => doc.vectorId);
    await collection.delete({ ids: vectorIds });

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    // When we roll out own response we have separate metadata and texts,
    // so for source collection we need to combine them.
    const sources = sourceDocuments.map((metadata, i) => {
      return { metadata: { ...metadata, text: contextTexts[i] } };
    });
    return {
      response: responseText,
      sources: this.curateSources(sources),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    // When we roll out own response we have separate metadata and texts,
    // so for source collection we need to combine them.
    const sources = sourceDocuments.map((metadata, i) => {
      return { metadata: { ...metadata, text: contextTexts[i] } };
    });
    return {
      response: responseText,
      sources: this.curateSources(sources),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    const details = await this.namespace(client, namespace);
    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${namespace} was deleted along with ${details?.vectorCount} vectors.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    await client.reset();
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      const { metadata = {} } = source;
      if (Object.keys(metadata).length > 0) {
        documents.push({
          ...metadata,
          ...(source.hasOwnProperty(""pageContent"")
            ? { text: source.pageContent }
            : {}),
        });
      }
    }

    return documents;
  },
};

module.exports.Chroma = Chroma;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4899,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR } = VAR(""VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR({ VAR: VAR.VAR.VAR,  }); VAR VAR = VAR VAR.VAR(); VAR (!VAR) VAR VAR VAR( ""VAR::VAR VAR VAR - VAR VAR VAR VAR?"" ); VAR { VAR }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR { VAR: VAR VAR.VAR() }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR VAR = VAR VAR .VAR({ VAR: VAR.VAR }) .VAR(() => VAR); VAR (!VAR) VAR; VAR += VAR VAR.VAR(); } VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR?.VAR || 0; }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = VAR VAR.VAR({ VAR: VAR }); VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR({ VAR: VAR, VAR: 4, }); VAR.VAR[0].VAR((VAR, VAR) => { VAR.VAR.VAR(VAR.VAR[0][VAR]); VAR.VAR.VAR(VAR.VAR[0][VAR]); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR .VAR({ VAR: VAR }) .VAR(() => VAR); VAR (!VAR) VAR VAR; VAR { ...VAR, VAR: VAR VAR.VAR(), }; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR VAR.VAR(VAR, VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR .VAR({ VAR: VAR }) .VAR((VAR) => { VAR.VAR(""VAR::VAR"", VAR.VAR); VAR VAR; }); VAR !!VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR({ VAR: VAR }); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR({ VAR: VAR, VAR: { ""VAR:VAR"": ""VAR"" }, }); VAR { VAR } = VAR; VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR VAR = { VAR: [], VAR: [], VAR: [], VAR: [], };   VAR.VAR((VAR) => { VAR VAR = VAR(); VAR { VAR: VAR, ...VAR } = VAR.VAR; VAR.VAR({ VAR, VAR: VAR }); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR.VAR); }); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR"", VAR); } VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], VAR: [], VAR: [], }; VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR[VAR]); VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR({ VAR: VAR, VAR: { ""VAR:VAR"": ""VAR"" }, }); VAR (VAR.VAR > 0) { VAR VAR = []; VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR"", VAR); VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR({ VAR: VAR, }); VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR({ VAR: VAR }); VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, });   VAR VAR = VAR.VAR((VAR, VAR) => { VAR { VAR: { ...VAR, VAR: VAR[VAR] } }; }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, });   VAR VAR = VAR.VAR((VAR, VAR) => { VAR { VAR: { ...VAR, VAR: VAR[VAR] } }; }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR VAR VAR ${VAR?.VAR} VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(); VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR { VAR = {} } = VAR; VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, ...(VAR.VAR(""VAR"") ? { VAR: VAR.VAR } : {}), }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/chroma/index.js,"const { ChromaClient } = require(""chromadb"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");

const Chroma = {
  name: ""Chroma"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""chroma"")
      throw new Error(""Chroma::Invalid ENV settings"");

    const client = new ChromaClient({
      path: process.env.CHROMA_ENDPOINT, // if not set will fallback to localhost:8000
    });

    const isAlive = await client.heartbeat();
    if (!isAlive)
      throw new Error(
        ""ChromaDB::Invalid Heartbeat received - is the instance online?""
      );
    return { client };
  },
  heartbeat: async function () {
    const { client } = await this.connect();
    return { heartbeat: await client.heartbeat() };
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const collections = await client.listCollections();
    var totalVectors = 0;
    for (const collectionObj of collections) {
      const collection = await client
        .getCollection({ name: collectionObj.name })
        .catch(() => null);
      if (!collection) continue;
      totalVectors += await collection.count();
    }
    return totalVectors;
  },
  namespaceCount: async function (_namespace = null) {
    const { client } = await this.connect();
    const namespace = await this.namespace(client, _namespace);
    return namespace?.vectorCount || 0;
  },
  similarityResponse: async function (client, namespace, queryVector) {
    const collection = await client.getCollection({ name: namespace });
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const response = await collection.query({
      queryEmbeddings: queryVector,
      nResults: 4,
    });
    response.ids[0].forEach((_, i) => {
      result.contextTexts.push(response.documents[0][i]);
      result.sourceDocuments.push(response.metadatas[0][i]);
    });

    return result;
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client
      .getCollection({ name: namespace })
      .catch(() => null);
    if (!collection) return null;

    return {
      ...collection,
      vectorCount: await collection.count(),
    };
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    return await this.namespaceExists(client, namespace);
  },
  namespaceExists: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client
      .getCollection({ name: namespace })
      .catch((e) => {
        console.error(""ChromaDB::namespaceExists"", e.message);
        return null;
      });
    return !!collection;
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    await client.deleteCollection({ name: namespace });
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const collection = await client.getOrCreateCollection({
          name: namespace,
          metadata: { ""hnsw:space"": ""cosine"" },
        });
        const { chunks } = cacheResult;
        const documentVectors = [];

        for (const chunk of chunks) {
          const submission = {
            ids: [],
            embeddings: [],
            metadatas: [],
            documents: [],
          };

          // Before sending to Chroma and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const { id: _id, ...metadata } = chunk.metadata;
            documentVectors.push({ docId, vectorId: id });
            submission.ids.push(id);
            submission.embeddings.push(chunk.values);
            submission.metadatas.push(metadata);
            submission.documents.push(metadata.text);
          });

          const additionResult = await collection.add(submission);
          if (!additionResult)
            throw new Error(""Error embedding into ChromaDB"", additionResult);
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `Chroma.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);
      const submission = {
        ids: [],
        embeddings: [],
        metadatas: [],
        documents: [],
      };

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            values: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            metadata: { ...metadata, text: textChunks[i] },
          };

          submission.ids.push(vectorRecord.id);
          submission.embeddings.push(vectorRecord.values);
          submission.metadatas.push(metadata);
          submission.documents.push(textChunks[i]);

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      const { client } = await this.connect();
      const collection = await client.getOrCreateCollection({
        name: namespace,
        metadata: { ""hnsw:space"": ""cosine"" },
      });

      if (vectors.length > 0) {
        const chunks = [];

        console.log(""Inserting vectorized chunks into Chroma collection."");
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        const additionResult = await collection.add(submission);
        if (!additionResult)
          throw new Error(""Error embedding into ChromaDB"", additionResult);

        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) return;
    const collection = await client.getCollection({
      name: namespace,
    });

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    const vectorIds = knownDocuments.map((doc) => doc.vectorId);
    await collection.delete({ ids: vectorIds });

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    // When we roll out own response we have separate metadata and texts,
    // so for source collection we need to combine them.
    const sources = sourceDocuments.map((metadata, i) => {
      return { metadata: { ...metadata, text: contextTexts[i] } };
    });
    return {
      response: responseText,
      sources: this.curateSources(sources),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    // When we roll out own response we have separate metadata and texts,
    // so for source collection we need to combine them.
    const sources = sourceDocuments.map((metadata, i) => {
      return { metadata: { ...metadata, text: contextTexts[i] } };
    });
    return {
      response: responseText,
      sources: this.curateSources(sources),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    const details = await this.namespace(client, namespace);
    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${namespace} was deleted along with ${details?.vectorCount} vectors.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    await client.reset();
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      const { metadata = {} } = source;
      if (Object.keys(metadata).length > 0) {
        documents.push({
          ...metadata,
          ...(source.hasOwnProperty(""pageContent"")
            ? { text: source.pageContent }
            : {}),
        });
      }
    }

    return documents;
  },
};

module.exports.Chroma = Chroma;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4898,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR } = VAR(""VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR({ VAR: VAR.VAR.VAR,  }); VAR VAR = VAR VAR.VAR(); VAR (!VAR) VAR VAR VAR( ""VAR::VAR VAR VAR - VAR VAR VAR VAR?"" ); VAR { VAR }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR { VAR: VAR VAR.VAR() }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR VAR = VAR VAR .VAR({ VAR: VAR.VAR }) .VAR(() => VAR); VAR (!VAR) VAR; VAR += VAR VAR.VAR(); } VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR?.VAR || 0; }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = VAR VAR.VAR({ VAR: VAR }); VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR({ VAR: VAR, VAR: 4, }); VAR.VAR[0].VAR((VAR, VAR) => { VAR.VAR.VAR(VAR.VAR[0][VAR]); VAR.VAR.VAR(VAR.VAR[0][VAR]); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR .VAR({ VAR: VAR }) .VAR(() => VAR); VAR (!VAR) VAR VAR; VAR { ...VAR, VAR: VAR VAR.VAR(), }; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR VAR.VAR(VAR, VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR .VAR({ VAR: VAR }) .VAR((VAR) => { VAR.VAR(""VAR::VAR"", VAR.VAR); VAR VAR; }); VAR !!VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR({ VAR: VAR }); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR({ VAR: VAR, VAR: { ""VAR:VAR"": ""VAR"" }, }); VAR { VAR } = VAR; VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR VAR = { VAR: [], VAR: [], VAR: [], VAR: [], };   VAR.VAR((VAR) => { VAR VAR = VAR(); VAR { VAR: VAR, ...VAR } = VAR.VAR; VAR.VAR({ VAR, VAR: VAR }); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR.VAR); }); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR"", VAR); } VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], VAR: [], VAR: [], }; VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR[VAR]); VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR({ VAR: VAR, VAR: { ""VAR:VAR"": ""VAR"" }, }); VAR (VAR.VAR > 0) { VAR VAR = []; VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR"", VAR); VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR({ VAR: VAR, }); VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR({ VAR: VAR }); VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, });   VAR VAR = VAR.VAR((VAR, VAR) => { VAR { VAR: { ...VAR, VAR: VAR[VAR] } }; }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, });   VAR VAR = VAR.VAR((VAR, VAR) => { VAR { VAR: { ...VAR, VAR: VAR[VAR] } }; }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR VAR VAR ${VAR?.VAR} VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(); VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR { VAR = {} } = VAR; VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, ...(VAR.VAR(""VAR"") ? { VAR: VAR.VAR } : {}), }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/lance/index.js,"const lancedb = require(""vectordb"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { OpenAIEmbeddings } = require(""langchain/embeddings/openai"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { chatPrompt } = require(""../../chats"");

const LanceDb = {
  uri: `${
    !!process.env.STORAGE_DIR ? `${process.env.STORAGE_DIR}/` : ""./storage/""
  }lancedb`,
  name: ""LanceDb"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""lancedb"")
      throw new Error(""LanceDB::Invalid ENV settings"");

    const client = await lancedb.connect(this.uri);
    return { client };
  },
  heartbeat: async function () {
    await this.connect();
    return { heartbeat: Number(new Date()) };
  },
  tables: async function () {
    const fs = require(""fs"");
    const { client } = await this.connect();
    const dirs = fs.readdirSync(client.uri);
    return dirs.map((folder) => folder.replace("".lance"", """"));
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const tables = await this.tables();
    let count = 0;
    for (const tableName of tables) {
      const table = await client.openTable(tableName);
      count += await table.countRows();
    }
    return count;
  },
  namespaceCount: async function (_namespace = null) {
    const { client } = await this.connect();
    const exists = await this.namespaceExists(client, _namespace);
    if (!exists) return 0;

    const table = await client.openTable(_namespace);
    return (await table.countRows()) || 0;
  },
  embedder: function () {
    return new OpenAIEmbeddings({ openAIApiKey: process.env.OPEN_AI_KEY });
  },
  similarityResponse: async function (client, namespace, queryVector) {
    const collection = await client.openTable(namespace);
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const response = await collection
      .search(queryVector)
      .metricType(""cosine"")
      .limit(5)
      .execute();

    response.forEach((item) => {
      const { vector: _, ...rest } = item;
      result.contextTexts.push(rest.text);
      result.sourceDocuments.push(rest);
    });

    return result;
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client.openTable(namespace).catch(() => false);
    if (!collection) return null;

    return {
      ...collection,
    };
  },
  updateOrCreateCollection: async function (client, data = [], namespace) {
    const hasNamespace = await this.hasNamespace(namespace);
    if (hasNamespace) {
      const collection = await client.openTable(namespace);
      await collection.add(data);
      return true;
    }

    await client.createTable(namespace, data);
    return true;
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    const exists = await this.namespaceExists(client, namespace);
    return exists;
  },
  namespaceExists: async function (_client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collections = await this.tables();
    return collections.includes(namespace);
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    const fs = require(""fs"");
    fs.rm(`${client.uri}/${namespace}.lance`, { recursive: true }, () => null);
    return true;
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { client } = await this.connect();
    const exists = await this.namespaceExists(client, namespace);
    if (!exists) {
      console.error(
        `LanceDB:deleteDocumentFromNamespace - namespace ${namespace} does not exist.`
      );
      return;
    }

    const { DocumentVectors } = require(""../../../models/vectors"");
    const table = await client.openTable(namespace);
    const vectorIds = (await DocumentVectors.where(`docId = '${docId}'`)).map(
      (record) => record.vectorId
    );

    await table.delete(`id IN (${vectorIds.map((v) => `'${v}'`).join("","")})`);
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const { chunks } = cacheResult;
        const documentVectors = [];
        const submissions = [];

        for (const chunk of chunks) {
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const { id: _id, ...metadata } = chunk.metadata;
            documentVectors.push({ docId, vectorId: id });
            submissions.push({ id: id, vector: chunk.values, ...metadata });
          });
        }

        await this.updateOrCreateCollection(client, submissions, namespace);
        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `xyz.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const submissions = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            values: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            metadata: { ...metadata, text: textChunks[i] },
          };

          vectors.push(vectorRecord);
          submissions.push({
            id: vectorRecord.id,
            vector: vectorRecord.values,
            ...vectorRecord.metadata,
          });
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      if (vectors.length > 0) {
        const chunks = [];
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        console.log(""Inserting vectorized chunks into LanceDB collection."");
        const { client } = await this.connect();
        await this.updateOrCreateCollection(client, submissions, namespace);
        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${namespace} was deleted.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    const fs = require(""fs"");
    fs.rm(`${client.uri}`, { recursive: true }, () => null);
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      const { text, vector: _v, score: _s, ...metadata } = source;
      if (Object.keys(metadata).length > 0) {
        documents.push({ ...metadata, text });
      }
    }

    return documents;
  },
};

module.exports.LanceDb = LanceDb;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4899,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR VAR = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""VAR/VAR/VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: `${ !!VAR.VAR.VAR ? `${VAR.VAR.VAR}/` : ""./VAR/"" }VAR`, VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR.VAR(VAR.VAR); VAR { VAR }; }, VAR: VAR VAR () { VAR VAR.VAR(); VAR { VAR: VAR(VAR VAR()) }; }, VAR: VAR VAR () { VAR VAR = VAR(""VAR""); VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR.VAR(VAR.VAR); VAR VAR.VAR((VAR) => VAR.VAR("".VAR"", """")); }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR += VAR VAR.VAR(); } VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) VAR 0; VAR VAR = VAR VAR.VAR(VAR); VAR (VAR VAR.VAR()) || 0; }, VAR: VAR () { VAR VAR VAR({ VAR: VAR.VAR.VAR }); }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR .VAR(VAR) .VAR(""VAR"") .VAR(5) .VAR(); VAR.VAR((VAR) => { VAR { VAR: VAR, ...VAR } = VAR; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR).VAR(() => VAR); VAR (!VAR) VAR VAR; VAR { ...VAR, }; }, VAR: VAR VAR (VAR, VAR = [], VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR (VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR VAR.VAR(VAR); VAR VAR; } VAR VAR.VAR(VAR, VAR); VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(); VAR VAR.VAR(VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR = VAR(""VAR""); VAR.VAR(`${VAR.VAR}/${VAR}.VAR`, { VAR: VAR }, () => VAR); VAR VAR; }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) { VAR.VAR( `VAR:VAR - VAR ${VAR} VAR VAR VAR.` ); VAR; } VAR { VAR } = VAR(""../../../VAR/VAR""); VAR VAR = VAR VAR.VAR(VAR); VAR VAR = (VAR VAR.VAR(`VAR = '${VAR}'`)).VAR( (VAR) => VAR.VAR ); VAR VAR.VAR(`VAR VAR (${VAR.VAR((VAR) => `'${VAR}'`).VAR("","")})`); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR; VAR VAR = []; VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR.VAR((VAR) => { VAR VAR = VAR(); VAR { VAR: VAR, ...VAR } = VAR.VAR; VAR.VAR({ VAR, VAR: VAR }); VAR.VAR({ VAR: VAR, VAR: VAR.VAR, ...VAR }); }); } VAR VAR.VAR(VAR, VAR, VAR); VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR(VAR); VAR.VAR({ VAR: VAR.VAR, VAR: VAR.VAR, ...VAR.VAR, }); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR (VAR.VAR > 0) { VAR VAR = []; VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR, VAR, VAR); VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR(""VAR""); VAR.VAR(`${VAR.VAR}`, { VAR: VAR }, () => VAR); VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR { VAR, VAR: VAR, VAR: VAR, ...VAR } = VAR; VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, VAR }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/lance/index.js,"const lancedb = require(""vectordb"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { OpenAIEmbeddings } = require(""langchain/embeddings/openai"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { chatPrompt } = require(""../../chats"");

const LanceDb = {
  uri: `${
    !!process.env.STORAGE_DIR ? `${process.env.STORAGE_DIR}/` : ""./storage/""
  }lancedb`,
  name: ""LanceDb"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""lancedb"")
      throw new Error(""LanceDB::Invalid ENV settings"");

    const client = await lancedb.connect(this.uri);
    return { client };
  },
  heartbeat: async function () {
    await this.connect();
    return { heartbeat: Number(new Date()) };
  },
  tables: async function () {
    const fs = require(""fs"");
    const { client } = await this.connect();
    const dirs = fs.readdirSync(client.uri);
    return dirs.map((folder) => folder.replace("".lance"", """"));
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const tables = await this.tables();
    let count = 0;
    for (const tableName of tables) {
      const table = await client.openTable(tableName);
      count += await table.countRows();
    }
    return count;
  },
  namespaceCount: async function (_namespace = null) {
    const { client } = await this.connect();
    const exists = await this.namespaceExists(client, _namespace);
    if (!exists) return 0;

    const table = await client.openTable(_namespace);
    return (await table.countRows()) || 0;
  },
  embedder: function () {
    return new OpenAIEmbeddings({ openAIApiKey: process.env.OPEN_AI_KEY });
  },
  similarityResponse: async function (client, namespace, queryVector) {
    const collection = await client.openTable(namespace);
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const response = await collection
      .search(queryVector)
      .metricType(""cosine"")
      .limit(5)
      .execute();

    response.forEach((item) => {
      const { vector: _, ...rest } = item;
      result.contextTexts.push(rest.text);
      result.sourceDocuments.push(rest);
    });

    return result;
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client.openTable(namespace).catch(() => false);
    if (!collection) return null;

    return {
      ...collection,
    };
  },
  updateOrCreateCollection: async function (client, data = [], namespace) {
    const hasNamespace = await this.hasNamespace(namespace);
    if (hasNamespace) {
      const collection = await client.openTable(namespace);
      await collection.add(data);
      return true;
    }

    await client.createTable(namespace, data);
    return true;
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    const exists = await this.namespaceExists(client, namespace);
    return exists;
  },
  namespaceExists: async function (_client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collections = await this.tables();
    return collections.includes(namespace);
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    const fs = require(""fs"");
    fs.rm(`${client.uri}/${namespace}.lance`, { recursive: true }, () => null);
    return true;
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { client } = await this.connect();
    const exists = await this.namespaceExists(client, namespace);
    if (!exists) {
      console.error(
        `LanceDB:deleteDocumentFromNamespace - namespace ${namespace} does not exist.`
      );
      return;
    }

    const { DocumentVectors } = require(""../../../models/vectors"");
    const table = await client.openTable(namespace);
    const vectorIds = (await DocumentVectors.where(`docId = '${docId}'`)).map(
      (record) => record.vectorId
    );

    await table.delete(`id IN (${vectorIds.map((v) => `'${v}'`).join("","")})`);
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const { chunks } = cacheResult;
        const documentVectors = [];
        const submissions = [];

        for (const chunk of chunks) {
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const { id: _id, ...metadata } = chunk.metadata;
            documentVectors.push({ docId, vectorId: id });
            submissions.push({ id: id, vector: chunk.values, ...metadata });
          });
        }

        await this.updateOrCreateCollection(client, submissions, namespace);
        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `xyz.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const submissions = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            values: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            metadata: { ...metadata, text: textChunks[i] },
          };

          vectors.push(vectorRecord);
          submissions.push({
            id: vectorRecord.id,
            vector: vectorRecord.values,
            ...vectorRecord.metadata,
          });
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      if (vectors.length > 0) {
        const chunks = [];
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        console.log(""Inserting vectorized chunks into LanceDB collection."");
        const { client } = await this.connect();
        await this.updateOrCreateCollection(client, submissions, namespace);
        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${namespace} was deleted.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    const fs = require(""fs"");
    fs.rm(`${client.uri}`, { recursive: true }, () => null);
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      const { text, vector: _v, score: _s, ...metadata } = source;
      if (Object.keys(metadata).length > 0) {
        documents.push({ ...metadata, text });
      }
    }

    return documents;
  },
};

module.exports.LanceDb = LanceDb;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4898,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR VAR = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""VAR/VAR/VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: `${ !!VAR.VAR.VAR ? `${VAR.VAR.VAR}/` : ""./VAR/"" }VAR`, VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR.VAR(VAR.VAR); VAR { VAR }; }, VAR: VAR VAR () { VAR VAR.VAR(); VAR { VAR: VAR(VAR VAR()) }; }, VAR: VAR VAR () { VAR VAR = VAR(""VAR""); VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR.VAR(VAR.VAR); VAR VAR.VAR((VAR) => VAR.VAR("".VAR"", """")); }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR += VAR VAR.VAR(); } VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) VAR 0; VAR VAR = VAR VAR.VAR(VAR); VAR (VAR VAR.VAR()) || 0; }, VAR: VAR () { VAR VAR VAR({ VAR: VAR.VAR.VAR }); }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR .VAR(VAR) .VAR(""VAR"") .VAR(5) .VAR(); VAR.VAR((VAR) => { VAR { VAR: VAR, ...VAR } = VAR; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR).VAR(() => VAR); VAR (!VAR) VAR VAR; VAR { ...VAR, }; }, VAR: VAR VAR (VAR, VAR = [], VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR (VAR) { VAR VAR = VAR VAR.VAR(VAR); VAR VAR.VAR(VAR); VAR VAR; } VAR VAR.VAR(VAR, VAR); VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(); VAR VAR.VAR(VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR = VAR(""VAR""); VAR.VAR(`${VAR.VAR}/${VAR}.VAR`, { VAR: VAR }, () => VAR); VAR VAR; }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) { VAR.VAR( `VAR:VAR - VAR ${VAR} VAR VAR VAR.` ); VAR; } VAR { VAR } = VAR(""../../../VAR/VAR""); VAR VAR = VAR VAR.VAR(VAR); VAR VAR = (VAR VAR.VAR(`VAR = '${VAR}'`)).VAR( (VAR) => VAR.VAR ); VAR VAR.VAR(`VAR VAR (${VAR.VAR((VAR) => `'${VAR}'`).VAR("","")})`); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR; VAR VAR = []; VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR.VAR((VAR) => { VAR VAR = VAR(); VAR { VAR: VAR, ...VAR } = VAR.VAR; VAR.VAR({ VAR, VAR: VAR }); VAR.VAR({ VAR: VAR, VAR: VAR.VAR, ...VAR }); }); } VAR VAR.VAR(VAR, VAR, VAR); VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR(VAR); VAR.VAR({ VAR: VAR.VAR, VAR: VAR.VAR, ...VAR.VAR, }); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR (VAR.VAR > 0) { VAR VAR = []; VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR, VAR, VAR); VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR(""VAR""); VAR.VAR(`${VAR.VAR}`, { VAR: VAR }, () => VAR); VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR { VAR, VAR: VAR, VAR: VAR, ...VAR } = VAR; VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, VAR }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/pinecone/index.js,"const { PineconeClient } = require(""@pinecone-database/pinecone"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");

const Pinecone = {
  name: ""Pinecone"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""pinecone"")
      throw new Error(""Pinecone::Invalid ENV settings"");

    const client = new PineconeClient();
    await client.init({
      apiKey: process.env.PINECONE_API_KEY,
      environment: process.env.PINECONE_ENVIRONMENT,
    });
    const pineconeIndex = client.Index(process.env.PINECONE_INDEX);
    const { status } = await client.describeIndex({
      indexName: process.env.PINECONE_INDEX,
    });

    if (!status.ready) throw new Error(""Pinecode::Index not ready."");
    return { client, pineconeIndex, indexName: process.env.PINECONE_INDEX };
  },
  totalIndicies: async function () {
    const { pineconeIndex } = await this.connect();
    const { namespaces } = await pineconeIndex.describeIndexStats1();
    return Object.values(namespaces).reduce(
      (a, b) => a + (b?.vectorCount || 0),
      0
    );
  },
  namespaceCount: async function (_namespace = null) {
    const { pineconeIndex } = await this.connect();
    const namespace = await this.namespace(pineconeIndex, _namespace);
    return namespace?.vectorCount || 0;
  },
  similarityResponse: async function (index, namespace, queryVector) {
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };
    const response = await index.query({
      queryRequest: {
        namespace,
        vector: queryVector,
        topK: 4,
        includeMetadata: true,
      },
    });

    response.matches.forEach((match) => {
      result.contextTexts.push(match.metadata.text);
      result.sourceDocuments.push(match);
    });

    return result;
  },
  namespace: async function (index, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const { namespaces } = await index.describeIndexStats1();
    return namespaces.hasOwnProperty(namespace) ? namespaces[namespace] : null;
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { pineconeIndex } = await this.connect();
    return await this.namespaceExists(pineconeIndex, namespace);
  },
  namespaceExists: async function (index, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const { namespaces } = await index.describeIndexStats1();
    return namespaces.hasOwnProperty(namespace);
  },
  deleteVectorsInNamespace: async function (index, namespace = null) {
    await index.delete1({ namespace, deleteAll: true });
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { pineconeIndex } = await this.connect();
        const { chunks } = cacheResult;
        const documentVectors = [];

        for (const chunk of chunks) {
          // Before sending to Pinecone and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          const newChunks = chunk.map((chunk) => {
            const id = uuidv4();
            documentVectors.push({ docId, vectorId: id });
            return { ...chunk, id };
          });

          // Push chunks with new ids to pinecone.
          await pineconeIndex.upsert({
            upsertRequest: {
              vectors: [...newChunks],
              namespace,
            },
          });
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `PineconeStore.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L167
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            values: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            metadata: { ...metadata, text: textChunks[i] },
          };

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      if (vectors.length > 0) {
        const chunks = [];
        const { pineconeIndex } = await this.connect();
        console.log(""Inserting vectorized chunks into Pinecone."");
        for (const chunk of toChunks(vectors, 100)) {
          chunks.push(chunk);
          await pineconeIndex.upsert({
            upsertRequest: {
              vectors: [...chunk],
              namespace,
            },
          });
        }
        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace))) return;

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    const vectorIds = knownDocuments.map((doc) => doc.vectorId);
    for (const batchOfVectorIds of toChunks(vectorIds, 1000)) {
      await pineconeIndex.delete1({
        ids: batchOfVectorIds,
        namespace,
      });
    }

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(pineconeIndex, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    const details = await this.namespace(pineconeIndex, namespace);
    await this.deleteVectorsInNamespace(pineconeIndex, namespace);
    return {
      message: `Namespace ${namespace} was deleted along with ${details.vectorCount} vectors.`,
    };
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      pineconeIndex,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
     Context:
     ${contextTexts
       .map((text, i) => {
         return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
       })
       .join("""")}`,
    };

    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace)))
      throw new Error(
        ""Invalid namespace - has it been collected and seeded yet?""
      );

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      pineconeIndex,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };

    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      const { metadata = {} } = source;
      if (Object.keys(metadata).length > 0) {
        documents.push({
          ...metadata,
          ...(source.hasOwnProperty(""pageContent"")
            ? { text: source.pageContent }
            : {}),
        });
      }
    }

    return documents;
  },
};

module.exports.Pinecone = Pinecone;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4899,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR } = VAR(""@VAR-VAR/VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR(); VAR VAR.VAR({ VAR: VAR.VAR.VAR, VAR: VAR.VAR.VAR, }); VAR VAR = VAR.VAR(VAR.VAR.VAR); VAR { VAR } = VAR VAR.VAR({ VAR: VAR.VAR.VAR, }); VAR (!VAR.VAR) VAR VAR VAR(""VAR::VAR VAR VAR.""); VAR { VAR, VAR, VAR: VAR.VAR.VAR }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR).VAR( (VAR, VAR) => VAR + (VAR?.VAR || 0), 0 ); }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR?.VAR || 0; }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR({ VAR: { VAR, VAR: VAR, VAR: 4, VAR: VAR, }, }); VAR.VAR.VAR((VAR) => { VAR.VAR.VAR(VAR.VAR.VAR); VAR.VAR.VAR(VAR); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR) ? VAR[VAR] : VAR; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR VAR.VAR(VAR, VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR({ VAR, VAR: VAR }); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR; VAR VAR = []; VAR (VAR VAR VAR VAR) {   VAR VAR = VAR.VAR((VAR) => { VAR VAR = VAR(); VAR.VAR({ VAR, VAR: VAR }); VAR { ...VAR, VAR }; });  VAR VAR.VAR({ VAR: { VAR: [...VAR], VAR, }, }); } VAR VAR.VAR(VAR); VAR VAR; }      VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR (VAR.VAR > 0) { VAR VAR = []; VAR { VAR } = VAR VAR.VAR(); VAR.VAR(""VAR VAR VAR VAR VAR.""); VAR (VAR VAR VAR VAR(VAR, 100)) { VAR.VAR(VAR); VAR VAR.VAR({ VAR: { VAR: [...VAR], VAR, }, }); } VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR (VAR VAR VAR VAR(VAR, 1000)) { VAR VAR.VAR({ VAR: VAR, VAR, }); } VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR VAR VAR ${VAR.VAR} VAR.`, }; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR( ""VAR VAR - VAR VAR VAR VAR VAR VAR VAR?"" ); VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR { VAR = {} } = VAR; VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, ...(VAR.VAR(""VAR"") ? { VAR: VAR.VAR } : {}), }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/pinecone/index.js,"const { PineconeClient } = require(""@pinecone-database/pinecone"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");

const Pinecone = {
  name: ""Pinecone"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""pinecone"")
      throw new Error(""Pinecone::Invalid ENV settings"");

    const client = new PineconeClient();
    await client.init({
      apiKey: process.env.PINECONE_API_KEY,
      environment: process.env.PINECONE_ENVIRONMENT,
    });
    const pineconeIndex = client.Index(process.env.PINECONE_INDEX);
    const { status } = await client.describeIndex({
      indexName: process.env.PINECONE_INDEX,
    });

    if (!status.ready) throw new Error(""Pinecode::Index not ready."");
    return { client, pineconeIndex, indexName: process.env.PINECONE_INDEX };
  },
  totalIndicies: async function () {
    const { pineconeIndex } = await this.connect();
    const { namespaces } = await pineconeIndex.describeIndexStats1();
    return Object.values(namespaces).reduce(
      (a, b) => a + (b?.vectorCount || 0),
      0
    );
  },
  namespaceCount: async function (_namespace = null) {
    const { pineconeIndex } = await this.connect();
    const namespace = await this.namespace(pineconeIndex, _namespace);
    return namespace?.vectorCount || 0;
  },
  similarityResponse: async function (index, namespace, queryVector) {
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };
    const response = await index.query({
      queryRequest: {
        namespace,
        vector: queryVector,
        topK: 4,
        includeMetadata: true,
      },
    });

    response.matches.forEach((match) => {
      result.contextTexts.push(match.metadata.text);
      result.sourceDocuments.push(match);
    });

    return result;
  },
  namespace: async function (index, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const { namespaces } = await index.describeIndexStats1();
    return namespaces.hasOwnProperty(namespace) ? namespaces[namespace] : null;
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { pineconeIndex } = await this.connect();
    return await this.namespaceExists(pineconeIndex, namespace);
  },
  namespaceExists: async function (index, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const { namespaces } = await index.describeIndexStats1();
    return namespaces.hasOwnProperty(namespace);
  },
  deleteVectorsInNamespace: async function (index, namespace = null) {
    await index.delete1({ namespace, deleteAll: true });
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { pineconeIndex } = await this.connect();
        const { chunks } = cacheResult;
        const documentVectors = [];

        for (const chunk of chunks) {
          // Before sending to Pinecone and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          const newChunks = chunk.map((chunk) => {
            const id = uuidv4();
            documentVectors.push({ docId, vectorId: id });
            return { ...chunk, id };
          });

          // Push chunks with new ids to pinecone.
          await pineconeIndex.upsert({
            upsertRequest: {
              vectors: [...newChunks],
              namespace,
            },
          });
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `PineconeStore.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L167
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            values: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            metadata: { ...metadata, text: textChunks[i] },
          };

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      if (vectors.length > 0) {
        const chunks = [];
        const { pineconeIndex } = await this.connect();
        console.log(""Inserting vectorized chunks into Pinecone."");
        for (const chunk of toChunks(vectors, 100)) {
          chunks.push(chunk);
          await pineconeIndex.upsert({
            upsertRequest: {
              vectors: [...chunk],
              namespace,
            },
          });
        }
        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace))) return;

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    const vectorIds = knownDocuments.map((doc) => doc.vectorId);
    for (const batchOfVectorIds of toChunks(vectorIds, 1000)) {
      await pineconeIndex.delete1({
        ids: batchOfVectorIds,
        namespace,
      });
    }

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(pineconeIndex, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    const details = await this.namespace(pineconeIndex, namespace);
    await this.deleteVectorsInNamespace(pineconeIndex, namespace);
    return {
      message: `Namespace ${namespace} was deleted along with ${details.vectorCount} vectors.`,
    };
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      pineconeIndex,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
     Context:
     ${contextTexts
       .map((text, i) => {
         return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
       })
       .join("""")}`,
    };

    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { pineconeIndex } = await this.connect();
    if (!(await this.namespaceExists(pineconeIndex, namespace)))
      throw new Error(
        ""Invalid namespace - has it been collected and seeded yet?""
      );

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      pineconeIndex,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };

    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      const { metadata = {} } = source;
      if (Object.keys(metadata).length > 0) {
        documents.push({
          ...metadata,
          ...(source.hasOwnProperty(""pageContent"")
            ? { text: source.pageContent }
            : {}),
        });
      }
    }

    return documents;
  },
};

module.exports.Pinecone = Pinecone;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4898,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR } = VAR(""@VAR-VAR/VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR(); VAR VAR.VAR({ VAR: VAR.VAR.VAR, VAR: VAR.VAR.VAR, }); VAR VAR = VAR.VAR(VAR.VAR.VAR); VAR { VAR } = VAR VAR.VAR({ VAR: VAR.VAR.VAR, }); VAR (!VAR.VAR) VAR VAR VAR(""VAR::VAR VAR VAR.""); VAR { VAR, VAR, VAR: VAR.VAR.VAR }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR).VAR( (VAR, VAR) => VAR + (VAR?.VAR || 0), 0 ); }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR?.VAR || 0; }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR({ VAR: { VAR, VAR: VAR, VAR: 4, VAR: VAR, }, }); VAR.VAR.VAR((VAR) => { VAR.VAR.VAR(VAR.VAR.VAR); VAR.VAR.VAR(VAR); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR) ? VAR[VAR] : VAR; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR VAR.VAR(VAR, VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR { VAR } = VAR VAR.VAR(); VAR VAR.VAR(VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR({ VAR, VAR: VAR }); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR; VAR VAR = []; VAR (VAR VAR VAR VAR) {   VAR VAR = VAR.VAR((VAR) => { VAR VAR = VAR(); VAR.VAR({ VAR, VAR: VAR }); VAR { ...VAR, VAR }; });  VAR VAR.VAR({ VAR: { VAR: [...VAR], VAR, }, }); } VAR VAR.VAR(VAR); VAR VAR; }      VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR (VAR.VAR > 0) { VAR VAR = []; VAR { VAR } = VAR VAR.VAR(); VAR.VAR(""VAR VAR VAR VAR VAR.""); VAR (VAR VAR VAR VAR(VAR, 100)) { VAR.VAR(VAR); VAR VAR.VAR({ VAR: { VAR: [...VAR], VAR, }, }); } VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR (VAR VAR VAR VAR(VAR, 1000)) { VAR VAR.VAR({ VAR: VAR, VAR, }); } VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR VAR VAR ${VAR.VAR} VAR.`, }; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR( ""VAR VAR - VAR VAR VAR VAR VAR VAR VAR?"" ); VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR { VAR = {} } = VAR; VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, ...(VAR.VAR(""VAR"") ? { VAR: VAR.VAR } : {}), }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/qdrant/index.js,"const { QdrantClient } = require(""@qdrant/js-client-rest"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");

const QDrant = {
  name: ""QDrant"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""qdrant"")
      throw new Error(""QDrant::Invalid ENV settings"");

    const client = new QdrantClient({
      url: process.env.QDRANT_ENDPOINT,
      ...(process.env.QDRANT_API_KEY
        ? { apiKey: process.env.QDRANT_API_KEY }
        : {}),
    });

    const isAlive = (await client.api(""cluster"")?.clusterStatus())?.ok || false;
    if (!isAlive)
      throw new Error(
        ""QDrant::Invalid Heartbeat received - is the instance online?""
      );

    return { client };
  },
  heartbeat: async function () {
    await this.connect();
    return { heartbeat: Number(new Date()) };
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const { collections } = await client.getCollections();
    var totalVectors = 0;
    for (const collection of collections) {
      if (!collection || !collection.name) continue;
      totalVectors +=
        (await this.namespace(client, collection.name))?.vectorCount || 0;
    }
    return totalVectors;
  },
  namespaceCount: async function (_namespace = null) {
    const { client } = await this.connect();
    const namespace = await this.namespace(client, _namespace);
    return namespace?.vectorCount || 0;
  },
  similarityResponse: async function (_client, namespace, queryVector) {
    const { client } = await this.connect();
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const responses = await client.search(namespace, {
      vector: queryVector,
      limit: 4,
    });

    responses.forEach((response) => {
      result.contextTexts.push(response?.payload?.text || """");
      result.sourceDocuments.push({
        ...(response?.payload || {}),
        id: response.id,
      });
    });

    return result;
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client.getCollection(namespace).catch(() => null);
    if (!collection) return null;

    return {
      name: namespace,
      ...collection,
      vectorCount: collection.vectors_count,
    };
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    return await this.namespaceExists(client, namespace);
  },
  namespaceExists: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client.getCollection(namespace).catch((e) => {
      console.error(""QDrant::namespaceExists"", e.message);
      return null;
    });
    return !!collection;
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    await client.deleteCollection(namespace);
    return true;
  },
  getOrCreateCollection: async function (client, namespace) {
    if (await this.namespaceExists(client, namespace)) {
      return await client.getCollection(namespace);
    }
    await client.createCollection(namespace, {
      vectors: {
        size: 1536, //TODO: Fixed to OpenAI models - when other embeddings exist make variable.
        distance: ""Cosine"",
      },
    });
    return await client.getCollection(namespace);
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const collection = await this.getOrCreateCollection(client, namespace);
        if (!collection)
          throw new Error(""Failed to create new QDrant collection!"", {
            namespace,
          });

        const { chunks } = cacheResult;
        const documentVectors = [];

        for (const chunk of chunks) {
          const submission = {
            ids: [],
            vectors: [],
            payloads: [],
          };

          // Before sending to Qdrant and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const { id: _id, ...payload } = chunk.payload;
            documentVectors.push({ docId, vectorId: id });
            submission.ids.push(id);
            submission.vectors.push(chunk.vector);
            submission.payloads.push(payload);
          });

          const additionResult = await client.upsert(namespace, {
            wait: true,
            batch: { ...submission },
          });
          if (additionResult?.status !== ""completed"")
            throw new Error(""Error embedding into QDrant"", additionResult);
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `Qdrant.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);
      const submission = {
        ids: [],
        vectors: [],
        payloads: [],
      };

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            vector: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            payload: { ...metadata, text: textChunks[i] },
          };

          submission.ids.push(vectorRecord.id);
          submission.vectors.push(vectorRecord.vector);
          submission.payloads.push(vectorRecord.payload);

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      const { client } = await this.connect();
      const collection = await this.getOrCreateCollection(client, namespace);
      if (!collection)
        throw new Error(""Failed to create new QDrant collection!"", {
          namespace,
        });

      if (vectors.length > 0) {
        const chunks = [];

        console.log(""Inserting vectorized chunks into QDrant collection."");
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        const additionResult = await client.upsert(namespace, {
          wait: true,
          batch: {
            ids: submission.ids,
            vectors: submission.vectors,
            payloads: submission.payloads,
          },
        });
        if (additionResult?.status !== ""completed"")
          throw new Error(""Error embedding into QDrant"", additionResult);

        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) return;

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    const vectorIds = knownDocuments.map((doc) => doc.vectorId);
    await client.delete(namespace, {
      wait: true,
      points: vectorIds,
    });

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    const details = await this.namespace(client, namespace);
    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${namespace} was deleted along with ${details?.vectorCount} vectors.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    const response = await client.getCollections();
    for (const collection of response.collections) {
      await client.deleteCollection(collection.name);
    }
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      if (Object.keys(source).length > 0) {
        documents.push({
          ...source,
        });
      }
    }

    return documents;
  },
};

module.exports.QDrant = QDrant;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4899,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR } = VAR(""@VAR/VAR-VAR-VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR({ VAR: VAR.VAR.VAR, ...(VAR.VAR.VAR ? { VAR: VAR.VAR.VAR } : {}), }); VAR VAR = (VAR VAR.VAR(""VAR"")?.VAR())?.VAR || VAR; VAR (!VAR) VAR VAR VAR( ""VAR::VAR VAR VAR - VAR VAR VAR VAR?"" ); VAR { VAR }; }, VAR: VAR VAR () { VAR VAR.VAR(); VAR { VAR: VAR(VAR VAR()) }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR VAR.VAR(); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR (!VAR || !VAR.VAR) VAR; VAR += (VAR VAR.VAR(VAR, VAR.VAR))?.VAR || 0; } VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR?.VAR || 0; }, VAR: VAR VAR (VAR, VAR, VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR, VAR: 4, }); VAR.VAR((VAR) => { VAR.VAR.VAR(VAR?.VAR?.VAR || """"); VAR.VAR.VAR({ ...(VAR?.VAR || {}), VAR: VAR.VAR, }); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR).VAR(() => VAR); VAR (!VAR) VAR VAR; VAR { VAR: VAR, ...VAR, VAR: VAR.VAR, }; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR VAR.VAR(VAR, VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR).VAR((VAR) => { VAR.VAR(""VAR::VAR"", VAR.VAR); VAR VAR; }); VAR !!VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR, VAR) { VAR (VAR VAR.VAR(VAR, VAR)) { VAR VAR VAR.VAR(VAR); } VAR VAR.VAR(VAR, { VAR: { VAR: 1536,  VAR: ""VAR"", }, }); VAR VAR VAR.VAR(VAR); }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR!"", { VAR, }); VAR { VAR } = VAR; VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR VAR = { VAR: [], VAR: [], VAR: [], };   VAR.VAR((VAR) => { VAR VAR = VAR(); VAR { VAR: VAR, ...VAR } = VAR.VAR; VAR.VAR({ VAR, VAR: VAR }); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); }); VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR, VAR: { ...VAR }, }); VAR (VAR?.VAR !== ""VAR"") VAR VAR VAR(""VAR VAR VAR VAR"", VAR); } VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], VAR: [], }; VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR!"", { VAR, }); VAR (VAR.VAR > 0) { VAR VAR = []; VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR, VAR: { VAR: VAR.VAR, VAR: VAR.VAR, VAR: VAR.VAR, }, }); VAR (VAR?.VAR !== ""VAR"") VAR VAR VAR(""VAR VAR VAR VAR"", VAR); VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR, { VAR: VAR, VAR: VAR, }); VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR VAR VAR ${VAR?.VAR} VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(); VAR (VAR VAR VAR VAR.VAR) { VAR VAR.VAR(VAR.VAR); } VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/qdrant/index.js,"const { QdrantClient } = require(""@qdrant/js-client-rest"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");

const QDrant = {
  name: ""QDrant"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""qdrant"")
      throw new Error(""QDrant::Invalid ENV settings"");

    const client = new QdrantClient({
      url: process.env.QDRANT_ENDPOINT,
      ...(process.env.QDRANT_API_KEY
        ? { apiKey: process.env.QDRANT_API_KEY }
        : {}),
    });

    const isAlive = (await client.api(""cluster"")?.clusterStatus())?.ok || false;
    if (!isAlive)
      throw new Error(
        ""QDrant::Invalid Heartbeat received - is the instance online?""
      );

    return { client };
  },
  heartbeat: async function () {
    await this.connect();
    return { heartbeat: Number(new Date()) };
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const { collections } = await client.getCollections();
    var totalVectors = 0;
    for (const collection of collections) {
      if (!collection || !collection.name) continue;
      totalVectors +=
        (await this.namespace(client, collection.name))?.vectorCount || 0;
    }
    return totalVectors;
  },
  namespaceCount: async function (_namespace = null) {
    const { client } = await this.connect();
    const namespace = await this.namespace(client, _namespace);
    return namespace?.vectorCount || 0;
  },
  similarityResponse: async function (_client, namespace, queryVector) {
    const { client } = await this.connect();
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const responses = await client.search(namespace, {
      vector: queryVector,
      limit: 4,
    });

    responses.forEach((response) => {
      result.contextTexts.push(response?.payload?.text || """");
      result.sourceDocuments.push({
        ...(response?.payload || {}),
        id: response.id,
      });
    });

    return result;
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client.getCollection(namespace).catch(() => null);
    if (!collection) return null;

    return {
      name: namespace,
      ...collection,
      vectorCount: collection.vectors_count,
    };
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    return await this.namespaceExists(client, namespace);
  },
  namespaceExists: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const collection = await client.getCollection(namespace).catch((e) => {
      console.error(""QDrant::namespaceExists"", e.message);
      return null;
    });
    return !!collection;
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    await client.deleteCollection(namespace);
    return true;
  },
  getOrCreateCollection: async function (client, namespace) {
    if (await this.namespaceExists(client, namespace)) {
      return await client.getCollection(namespace);
    }
    await client.createCollection(namespace, {
      vectors: {
        size: 1536, //TODO: Fixed to OpenAI models - when other embeddings exist make variable.
        distance: ""Cosine"",
      },
    });
    return await client.getCollection(namespace);
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const { pageContent, docId, ...metadata } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const collection = await this.getOrCreateCollection(client, namespace);
        if (!collection)
          throw new Error(""Failed to create new QDrant collection!"", {
            namespace,
          });

        const { chunks } = cacheResult;
        const documentVectors = [];

        for (const chunk of chunks) {
          const submission = {
            ids: [],
            vectors: [],
            payloads: [],
          };

          // Before sending to Qdrant and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const { id: _id, ...payload } = chunk.payload;
            documentVectors.push({ docId, vectorId: id });
            submission.ids.push(id);
            submission.vectors.push(chunk.vector);
            submission.payloads.push(payload);
          });

          const additionResult = await client.upsert(namespace, {
            wait: true,
            batch: { ...submission },
          });
          if (additionResult?.status !== ""completed"")
            throw new Error(""Error embedding into QDrant"", additionResult);
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `Qdrant.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);
      const submission = {
        ids: [],
        vectors: [],
        payloads: [],
      };

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const vectorRecord = {
            id: uuidv4(),
            vector: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/2def486af734c0ca87285a48f1a04c057ab74bdf/langchain/src/vectorstores/pinecone.ts#L64
            payload: { ...metadata, text: textChunks[i] },
          };

          submission.ids.push(vectorRecord.id);
          submission.vectors.push(vectorRecord.vector);
          submission.payloads.push(vectorRecord.payload);

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      const { client } = await this.connect();
      const collection = await this.getOrCreateCollection(client, namespace);
      if (!collection)
        throw new Error(""Failed to create new QDrant collection!"", {
          namespace,
        });

      if (vectors.length > 0) {
        const chunks = [];

        console.log(""Inserting vectorized chunks into QDrant collection."");
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        const additionResult = await client.upsert(namespace, {
          wait: true,
          batch: {
            ids: submission.ids,
            vectors: submission.vectors,
            payloads: submission.payloads,
          },
        });
        if (additionResult?.status !== ""completed"")
          throw new Error(""Error embedding into QDrant"", additionResult);

        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) return;

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    const vectorIds = knownDocuments.map((doc) => doc.vectorId);
    await client.delete(namespace, {
      wait: true,
      points: vectorIds,
    });

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace)))
      throw new Error(""Namespace by that name does not exist."");

    const details = await this.namespace(client, namespace);
    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${namespace} was deleted along with ${details?.vectorCount} vectors.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    const response = await client.getCollections();
    for (const collection of response.collections) {
      await client.deleteCollection(collection.name);
    }
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      if (Object.keys(source).length > 0) {
        documents.push({
          ...source,
        });
      }
    }

    return documents;
  },
};

module.exports.QDrant = QDrant;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4898,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR } = VAR(""@VAR/VAR-VAR-VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR({ VAR: VAR.VAR.VAR, ...(VAR.VAR.VAR ? { VAR: VAR.VAR.VAR } : {}), }); VAR VAR = (VAR VAR.VAR(""VAR"")?.VAR())?.VAR || VAR; VAR (!VAR) VAR VAR VAR( ""VAR::VAR VAR VAR - VAR VAR VAR VAR?"" ); VAR { VAR }; }, VAR: VAR VAR () { VAR VAR.VAR(); VAR { VAR: VAR(VAR VAR()) }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR { VAR } = VAR VAR.VAR(); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR (!VAR || !VAR.VAR) VAR; VAR += (VAR VAR.VAR(VAR, VAR.VAR))?.VAR || 0; } VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR?.VAR || 0; }, VAR: VAR VAR (VAR, VAR, VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR, VAR: 4, }); VAR.VAR((VAR) => { VAR.VAR.VAR(VAR?.VAR?.VAR || """"); VAR.VAR.VAR({ ...(VAR?.VAR || {}), VAR: VAR.VAR, }); }); VAR VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR).VAR(() => VAR); VAR (!VAR) VAR VAR; VAR { VAR: VAR, ...VAR, VAR: VAR.VAR, }; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR VAR.VAR(VAR, VAR); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR).VAR((VAR) => { VAR.VAR(""VAR::VAR"", VAR.VAR); VAR VAR; }); VAR !!VAR; }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR, VAR) { VAR (VAR VAR.VAR(VAR, VAR)) { VAR VAR VAR.VAR(VAR); } VAR VAR.VAR(VAR, { VAR: { VAR: 1536,  VAR: ""VAR"", }, }); VAR VAR VAR.VAR(VAR); }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR!"", { VAR, }); VAR { VAR } = VAR; VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR VAR = { VAR: [], VAR: [], VAR: [], };   VAR.VAR((VAR) => { VAR VAR = VAR(); VAR { VAR: VAR, ...VAR } = VAR.VAR; VAR.VAR({ VAR, VAR: VAR }); VAR.VAR.VAR(VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); }); VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR, VAR: { ...VAR }, }); VAR (VAR?.VAR !== ""VAR"") VAR VAR VAR(""VAR VAR VAR VAR"", VAR); } VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], VAR: [], }; VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = { VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR!"", { VAR, }); VAR (VAR.VAR > 0) { VAR VAR = []; VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR, VAR: { VAR: VAR.VAR, VAR: VAR.VAR, VAR: VAR.VAR, }, }); VAR (VAR?.VAR !== ""VAR"") VAR VAR VAR(""VAR VAR VAR VAR"", VAR); VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR, { VAR: VAR, VAR: VAR, }); VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR VAR(""VAR VAR VAR VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR} VAR VAR VAR VAR ${VAR?.VAR} VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(); VAR (VAR VAR VAR VAR.VAR) { VAR VAR.VAR(VAR.VAR); } VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR({ ...VAR, }); } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/weaviate/index.js,"const { default: weaviate } = require(""weaviate-ts-client"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");
const { camelCase } = require(""../../helpers/camelcase"");

const Weaviate = {
  name: ""Weaviate"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""weaviate"")
      throw new Error(""Weaviate::Invalid ENV settings"");

    const weaviateUrl = new URL(process.env.WEAVIATE_ENDPOINT);
    const options = {
      scheme: weaviateUrl.protocol?.replace("":"", """") || ""http"",
      host: weaviateUrl?.host,
      ...(process.env?.WEAVIATE_API_KEY?.length > 0
        ? { apiKey: new weaviate.ApiKey(process.env?.WEAVIATE_API_KEY) }
        : {}),
    };
    const client = weaviate.client(options);
    const isAlive = await await client.misc.liveChecker().do();
    if (!isAlive)
      throw new Error(
        ""Weaviate::Invalid Alive signal received - is the service online?""
      );
    return { client };
  },
  heartbeat: async function () {
    await this.connect();
    return { heartbeat: Number(new Date()) };
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const collectionNames = await this.allNamespaces(client);
    var totalVectors = 0;
    for (const name of collectionNames) {
      totalVectors += await this.namespaceCountWithClient(client, name);
    }
    return totalVectors;
  },
  namespaceCountWithClient: async function (client, namespace) {
    try {
      const response = await client.graphql
        .aggregate()
        .withClassName(camelCase(namespace))
        .withFields(""meta { count }"")
        .do();
      return (
        response?.data?.Aggregate?.[camelCase(namespace)]?.[0]?.meta?.count || 0
      );
    } catch (e) {
      console.error(`Weaviate:namespaceCountWithClient`, e.message);
      return 0;
    }
  },
  namespaceCount: async function (namespace = null) {
    try {
      const { client } = await this.connect();
      const response = await client.graphql
        .aggregate()
        .withClassName(camelCase(namespace))
        .withFields(""meta { count }"")
        .do();

      return (
        response?.data?.Aggregate?.[camelCase(namespace)]?.[0]?.meta?.count || 0
      );
    } catch (e) {
      console.error(`Weaviate:namespaceCountWithClient`, e.message);
      return 0;
    }
  },
  similarityResponse: async function (client, namespace, queryVector) {
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const weaviateClass = await this.namespace(client, namespace);
    const fields = weaviateClass.properties.map((prop) => prop.name).join("" "");
    const queryResponse = await client.graphql
      .get()
      .withClassName(camelCase(namespace))
      .withFields(`${fields} _additional { id }`)
      .withNearVector({ vector: queryVector })
      .withLimit(4)
      .do();

    const responses = queryResponse?.data?.Get?.[camelCase(namespace)];
    responses.forEach((response) => {
      // In Weaviate we have to pluck id from _additional and spread it into the rest
      // of the properties.
      const {
        _additional: { id },
        ...rest
      } = response;
      result.contextTexts.push(rest.text);
      result.sourceDocuments.push({ ...rest, id });
    });

    return result;
  },
  allNamespaces: async function (client) {
    try {
      const { classes = [] } = await client.schema.getter().do();
      return classes.map((classObj) => classObj.class);
    } catch (e) {
      console.error(""Weaviate::AllNamespace"", e);
      return [];
    }
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    if (!(await this.namespaceExists(client, namespace))) return null;

    const weaviateClass = await client.schema
      .classGetter()
      .withClassName(camelCase(namespace))
      .do();

    return {
      ...weaviateClass,
      vectorCount: await this.namespaceCount(namespace),
    };
  },
  addVectors: async function (client, vectors = []) {
    const response = { success: true, errors: new Set([]) };
    const results = await client.batch
      .objectsBatcher()
      .withObjects(...vectors)
      .do();

    results.forEach((res) => {
      const { status, errors = [] } = res.result;
      if (status === ""SUCCESS"" || errors.length === 0) return;
      response.success = false;
      response.errors.add(errors.error?.[0]?.message || null);
    });

    response.errors = [...response.errors];
    return response;
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    const weaviateClasses = await this.allNamespaces(client);
    return weaviateClasses.includes(camelCase(namespace));
  },
  namespaceExists: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const weaviateClasses = await this.allNamespaces(client);
    return weaviateClasses.includes(camelCase(namespace));
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    await client.schema.classDeleter().withClassName(camelCase(namespace)).do();
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const {
        pageContent,
        docId,
        id: _id, // Weaviate will abort if `id` is present in properties
        ...metadata
      } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const weaviateClassExits = await this.hasNamespace(namespace);
        if (!weaviateClassExits) {
          await client.schema
            .classCreator()
            .withClass({
              class: camelCase(namespace),
              description: `Class created by AnythingLLM named ${camelCase(
                namespace
              )}`,
              vectorizer: ""none"",
            })
            .do();
        }

        const { chunks } = cacheResult;
        const documentVectors = [];
        const vectors = [];

        for (const chunk of chunks) {
          // Before sending to Weaviate and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const flattenedMetadata = this.flattenObjectForWeaviate(
              chunk.properties
            );
            documentVectors.push({ docId, vectorId: id });
            const vectorRecord = {
              id,
              class: camelCase(namespace),
              vector: chunk.vector || chunk.values || [],
              properties: { ...flattenedMetadata },
            };
            vectors.push(vectorRecord);
          });

          const { success: additionResult, errors = [] } =
            await this.addVectors(client, vectors);
          if (!additionResult) {
            console.error(""Weaviate::addVectors failed to insert"", errors);
            throw new Error(""Error embedding into Weaviate"");
          }
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `Chroma.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);
      const submission = {
        ids: [],
        vectors: [],
        properties: [],
      };

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const flattenedMetadata = this.flattenObjectForWeaviate(metadata);
          const vectorRecord = {
            class: camelCase(namespace),
            id: uuidv4(),
            vector: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/5485c4af50c063e257ad54f4393fa79e0aff6462/langchain/src/vectorstores/weaviate.ts#L133
            properties: { ...flattenedMetadata, text: textChunks[i] },
          };

          submission.ids.push(vectorRecord.id);
          submission.vectors.push(vectorRecord.values);
          submission.properties.push(metadata);

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      const { client } = await this.connect();
      const weaviateClassExits = await this.hasNamespace(namespace);
      if (!weaviateClassExits) {
        await client.schema
          .classCreator()
          .withClass({
            class: camelCase(namespace),
            description: `Class created by AnythingLLM named ${camelCase(
              namespace
            )}`,
            vectorizer: ""none"",
          })
          .do();
      }

      if (vectors.length > 0) {
        const chunks = [];
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        console.log(""Inserting vectorized chunks into Weaviate collection."");
        const { success: additionResult, errors = [] } = await this.addVectors(
          client,
          vectors
        );
        if (!additionResult) {
          console.error(""Weaviate::addVectors failed to insert"", errors);
          throw new Error(""Error embedding into Weaviate"");
        }
        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) return;

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    for (const doc of knownDocuments) {
      await client.data
        .deleter()
        .withClassName(camelCase(namespace))
        .withId(doc.vectorId)
        .do();
    }

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );

    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    const details = await this.namespace(client, namespace);
    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${camelCase(namespace)} was deleted along with ${
        details?.vectorCount
      } vectors.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    const weaviateClasses = await this.allNamespaces(client);
    for (const weaviateClass of weaviateClasses) {
      await client.schema.classDeleter().withClassName(weaviateClass).do();
    }
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      if (Object.keys(source).length > 0) {
        documents.push(source);
      }
    }

    return documents;
  },
  flattenObjectForWeaviate: function (obj = {}) {
    // Note this function is not generic, it is designed specifically for Weaviate
    // https://weaviate.io/developers/weaviate/config-refs/datatypes#introduction
    // Credit to LangchainJS
    // https://github.com/hwchase17/langchainjs/blob/5485c4af50c063e257ad54f4393fa79e0aff6462/langchain/src/vectorstores/weaviate.ts#L11C1-L50C3
    const flattenedObject = {};

    for (const key in obj) {
      if (!Object.hasOwn(obj, key)) {
        continue;
      }
      const value = obj[key];
      if (typeof obj[key] === ""object"" && !Array.isArray(value)) {
        const recursiveResult = this.flattenObjectForWeaviate(value);

        for (const deepKey in recursiveResult) {
          if (Object.hasOwn(obj, key)) {
            flattenedObject[`${key}_${deepKey}`] = recursiveResult[deepKey];
          }
        }
      } else if (Array.isArray(value)) {
        if (
          value.length > 0 &&
          typeof value[0] !== ""object"" &&
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          value.every((el) => typeof el === typeof value[0])
        ) {
          // Weaviate only supports arrays of primitive types,
          // where all elements are of the same type
          flattenedObject[key] = value;
        }
      } else {
        flattenedObject[key] = value;
      }
    }

    return flattenedObject;
  },
};

module.exports.Weaviate = Weaviate;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4899,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR: VAR } = VAR(""VAR-VAR-VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR/VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR(VAR.VAR.VAR); VAR VAR = { VAR: VAR.VAR?.VAR("":"", """") || ""VAR"", VAR: VAR?.VAR, ...(VAR.VAR?.VAR?.VAR > 0 ? { VAR: VAR VAR.VAR(VAR.VAR?.VAR) } : {}), }; VAR VAR = VAR.VAR(VAR); VAR VAR = VAR VAR VAR.VAR.VAR().VAR(); VAR (!VAR) VAR VAR VAR( ""VAR::VAR VAR VAR VAR - VAR VAR VAR VAR?"" ); VAR { VAR }; }, VAR: VAR VAR () { VAR VAR.VAR(); VAR { VAR: VAR(VAR VAR()) }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR += VAR VAR.VAR(VAR, VAR); } VAR VAR; }, VAR: VAR VAR (VAR, VAR) { VAR { VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(""VAR { VAR }"") .VAR(); VAR ( VAR?.VAR?.VAR?.[VAR(VAR)]?.[0]?.VAR?.VAR || 0 ); } VAR (VAR) { VAR.VAR(`VAR:VAR`, VAR.VAR); VAR 0; } }, VAR: VAR VAR (VAR = VAR) { VAR { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(""VAR { VAR }"") .VAR(); VAR ( VAR?.VAR?.VAR?.[VAR(VAR)]?.[0]?.VAR?.VAR || 0 ); } VAR (VAR) { VAR.VAR(`VAR:VAR`, VAR.VAR); VAR 0; } }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR = VAR.VAR.VAR((VAR) => VAR.VAR).VAR("" ""); VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(`${VAR} VAR { VAR }`) .VAR({ VAR: VAR }) .VAR(4) .VAR(); VAR VAR = VAR?.VAR?.VAR?.[VAR(VAR)]; VAR.VAR((VAR) => {   VAR { VAR: { VAR }, ...VAR } = VAR; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR({ ...VAR, VAR }); }); VAR VAR; }, VAR: VAR VAR (VAR) { VAR { VAR { VAR = [] } = VAR VAR.VAR.VAR().VAR(); VAR VAR.VAR((VAR) => VAR.VAR); } VAR (VAR) { VAR.VAR(""VAR::VAR"", VAR); VAR []; } }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR; VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(); VAR { ...VAR, VAR: VAR VAR.VAR(VAR), }; }, VAR: VAR VAR (VAR, VAR = []) { VAR VAR = { VAR: VAR, VAR: VAR VAR([]) }; VAR VAR = VAR VAR.VAR .VAR() .VAR(...VAR) .VAR(); VAR.VAR((VAR) => { VAR { VAR, VAR = [] } = VAR.VAR; VAR (VAR === ""VAR"" || VAR.VAR === 0) VAR; VAR.VAR = VAR; VAR.VAR.VAR(VAR.VAR?.[0]?.VAR || VAR); }); VAR.VAR = [...VAR.VAR]; VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR VAR.VAR(VAR(VAR)); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR); VAR VAR.VAR(VAR(VAR)); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR.VAR().VAR(VAR(VAR)).VAR(); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, VAR: VAR,  ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) { VAR VAR.VAR .VAR() .VAR({ VAR: VAR(VAR), VAR: `VAR VAR VAR VAR VAR ${VAR( VAR )}`, VAR: ""VAR"", }) .VAR(); } VAR { VAR } = VAR; VAR VAR = []; VAR VAR = []; VAR (VAR VAR VAR VAR) {   VAR.VAR((VAR) => { VAR VAR = VAR(); VAR VAR = VAR.VAR( VAR.VAR ); VAR.VAR({ VAR, VAR: VAR }); VAR VAR = { VAR, VAR: VAR(VAR), VAR: VAR.VAR || VAR.VAR || [], VAR: { ...VAR }, }; VAR.VAR(VAR); }); VAR { VAR: VAR, VAR = [] } = VAR VAR.VAR(VAR, VAR); VAR (!VAR) { VAR.VAR(""VAR::VAR VAR VAR VAR"", VAR); VAR VAR VAR(""VAR VAR VAR VAR""); } } VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], VAR: [], }; VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = VAR.VAR(VAR); VAR VAR = { VAR: VAR(VAR), VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) { VAR VAR.VAR .VAR() .VAR({ VAR: VAR(VAR), VAR: `VAR VAR VAR VAR VAR ${VAR( VAR )}`, VAR: ""VAR"", }) .VAR(); } VAR (VAR.VAR > 0) { VAR VAR = []; VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR { VAR: VAR, VAR = [] } = VAR VAR.VAR( VAR, VAR ); VAR (!VAR) { VAR.VAR(""VAR::VAR VAR VAR VAR"", VAR); VAR VAR VAR(""VAR VAR VAR VAR""); } VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR (VAR VAR VAR VAR) { VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(VAR.VAR) .VAR(); } VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR(VAR)} VAR VAR VAR VAR ${ VAR?.VAR } VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR (VAR VAR VAR VAR) { VAR VAR.VAR.VAR().VAR(VAR).VAR(); } VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR(VAR); } } VAR VAR; }, VAR: VAR (VAR = {}) {     VAR VAR = {}; VAR (VAR VAR VAR VAR) { VAR (!VAR.VAR(VAR, VAR)) { VAR; } VAR VAR = VAR[VAR]; VAR (VAR VAR[VAR] === ""VAR"" && !VAR.VAR(VAR)) { VAR VAR = VAR.VAR(VAR); VAR (VAR VAR VAR VAR) { VAR (VAR.VAR(VAR, VAR)) { VAR[`${VAR}VAR${VAR}`] = VAR[VAR]; } } } VAR VAR (VAR.VAR(VAR)) { VAR ( VAR.VAR > 0 && VAR VAR[0] !== ""VAR"" &&  VAR.VAR((VAR) => VAR VAR === VAR VAR[0]) ) {   VAR[VAR] = VAR; } } VAR { VAR[VAR] = VAR; } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
anything-llm_dc3dfbf31495fe316b21ee184b9317b38101d30e,server/utils/vectorDbProviders/weaviate/index.js,"const { default: weaviate } = require(""weaviate-ts-client"");
const { RecursiveCharacterTextSplitter } = require(""langchain/text_splitter"");
const { storeVectorResult, cachedVectorInformation } = require(""../../files"");
const { v4: uuidv4 } = require(""uuid"");
const { toChunks, getLLMProvider } = require(""../../helpers"");
const { chatPrompt } = require(""../../chats"");
const { camelCase } = require(""../../helpers/camelcase"");

const Weaviate = {
  name: ""Weaviate"",
  connect: async function () {
    if (process.env.VECTOR_DB !== ""weaviate"")
      throw new Error(""Weaviate::Invalid ENV settings"");

    const weaviateUrl = new URL(process.env.WEAVIATE_ENDPOINT);
    const options = {
      scheme: weaviateUrl.protocol?.replace("":"", """") || ""http"",
      host: weaviateUrl?.host,
      ...(process.env?.WEAVIATE_API_KEY?.length > 0
        ? { apiKey: new weaviate.ApiKey(process.env?.WEAVIATE_API_KEY) }
        : {}),
    };
    const client = weaviate.client(options);
    const isAlive = await await client.misc.liveChecker().do();
    if (!isAlive)
      throw new Error(
        ""Weaviate::Invalid Alive signal received - is the service online?""
      );
    return { client };
  },
  heartbeat: async function () {
    await this.connect();
    return { heartbeat: Number(new Date()) };
  },
  totalIndicies: async function () {
    const { client } = await this.connect();
    const collectionNames = await this.allNamespaces(client);
    var totalVectors = 0;
    for (const name of collectionNames) {
      totalVectors += await this.namespaceCountWithClient(client, name);
    }
    return totalVectors;
  },
  namespaceCountWithClient: async function (client, namespace) {
    try {
      const response = await client.graphql
        .aggregate()
        .withClassName(camelCase(namespace))
        .withFields(""meta { count }"")
        .do();
      return (
        response?.data?.Aggregate?.[camelCase(namespace)]?.[0]?.meta?.count || 0
      );
    } catch (e) {
      console.error(`Weaviate:namespaceCountWithClient`, e.message);
      return 0;
    }
  },
  namespaceCount: async function (namespace = null) {
    try {
      const { client } = await this.connect();
      const response = await client.graphql
        .aggregate()
        .withClassName(camelCase(namespace))
        .withFields(""meta { count }"")
        .do();

      return (
        response?.data?.Aggregate?.[camelCase(namespace)]?.[0]?.meta?.count || 0
      );
    } catch (e) {
      console.error(`Weaviate:namespaceCountWithClient`, e.message);
      return 0;
    }
  },
  similarityResponse: async function (client, namespace, queryVector) {
    const result = {
      contextTexts: [],
      sourceDocuments: [],
    };

    const weaviateClass = await this.namespace(client, namespace);
    const fields = weaviateClass.properties.map((prop) => prop.name).join("" "");
    const queryResponse = await client.graphql
      .get()
      .withClassName(camelCase(namespace))
      .withFields(`${fields} _additional { id }`)
      .withNearVector({ vector: queryVector })
      .withLimit(4)
      .do();

    const responses = queryResponse?.data?.Get?.[camelCase(namespace)];
    responses.forEach((response) => {
      // In Weaviate we have to pluck id from _additional and spread it into the rest
      // of the properties.
      const {
        _additional: { id },
        ...rest
      } = response;
      result.contextTexts.push(rest.text);
      result.sourceDocuments.push({ ...rest, id });
    });

    return result;
  },
  allNamespaces: async function (client) {
    try {
      const { classes = [] } = await client.schema.getter().do();
      return classes.map((classObj) => classObj.class);
    } catch (e) {
      console.error(""Weaviate::AllNamespace"", e);
      return [];
    }
  },
  namespace: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    if (!(await this.namespaceExists(client, namespace))) return null;

    const weaviateClass = await client.schema
      .classGetter()
      .withClassName(camelCase(namespace))
      .do();

    return {
      ...weaviateClass,
      vectorCount: await this.namespaceCount(namespace),
    };
  },
  addVectors: async function (client, vectors = []) {
    const response = { success: true, errors: new Set([]) };
    const results = await client.batch
      .objectsBatcher()
      .withObjects(...vectors)
      .do();

    results.forEach((res) => {
      const { status, errors = [] } = res.result;
      if (status === ""SUCCESS"" || errors.length === 0) return;
      response.success = false;
      response.errors.add(errors.error?.[0]?.message || null);
    });

    response.errors = [...response.errors];
    return response;
  },
  hasNamespace: async function (namespace = null) {
    if (!namespace) return false;
    const { client } = await this.connect();
    const weaviateClasses = await this.allNamespaces(client);
    return weaviateClasses.includes(camelCase(namespace));
  },
  namespaceExists: async function (client, namespace = null) {
    if (!namespace) throw new Error(""No namespace value provided."");
    const weaviateClasses = await this.allNamespaces(client);
    return weaviateClasses.includes(camelCase(namespace));
  },
  deleteVectorsInNamespace: async function (client, namespace = null) {
    await client.schema.classDeleter().withClassName(camelCase(namespace)).do();
    return true;
  },
  addDocumentToNamespace: async function (
    namespace,
    documentData = {},
    fullFilePath = null
  ) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    try {
      const {
        pageContent,
        docId,
        id: _id, // Weaviate will abort if `id` is present in properties
        ...metadata
      } = documentData;
      if (!pageContent || pageContent.length == 0) return false;

      console.log(""Adding new vectorized document into namespace"", namespace);
      const cacheResult = await cachedVectorInformation(fullFilePath);
      if (cacheResult.exists) {
        const { client } = await this.connect();
        const weaviateClassExits = await this.hasNamespace(namespace);
        if (!weaviateClassExits) {
          await client.schema
            .classCreator()
            .withClass({
              class: camelCase(namespace),
              description: `Class created by AnythingLLM named ${camelCase(
                namespace
              )}`,
              vectorizer: ""none"",
            })
            .do();
        }

        const { chunks } = cacheResult;
        const documentVectors = [];
        const vectors = [];

        for (const chunk of chunks) {
          // Before sending to Weaviate and saving the records to our db
          // we need to assign the id of each chunk that is stored in the cached file.
          chunk.forEach((chunk) => {
            const id = uuidv4();
            const flattenedMetadata = this.flattenObjectForWeaviate(
              chunk.properties
            );
            documentVectors.push({ docId, vectorId: id });
            const vectorRecord = {
              id,
              class: camelCase(namespace),
              vector: chunk.vector || chunk.values || [],
              properties: { ...flattenedMetadata },
            };
            vectors.push(vectorRecord);
          });

          const { success: additionResult, errors = [] } =
            await this.addVectors(client, vectors);
          if (!additionResult) {
            console.error(""Weaviate::addVectors failed to insert"", errors);
            throw new Error(""Error embedding into Weaviate"");
          }
        }

        await DocumentVectors.bulkInsert(documentVectors);
        return true;
      }

      // If we are here then we are going to embed and store a novel document.
      // We have to do this manually as opposed to using LangChains `Chroma.fromDocuments`
      // because we then cannot atomically control our namespace to granularly find/remove documents
      // from vectordb.
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 20,
      });
      const textChunks = await textSplitter.splitText(pageContent);

      console.log(""Chunks created from document:"", textChunks.length);
      const LLMConnector = getLLMProvider();
      const documentVectors = [];
      const vectors = [];
      const vectorValues = await LLMConnector.embedChunks(textChunks);
      const submission = {
        ids: [],
        vectors: [],
        properties: [],
      };

      if (!!vectorValues && vectorValues.length > 0) {
        for (const [i, vector] of vectorValues.entries()) {
          const flattenedMetadata = this.flattenObjectForWeaviate(metadata);
          const vectorRecord = {
            class: camelCase(namespace),
            id: uuidv4(),
            vector: vector,
            // [DO NOT REMOVE]
            // LangChain will be unable to find your text if you embed manually and dont include the `text` key.
            // https://github.com/hwchase17/langchainjs/blob/5485c4af50c063e257ad54f4393fa79e0aff6462/langchain/src/vectorstores/weaviate.ts#L133
            properties: { ...flattenedMetadata, text: textChunks[i] },
          };

          submission.ids.push(vectorRecord.id);
          submission.vectors.push(vectorRecord.values);
          submission.properties.push(metadata);

          vectors.push(vectorRecord);
          documentVectors.push({ docId, vectorId: vectorRecord.id });
        }
      } else {
        console.error(
          ""Could not use OpenAI to embed document chunks! This document will not be recorded.""
        );
      }

      const { client } = await this.connect();
      const weaviateClassExits = await this.hasNamespace(namespace);
      if (!weaviateClassExits) {
        await client.schema
          .classCreator()
          .withClass({
            class: camelCase(namespace),
            description: `Class created by AnythingLLM named ${camelCase(
              namespace
            )}`,
            vectorizer: ""none"",
          })
          .do();
      }

      if (vectors.length > 0) {
        const chunks = [];
        for (const chunk of toChunks(vectors, 500)) chunks.push(chunk);

        console.log(""Inserting vectorized chunks into Weaviate collection."");
        const { success: additionResult, errors = [] } = await this.addVectors(
          client,
          vectors
        );
        if (!additionResult) {
          console.error(""Weaviate::addVectors failed to insert"", errors);
          throw new Error(""Error embedding into Weaviate"");
        }
        await storeVectorResult(chunks, fullFilePath);
      }

      await DocumentVectors.bulkInsert(documentVectors);
      return true;
    } catch (e) {
      console.error(e);
      console.error(""addDocumentToNamespace"", e.message);
      return false;
    }
  },
  deleteDocumentFromNamespace: async function (namespace, docId) {
    const { DocumentVectors } = require(""../../../models/vectors"");
    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) return;

    const knownDocuments = await DocumentVectors.where(`docId = '${docId}'`);
    if (knownDocuments.length === 0) return;

    for (const doc of knownDocuments) {
      await client.data
        .deleter()
        .withClassName(camelCase(namespace))
        .withId(doc.vectorId)
        .do();
    }

    const indexes = knownDocuments.map((doc) => doc.id);
    await DocumentVectors.deleteIds(indexes);
    return true;
  },
  query: async function (reqBody = {}) {
    const { namespace = null, input, workspace = {} } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );

    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  // This implementation of chat uses the chat history and modifies the system prompt at execution
  // this is improved over the regular langchain implementation so that chats do not directly modify embeddings
  // because then multi-user support will have all conversations mutating the base vector collection to which then
  // the only solution is replicating entire vector databases per user - which will very quickly consume space on VectorDbs
  chat: async function (reqBody = {}) {
    const {
      namespace = null,
      input,
      workspace = {},
      chatHistory = [],
    } = reqBody;
    if (!namespace || !input) throw new Error(""Invalid request body"");

    const { client } = await this.connect();
    if (!(await this.namespaceExists(client, namespace))) {
      return {
        response: null,
        sources: [],
        message: ""Invalid query - no documents found for workspace!"",
      };
    }

    const LLMConnector = getLLMProvider();
    const queryVector = await LLMConnector.embedTextInput(input);
    const { contextTexts, sourceDocuments } = await this.similarityResponse(
      client,
      namespace,
      queryVector
    );
    const prompt = {
      role: ""system"",
      content: `${chatPrompt(workspace)}
    Context:
    ${contextTexts
      .map((text, i) => {
        return `[CONTEXT ${i}]:\n${text}\n[END CONTEXT ${i}]\n\n`;
      })
      .join("""")}`,
    };
    const memory = [prompt, ...chatHistory, { role: ""user"", content: input }];
    const responseText = await LLMConnector.getChatCompletion(memory, {
      temperature: workspace?.openAiTemp ?? 0.7,
    });

    return {
      response: responseText,
      sources: this.curateSources(sourceDocuments),
      message: false,
    };
  },
  ""namespace-stats"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    if (!namespace) throw new Error(""namespace required"");
    const { client } = await this.connect();
    const stats = await this.namespace(client, namespace);
    return stats
      ? stats
      : { message: ""No stats were able to be fetched from DB for namespace"" };
  },
  ""delete-namespace"": async function (reqBody = {}) {
    const { namespace = null } = reqBody;
    const { client } = await this.connect();
    const details = await this.namespace(client, namespace);
    await this.deleteVectorsInNamespace(client, namespace);
    return {
      message: `Namespace ${camelCase(namespace)} was deleted along with ${
        details?.vectorCount
      } vectors.`,
    };
  },
  reset: async function () {
    const { client } = await this.connect();
    const weaviateClasses = await this.allNamespaces(client);
    for (const weaviateClass of weaviateClasses) {
      await client.schema.classDeleter().withClassName(weaviateClass).do();
    }
    return { reset: true };
  },
  curateSources: function (sources = []) {
    const documents = [];
    for (const source of sources) {
      if (Object.keys(source).length > 0) {
        documents.push(source);
      }
    }

    return documents;
  },
  flattenObjectForWeaviate: function (obj = {}) {
    // Note this function is not generic, it is designed specifically for Weaviate
    // https://weaviate.io/developers/weaviate/config-refs/datatypes#introduction
    // Credit to LangchainJS
    // https://github.com/hwchase17/langchainjs/blob/5485c4af50c063e257ad54f4393fa79e0aff6462/langchain/src/vectorstores/weaviate.ts#L11C1-L50C3
    const flattenedObject = {};

    for (const key in obj) {
      if (!Object.hasOwn(obj, key)) {
        continue;
      }
      const value = obj[key];
      if (typeof obj[key] === ""object"" && !Array.isArray(value)) {
        const recursiveResult = this.flattenObjectForWeaviate(value);

        for (const deepKey in recursiveResult) {
          if (Object.hasOwn(obj, key)) {
            flattenedObject[`${key}_${deepKey}`] = recursiveResult[deepKey];
          }
        }
      } else if (Array.isArray(value)) {
        if (
          value.length > 0 &&
          typeof value[0] !== ""object"" &&
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          value.every((el) => typeof el === typeof value[0])
        ) {
          // Weaviate only supports arrays of primitive types,
          // where all elements are of the same type
          flattenedObject[key] = value;
        }
      } else {
        flattenedObject[key] = value;
      }
    }

    return flattenedObject;
  },
};

module.exports.Weaviate = Weaviate;",,dc3dfbf31495fe316b21ee184b9317b38101d30e,CVE-2023-4898,dc3dfbf31495fe316b21ee184b9317b38101d30e,https://github.com/mintplex-labs/anything-llm,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR { VAR: VAR } = VAR(""VAR-VAR-VAR""); VAR { VAR } = VAR(""VAR/VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR: VAR } = VAR(""VAR""); VAR { VAR, VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR""); VAR { VAR } = VAR(""../../VAR/VAR""); VAR VAR = { VAR: ""VAR"", VAR: VAR VAR () { VAR (VAR.VAR.VAR !== ""VAR"") VAR VAR VAR(""VAR::VAR VAR VAR""); VAR VAR = VAR VAR(VAR.VAR.VAR); VAR VAR = { VAR: VAR.VAR?.VAR("":"", """") || ""VAR"", VAR: VAR?.VAR, ...(VAR.VAR?.VAR?.VAR > 0 ? { VAR: VAR VAR.VAR(VAR.VAR?.VAR) } : {}), }; VAR VAR = VAR.VAR(VAR); VAR VAR = VAR VAR VAR.VAR.VAR().VAR(); VAR (!VAR) VAR VAR VAR( ""VAR::VAR VAR VAR VAR - VAR VAR VAR VAR?"" ); VAR { VAR }; }, VAR: VAR VAR () { VAR VAR.VAR(); VAR { VAR: VAR(VAR VAR()) }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR VAR = 0; VAR (VAR VAR VAR VAR) { VAR += VAR VAR.VAR(VAR, VAR); } VAR VAR; }, VAR: VAR VAR (VAR, VAR) { VAR { VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(""VAR { VAR }"") .VAR(); VAR ( VAR?.VAR?.VAR?.[VAR(VAR)]?.[0]?.VAR?.VAR || 0 ); } VAR (VAR) { VAR.VAR(`VAR:VAR`, VAR.VAR); VAR 0; } }, VAR: VAR VAR (VAR = VAR) { VAR { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(""VAR { VAR }"") .VAR(); VAR ( VAR?.VAR?.VAR?.[VAR(VAR)]?.[0]?.VAR?.VAR || 0 ); } VAR (VAR) { VAR.VAR(`VAR:VAR`, VAR.VAR); VAR 0; } }, VAR: VAR VAR (VAR, VAR, VAR) { VAR VAR = { VAR: [], VAR: [], }; VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR = VAR.VAR.VAR((VAR) => VAR.VAR).VAR("" ""); VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(`${VAR} VAR { VAR }`) .VAR({ VAR: VAR }) .VAR(4) .VAR(); VAR VAR = VAR?.VAR?.VAR?.[VAR(VAR)]; VAR.VAR((VAR) => {   VAR { VAR: { VAR }, ...VAR } = VAR; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR({ ...VAR, VAR }); }); VAR VAR; }, VAR: VAR VAR (VAR) { VAR { VAR { VAR = [] } = VAR VAR.VAR.VAR().VAR(); VAR VAR.VAR((VAR) => VAR.VAR); } VAR (VAR) { VAR.VAR(""VAR::VAR"", VAR); VAR []; } }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR VAR; VAR VAR = VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(); VAR { ...VAR, VAR: VAR VAR.VAR(VAR), }; }, VAR: VAR VAR (VAR, VAR = []) { VAR VAR = { VAR: VAR, VAR: VAR VAR([]) }; VAR VAR = VAR VAR.VAR .VAR() .VAR(...VAR) .VAR(); VAR.VAR((VAR) => { VAR { VAR, VAR = [] } = VAR.VAR; VAR (VAR === ""VAR"" || VAR.VAR === 0) VAR; VAR.VAR = VAR; VAR.VAR.VAR(VAR.VAR?.[0]?.VAR || VAR); }); VAR.VAR = [...VAR.VAR]; VAR VAR; }, VAR: VAR VAR (VAR = VAR) { VAR (!VAR) VAR VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR VAR.VAR(VAR(VAR)); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR (!VAR) VAR VAR VAR(""VAR VAR VAR VAR.""); VAR VAR = VAR VAR.VAR(VAR); VAR VAR.VAR(VAR(VAR)); }, VAR: VAR VAR (VAR, VAR = VAR) { VAR VAR.VAR.VAR().VAR(VAR(VAR)).VAR(); VAR VAR; }, VAR: VAR VAR ( VAR, VAR = {}, VAR = VAR ) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR { VAR, VAR, VAR: VAR,  ...VAR } = VAR; VAR (!VAR || VAR.VAR == 0) VAR VAR; VAR.VAR(""VAR VAR VAR VAR VAR VAR"", VAR); VAR VAR = VAR VAR(VAR); VAR (VAR.VAR) { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) { VAR VAR.VAR .VAR() .VAR({ VAR: VAR(VAR), VAR: `VAR VAR VAR VAR VAR ${VAR( VAR )}`, VAR: ""VAR"", }) .VAR(); } VAR { VAR } = VAR; VAR VAR = []; VAR VAR = []; VAR (VAR VAR VAR VAR) {   VAR.VAR((VAR) => { VAR VAR = VAR(); VAR VAR = VAR.VAR( VAR.VAR ); VAR.VAR({ VAR, VAR: VAR }); VAR VAR = { VAR, VAR: VAR(VAR), VAR: VAR.VAR || VAR.VAR || [], VAR: { ...VAR }, }; VAR.VAR(VAR); }); VAR { VAR: VAR, VAR = [] } = VAR VAR.VAR(VAR, VAR); VAR (!VAR) { VAR.VAR(""VAR::VAR VAR VAR VAR"", VAR); VAR VAR VAR(""VAR VAR VAR VAR""); } } VAR VAR.VAR(VAR); VAR VAR; }     VAR VAR = VAR VAR({ VAR: 1000, VAR: 20, }); VAR VAR = VAR VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR:"", VAR.VAR); VAR VAR = VAR(); VAR VAR = []; VAR VAR = []; VAR VAR = VAR VAR.VAR(VAR); VAR VAR = { VAR: [], VAR: [], VAR: [], }; VAR (!!VAR && VAR.VAR > 0) { VAR (VAR [VAR, VAR] VAR VAR.VAR()) { VAR VAR = VAR.VAR(VAR); VAR VAR = { VAR: VAR(VAR), VAR: VAR(), VAR: VAR,    VAR: { ...VAR, VAR: VAR[VAR] }, }; VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR.VAR); VAR.VAR.VAR(VAR); VAR.VAR(VAR); VAR.VAR({ VAR, VAR: VAR.VAR }); } } VAR { VAR.VAR( ""VAR VAR VAR VAR VAR VAR VAR VAR! VAR VAR VAR VAR VAR VAR."" ); } VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR (!VAR) { VAR VAR.VAR .VAR() .VAR({ VAR: VAR(VAR), VAR: `VAR VAR VAR VAR VAR ${VAR( VAR )}`, VAR: ""VAR"", }) .VAR(); } VAR (VAR.VAR > 0) { VAR VAR = []; VAR (VAR VAR VAR VAR(VAR, 500)) VAR.VAR(VAR); VAR.VAR(""VAR VAR VAR VAR VAR VAR.""); VAR { VAR: VAR, VAR = [] } = VAR VAR.VAR( VAR, VAR ); VAR (!VAR) { VAR.VAR(""VAR::VAR VAR VAR VAR"", VAR); VAR VAR VAR(""VAR VAR VAR VAR""); } VAR VAR(VAR, VAR); } VAR VAR.VAR(VAR); VAR VAR; } VAR (VAR) { VAR.VAR(VAR); VAR.VAR(""VAR"", VAR.VAR); VAR VAR; } }, VAR: VAR VAR (VAR, VAR) { VAR { VAR } = VAR(""../../../VAR/VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) VAR; VAR VAR = VAR VAR.VAR(`VAR = '${VAR}'`); VAR (VAR.VAR === 0) VAR; VAR (VAR VAR VAR VAR) { VAR VAR.VAR .VAR() .VAR(VAR(VAR)) .VAR(VAR.VAR) .VAR(); } VAR VAR = VAR.VAR((VAR) => VAR.VAR); VAR VAR.VAR(VAR); VAR VAR; }, VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {} } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; },     VAR: VAR VAR (VAR = {}) { VAR { VAR = VAR, VAR, VAR = {}, VAR = [], } = VAR; VAR (!VAR || !VAR) VAR VAR VAR(""VAR VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR (!(VAR VAR.VAR(VAR, VAR))) { VAR { VAR: VAR, VAR: [], VAR: ""VAR VAR - VAR VAR VAR VAR VAR!"", }; } VAR VAR = VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR { VAR, VAR } = VAR VAR.VAR( VAR, VAR, VAR ); VAR VAR = { VAR: ""VAR"", VAR: `${VAR(VAR)} VAR: ${VAR .VAR((VAR, VAR) => { VAR `[VAR ${VAR}]:\VAR${VAR}\VAR[VAR VAR ${VAR}]\VAR\VAR`; }) .VAR("""")}`, }; VAR VAR = [VAR, ...VAR, { VAR: ""VAR"", VAR: VAR }]; VAR VAR = VAR VAR.VAR(VAR, { VAR: VAR?.VAR ?? 0.7, }); VAR { VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR, }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR (!VAR) VAR VAR VAR(""VAR VAR""); VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR ? VAR : { VAR: ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"" }; }, ""VAR-VAR"": VAR VAR (VAR = {}) { VAR { VAR = VAR } = VAR; VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR, VAR); VAR VAR.VAR(VAR, VAR); VAR { VAR: `VAR ${VAR(VAR)} VAR VAR VAR VAR ${ VAR?.VAR } VAR.`, }; }, VAR: VAR VAR () { VAR { VAR } = VAR VAR.VAR(); VAR VAR = VAR VAR.VAR(VAR); VAR (VAR VAR VAR VAR) { VAR VAR.VAR.VAR().VAR(VAR).VAR(); } VAR { VAR: VAR }; }, VAR: VAR (VAR = []) { VAR VAR = []; VAR (VAR VAR VAR VAR) { VAR (VAR.VAR(VAR).VAR > 0) { VAR.VAR(VAR); } } VAR VAR; }, VAR: VAR (VAR = {}) {     VAR VAR = {}; VAR (VAR VAR VAR VAR) { VAR (!VAR.VAR(VAR, VAR)) { VAR; } VAR VAR = VAR[VAR]; VAR (VAR VAR[VAR] === ""VAR"" && !VAR.VAR(VAR)) { VAR VAR = VAR.VAR(VAR); VAR (VAR VAR VAR VAR) { VAR (VAR.VAR(VAR, VAR)) { VAR[`${VAR}VAR${VAR}`] = VAR[VAR]; } } } VAR VAR (VAR.VAR(VAR)) { VAR ( VAR.VAR > 0 && VAR VAR[0] !== ""VAR"" &&  VAR.VAR((VAR) => VAR VAR === VAR VAR[0]) ) {   VAR[VAR] = VAR; } } VAR { VAR[VAR] = VAR; } } VAR VAR; }, }; VAR.VAR.VAR = VAR; ",14,9
argo-cd_3a28c8a18cc2aa84fe81492625545d25c7a90bc3,server/application/application.go,"	watchAPIBufferSize  = env.ParseNumFromEnv(argocommon.EnvWatchAPIBufferSize, 1000, 0, math.MaxInt32)
	permissionDeniedErr = status.Error(codes.PermissionDenied, ""permission denied"")
	appBroadcaster    Broadcaster
	appBroadcaster Broadcaster,
	if appBroadcaster == nil {
		appBroadcaster = &broadcasterHandler{}
	}
// getAppEnforceRBAC gets the Application with the given name in the given namespace. If no namespace is
// specified, the Application is fetched from the default namespace (the one in which the API server is running).
//
// If the Application does not exist, then we have no way of determining if the user would have had access to get that
// Application. Verifying access requires knowing the Application's name, namespace, and project. The user may specify,
// at minimum, the Application name.
//
// So to prevent a malicious user from inferring the existence or absense of the Application or namespace, we respond
// ""permission denied"" if the Application does not exist.
func (s *Server) getAppEnforceRBAC(ctx context.Context, action, namespace, name string, getApp func() (*appv1.Application, error)) (*appv1.Application, error) {
	logCtx := log.WithFields(map[string]interface{}{
		""application"": name,
		""namespace"":   namespace,
	})
	a, err := getApp()
	if err != nil {
		if apierr.IsNotFound(err) {
			logCtx.Warn(""application does not exist"")
			return nil, permissionDeniedErr
		}
		logCtx.Errorf(""failed to get application: %s"", err)
		return nil, permissionDeniedErr
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, action, a.RBACName(s.ns)); err != nil {
		logCtx.WithFields(map[string]interface{}{
			""project"":                a.Spec.Project,
			argocommon.SecurityField: argocommon.SecurityMedium,
		}).Warnf(""user tried to %s application which they do not have access to: %s"", action, err)
		return nil, permissionDeniedErr
	}
	return a, nil
}

// getApplicationEnforceRBACInformer uses an informer to get an Application. If the app does not exist, permission is
// denied, or any other error occurs when getting the app, we return a permission denied error to obscure any sensitive
// information.
func (s *Server) getApplicationEnforceRBACInformer(ctx context.Context, action, namespace, name string) (*appv1.Application, error) {
	namespaceOrDefault := s.appNamespaceOrDefault(namespace)
	return s.getAppEnforceRBAC(ctx, action, namespaceOrDefault, name, func() (*appv1.Application, error) {
		return s.appLister.Applications(namespaceOrDefault).Get(name)
	})
}

// getApplicationEnforceRBACClient uses a client to get an Application. If the app does not exist, permission is denied,
// or any other error occurs when getting the app, we return a permission denied error to obscure any sensitive
// information.
func (s *Server) getApplicationEnforceRBACClient(ctx context.Context, action, namespace, name, resourceVersion string) (*appv1.Application, error) {
	namespaceOrDefault := s.appNamespaceOrDefault(namespace)
	return s.getAppEnforceRBAC(ctx, action, namespaceOrDefault, name, func() (*appv1.Application, error) {
		return s.appclientset.ArgoprojV1alpha1().Applications(namespaceOrDefault).Get(ctx, name, metav1.GetOptions{
			ResourceVersion: resourceVersion,
		})
	})
}

	a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetAppNamespace(), q.GetName())
	a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, query.GetAppNamespace(), query.GetName())
	a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, appNs, appName, q.GetResourceVersion())
	a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetAppNamespace(), q.GetName())
func (s *Server) validateAndUpdateApp(ctx context.Context, newApp *appv1.Application, merge bool, validate bool, action string) (*appv1.Application, error) {
	app, err := s.getApplicationEnforceRBACClient(ctx, action, newApp.Namespace, newApp.Name, """")
		return nil, err
	return s.validateAndUpdateApp(ctx, q.Application, false, validate, rbacpolicy.ActionUpdate)
	a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionUpdate, q.GetAppNamespace(), q.GetName(), """")
	a, err = s.validateAndUpdateApp(ctx, a, false, validate, rbacpolicy.ActionUpdate)
	app, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetAppNamespace(), q.GetName(), """")
		return nil, err
	return s.validateAndUpdateApp(ctx, newApp, false, true, rbacpolicy.ActionUpdate)
	a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, appNs, appName, """")
		return nil, err
			// Offer no hint that the project does not exist.
			log.Warnf(""User attempted to create/update application in non-existent project %q"", app.Spec.Project)
			return permissionDeniedErr
		return &tree, fmt.Errorf(""error getting cached app resource tree: %w"", err)
	a, err := s.getApplicationEnforceRBACInformer(ctx, action, q.GetAppNamespace(), q.GetName())
		return nil, err
	if manifest == nil {
		return nil, fmt.Errorf(""failed to patch resource: manifest was nil"")
	}
	a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetAppNamespace(), q.GetApplicationName())
	_, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetAppNamespace(), q.GetApplicationName())
	a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetAppNamespace(), q.GetName())
	a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetAppNamespace(), q.GetApplicationName())
		return nil, err
		return nil, fmt.Errorf(""error getting cached app managed resources: %w"", err)
	a, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetAppNamespace(), q.GetName())
	a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, syncReq.GetAppNamespace(), syncReq.GetName(), """")
		return nil, err
	appName := syncReq.GetName()
	appNs := s.appNamespaceOrDefault(syncReq.GetAppNamespace())
	appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)
	a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, rollbackReq.GetAppNamespace(), rollbackReq.GetName(), """")
	appName := rollbackReq.GetName()
	appNs := s.appNamespaceOrDefault(rollbackReq.GetAppNamespace())
	appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)
	a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, req.GetNamespace(), req.GetName(), """")
		return nil, err
	a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, appNs, appName, """")
		app, err = s.getApplicationEnforceRBACInformer(ctx, rbacRequest, q.GetAppNamespace(), q.GetName())
			return nil, nil, nil, nil, err
			return nil, nil, nil, nil, err
	a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetAppNamespace(), q.GetName(), """")","	watchAPIBufferSize = env.ParseNumFromEnv(argocommon.EnvWatchAPIBufferSize, 1000, 0, math.MaxInt32)
	appBroadcaster    *broadcasterHandler
	appBroadcaster := &broadcasterHandler{}
	appName := q.GetName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	a, err := s.appLister.Applications(appNs).Get(appName)
		return nil, fmt.Errorf(""error getting application: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
	appName := query.GetName()
	appNs := s.appNamespaceOrDefault(query.GetAppNamespace())
	a, err := s.appLister.Applications(appNs).Get(appName)

		return fmt.Errorf(""error getting application: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
	a, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{
		ResourceVersion: q.GetResourceVersion(),
	})

		return nil, fmt.Errorf(""error getting application: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
	appName := q.GetName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	a, err := s.appLister.Applications(appNs).Get(appName)
		return nil, fmt.Errorf(""error getting application: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
func (s *Server) validateAndUpdateApp(ctx context.Context, newApp *appv1.Application, merge bool, validate bool) (*appv1.Application, error) {
	app, err := s.appclientset.ArgoprojV1alpha1().Applications(newApp.Namespace).Get(ctx, newApp.Name, metav1.GetOptions{})
		return nil, fmt.Errorf(""error getting application: %w"", err)
	return s.validateAndUpdateApp(ctx, q.Application, false, validate)
	appName := q.GetName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	a, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{})
		return nil, fmt.Errorf(""error getting application: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {
	a, err = s.validateAndUpdateApp(ctx, a, false, validate)
	appName := q.GetName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	app, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{})
		return nil, fmt.Errorf(""error getting application: %w"", err)
	return s.validateAndUpdateApp(ctx, newApp, false, true)
	a, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{})
		return nil, fmt.Errorf(""error getting application: %w"", err)
			return status.Errorf(codes.InvalidArgument, ""application references project %s which does not exist"", app.Spec.Project)
		return &tree, fmt.Errorf(""error getting cached app state: %w"", err)
	appName := q.GetName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	a, err := s.appLister.Applications(appNs).Get(appName)
		return nil, nil, nil, fmt.Errorf(""error getting app by name: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, action, a.RBACName(s.ns)); err != nil {

		return nil, fmt.Errorf(""error getting app live resource: %w"", err)
		return nil, fmt.Errorf(""error getting app live resource: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {
		return nil, fmt.Errorf(""error getting live resource for delete: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionDelete, a.RBACName(s.ns)); err != nil {
	appName := q.GetApplicationName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	a, err := s.appLister.Applications(appNs).Get(appName)
		return nil, fmt.Errorf(""error getting application by name: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
	appName := q.GetApplicationName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	a, err := s.appLister.Applications(appNs).Get(appName)
		return fmt.Errorf(""error getting application by name: %w"", err)
	}

	if err := s.enf.EnforceErr(ws.Context().Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
	appName := q.GetName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	a, err := s.appLister.Applications(appNs).Get(appName)
		return nil, fmt.Errorf(""error getting app by name: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
	appName := q.GetApplicationName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	a, err := s.appLister.Applications(appNs).Get(appName)
		return nil, fmt.Errorf(""error getting application: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
		return nil, fmt.Errorf(""error verifying rbac: %w"", err)
		return nil, fmt.Errorf(""error getting cached app state: %w"", err)
	appName := q.GetName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	a, err := s.appLister.Applications(appNs).Get(appName)
		return fmt.Errorf(""error getting application by name: %w"", err)
	}

	if err := s.enf.EnforceErr(ws.Context().Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
	appName := syncReq.GetName()
	appNs := s.appNamespaceOrDefault(syncReq.GetAppNamespace())
	appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)
	a, err := appIf.Get(ctx, appName, metav1.GetOptions{})
		return nil, fmt.Errorf(""error getting application by name: %w"", err)
	appName := rollbackReq.GetName()
	appNs := s.appNamespaceOrDefault(rollbackReq.GetAppNamespace())
	appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)
	a, err := appIf.Get(ctx, appName, metav1.GetOptions{})
		return nil, fmt.Errorf(""error getting application by name: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionSync, a.RBACName(s.ns)); err != nil {
	appName := req.GetName()
	appNs := s.appNamespaceOrDefault(req.GetNamespace())

	a, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{})
		log.WithFields(map[string]interface{}{
			""application"": appName,
			""ns"":          appNs,
		}).Errorf(""failed to get application, error=%v"", err.Error())
		return nil, fmt.Errorf(""error getting application"")
	}

	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {
		log.WithFields(map[string]interface{}{
			""application"": appName,
			""ns"":          appNs,
		}).Warnf(""unauthorized access to app, error=%v"", err.Error())
		return nil, fmt.Errorf(""error getting application"")
	a, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{})
		return nil, fmt.Errorf(""error getting application by name: %w"", err)
	}
	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionSync, a.RBACName(s.ns)); err != nil {
		namespace := s.appNamespaceOrDefault(q.GetAppNamespace())
		app, err = s.appLister.Applications(namespace).Get(q.GetName())
			return nil, nil, nil, nil, fmt.Errorf(""error getting app by name: %w"", err)
			return nil, nil, nil, nil, fmt.Errorf(""error getting app live resource: %w"", err)
	appName := q.GetName()
	appNs := s.appNamespaceOrDefault(q.GetAppNamespace())
	appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)
	a, err := appIf.Get(ctx, appName, metav1.GetOptions{})
		return nil, fmt.Errorf(""error getting application by name: %w"", err)
	}

	if err := s.enf.EnforceErr(ctx.Value(""claims""), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {",3a28c8a18cc2aa84fe81492625545d25c7a90bc3,CVE-2022-41354,3a28c8a18cc2aa84fe81492625545d25c7a90bc3,https://github.com/argoproj/argo-cd,GHSD_DIRECT_COMMIT,1337,COMPLETED,"VAR = VAR.VAR(VAR.VAR, 1000, 0, VAR.VAR) VAR = VAR.VAR(VAR.VAR, ""VAR VAR"") VAR VAR VAR VAR, VAR VAR == VAR { VAR = &VAR{} }          VAR (VAR *VAR) VAR(VAR VAR.VAR, VAR, VAR, VAR VAR, VAR VAR() (*VAR.VAR, VAR)) (*VAR.VAR, VAR) { VAR := VAR.VAR(VAR[VAR]VAR{}{ ""VAR"": VAR, ""VAR"": VAR, }) VAR, VAR := VAR() VAR VAR != VAR { VAR VAR.VAR(VAR) { VAR.VAR(""VAR VAR VAR VAR"") VAR VAR, VAR } VAR.VAR(""VAR VAR VAR VAR: %VAR"", VAR) VAR VAR, VAR } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR.VAR(VAR[VAR]VAR{}{ ""VAR"": VAR.VAR.VAR, VAR.VAR: VAR.VAR, }).VAR(""VAR VAR VAR %VAR VAR VAR VAR VAR VAR VAR VAR VAR: %VAR"", VAR, VAR) VAR VAR, VAR } VAR VAR, VAR }    VAR (VAR *VAR) VAR(VAR VAR.VAR, VAR, VAR, VAR VAR) (*VAR.VAR, VAR) { VAR := VAR.VAR(VAR) VAR VAR.VAR(VAR, VAR, VAR, VAR, VAR() (*VAR.VAR, VAR) { VAR VAR.VAR.VAR(VAR).VAR(VAR) }) }    VAR (VAR *VAR) VAR(VAR VAR.VAR, VAR, VAR, VAR, VAR VAR) (*VAR.VAR, VAR) { VAR := VAR.VAR(VAR) VAR VAR.VAR(VAR, VAR, VAR, VAR, VAR() (*VAR.VAR, VAR) { VAR VAR.VAR.VAR().VAR(VAR).VAR(VAR, VAR, VAR.VAR{ VAR: VAR, }) }) } VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR()) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR()) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR, VAR, VAR.VAR()) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR()) VAR (VAR *VAR) VAR(VAR VAR.VAR, VAR *VAR.VAR, VAR VAR, VAR VAR, VAR VAR) (*VAR.VAR, VAR) { VAR, VAR := VAR.VAR(VAR, VAR, VAR.VAR, VAR.VAR, """") VAR VAR, VAR VAR VAR.VAR(VAR, VAR.VAR, VAR, VAR, VAR.VAR) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR(), """") VAR, VAR = VAR.VAR(VAR, VAR, VAR, VAR, VAR.VAR) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR(), """") VAR VAR, VAR VAR VAR.VAR(VAR, VAR, VAR, VAR, VAR.VAR) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR, VAR, """") VAR VAR, VAR  VAR.VAR(""VAR VAR VAR VAR/VAR VAR VAR VAR-VAR VAR %VAR"", VAR.VAR.VAR) VAR VAR VAR &VAR, VAR.VAR(""VAR VAR VAR VAR VAR VAR: %VAR"", VAR) VAR, VAR := VAR.VAR(VAR, VAR, VAR.VAR(), VAR.VAR()) VAR VAR, VAR VAR VAR == VAR { VAR VAR, VAR.VAR(""VAR VAR VAR VAR: VAR VAR VAR"") } VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR()) VAR, VAR := VAR.VAR(VAR.VAR(), VAR.VAR, VAR.VAR(), VAR.VAR()) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR()) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR()) VAR VAR, VAR VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR VAR: %VAR"", VAR) VAR, VAR := VAR.VAR(VAR.VAR(), VAR.VAR, VAR.VAR(), VAR.VAR()) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR(), """") VAR VAR, VAR VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR := VAR.VAR.VAR().VAR(VAR) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR(), """") VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR := VAR.VAR.VAR().VAR(VAR) VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR(), """") VAR VAR, VAR VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR, VAR, """") VAR, VAR = VAR.VAR(VAR, VAR, VAR.VAR(), VAR.VAR()) VAR VAR, VAR, VAR, VAR, VAR VAR VAR, VAR, VAR, VAR, VAR VAR, VAR := VAR.VAR(VAR, VAR.VAR, VAR.VAR(), VAR.VAR(), """") VAR = VAR.VAR(VAR.VAR, 1000, 0, VAR.VAR) VAR *VAR VAR := &VAR{} VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR(VAR).VAR(VAR) VAR VAR, VAR.VAR(""VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR(VAR).VAR(VAR) VAR VAR.VAR(""VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR, VAR := VAR.VAR.VAR().VAR(VAR).VAR(VAR, VAR, VAR.VAR{ VAR: VAR.VAR(), }) VAR VAR, VAR.VAR(""VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR(VAR).VAR(VAR) VAR VAR, VAR.VAR(""VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR (VAR *VAR) VAR(VAR VAR.VAR, VAR *VAR.VAR, VAR VAR, VAR VAR) (*VAR.VAR, VAR) { VAR, VAR := VAR.VAR.VAR().VAR(VAR.VAR).VAR(VAR, VAR.VAR, VAR.VAR{}) VAR VAR, VAR.VAR(""VAR VAR VAR: %VAR"", VAR) VAR VAR.VAR(VAR, VAR.VAR, VAR, VAR) VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR().VAR(VAR).VAR(VAR, VAR, VAR.VAR{}) VAR VAR, VAR.VAR(""VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR, VAR = VAR.VAR(VAR, VAR, VAR, VAR) VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR().VAR(VAR).VAR(VAR, VAR, VAR.VAR{}) VAR VAR, VAR.VAR(""VAR VAR VAR: %VAR"", VAR) VAR VAR.VAR(VAR, VAR, VAR, VAR) VAR, VAR := VAR.VAR.VAR().VAR(VAR).VAR(VAR, VAR, VAR.VAR{}) VAR VAR, VAR.VAR(""VAR VAR VAR: %VAR"", VAR) VAR VAR.VAR(VAR.VAR, ""VAR VAR VAR %VAR VAR VAR VAR VAR"", VAR.VAR.VAR) VAR &VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR(VAR).VAR(VAR) VAR VAR, VAR, VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR(VAR).VAR(VAR) VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR(VAR).VAR(VAR) VAR VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR().VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR(VAR).VAR(VAR) VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR(VAR).VAR(VAR) VAR VAR, VAR.VAR(""VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR VAR, VAR.VAR(""VAR VAR VAR: %VAR"", VAR) VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR(VAR).VAR(VAR) VAR VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR().VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR := VAR.VAR.VAR().VAR(VAR) VAR, VAR := VAR.VAR(VAR, VAR, VAR.VAR{}) VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR := VAR.VAR.VAR().VAR(VAR) VAR, VAR := VAR.VAR(VAR, VAR, VAR.VAR{}) VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR, VAR := VAR.VAR.VAR().VAR(VAR).VAR(VAR, VAR, VAR.VAR{}) VAR.VAR(VAR[VAR]VAR{}{ ""VAR"": VAR, ""VAR"": VAR, }).VAR(""VAR VAR VAR VAR, VAR=%VAR"", VAR.VAR()) VAR VAR, VAR.VAR(""VAR VAR VAR"") } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR.VAR(VAR[VAR]VAR{}{ ""VAR"": VAR, ""VAR"": VAR, }).VAR(""VAR VAR VAR VAR, VAR=%VAR"", VAR.VAR()) VAR VAR, VAR.VAR(""VAR VAR VAR"") VAR, VAR := VAR.VAR.VAR().VAR(VAR).VAR(VAR, VAR, VAR.VAR{}) VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR { VAR := VAR.VAR(VAR.VAR()) VAR, VAR = VAR.VAR.VAR(VAR).VAR(VAR.VAR()) VAR VAR, VAR, VAR, VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) VAR VAR, VAR, VAR, VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) VAR := VAR.VAR() VAR := VAR.VAR(VAR.VAR()) VAR := VAR.VAR.VAR().VAR(VAR) VAR, VAR := VAR.VAR(VAR, VAR, VAR.VAR{}) VAR VAR, VAR.VAR(""VAR VAR VAR VAR VAR: %VAR"", VAR) } VAR VAR := VAR.VAR.VAR(VAR.VAR(""VAR""), VAR.VAR, VAR.VAR, VAR.VAR(VAR.VAR)); VAR != VAR {",14,9
cacti_a02f223f4e9427b0411a3f25672fd86764e63b54,auth_login.php,"					$referer = sanitize_uri($_SERVER['REDIRECT_URL']);
				} elseif (isset($_SERVER['HTTP_REFERER'])) {
					$referer = sanitize_uri($_SERVER['HTTP_REFERER']);
				} elseif (isset($_SERVER['REQUEST_URI'])) {
					$referer = sanitize_uri($_SERVER['REQUEST_URI']);
				} elseif (!is_realm_allowed(8)) {
			db_execute_prepared('INSERT IGNORE INTO user_log
				(username, user_id, result, ip, time)
				VALUES (?, 0, 0, ?, NOW())',
				array($username, $_SERVER['REMOTE_ADDR']));
				$user_auth   = true;
				$copy_user   = true;
				$realm       = get_nfilter_request_var('realm');

				$domain_name = db_fetch_cell_prepared('SELECT domain_name
					FROM user_domains
					WHERE domain_id = ?',
					array($realm-1000));

				cacti_log(""LOGIN: LDAP User '$username' Authenticated from Domain '$domain_name'"", false, 'AUTH');

				$user = db_fetch_row_prepared('SELECT *
					FROM user_auth
					WHERE username = ?
					AND realm = ?',
					array($username, $realm));
				$template_user = db_fetch_cell_prepared('SELECT user_id
					FROM user_domains
					WHERE domain_id = ?',
					array(get_nfilter_request_var('realm')-1000));

				$template_username = db_fetch_cell_prepared('SELECT username
					FROM user_auth
					WHERE id = ?',
					array($template_user));

				if (!sizeof($user) && $copy_user && !empty($template_user) && $username != '') {

						$user = db_fetch_row_prepared('SELECT *
							FROM user_auth
							WHERE username = ?
							AND realm = ?',
							array($username, $realm));
	$ld = db_fetch_row_prepared('SELECT *
		FROM user_domains_ldap
		WHERE domain_id = ?',
		array($realm-1000));
		} else {
	$ld = db_fetch_row_prepared('SELECT *
		FROM user_domains_ldap
		WHERE domain_id = ?',
		array($realm-1000));
		} else {

										foreach ($realms as $index => $realm) {","					$referer = $_SERVER['REDIRECT_URL'];
				} else if (isset($_SERVER['HTTP_REFERER'])) {
					$referer = $_SERVER['HTTP_REFERER'];
				} else if (isset($_SERVER['REQUEST_URI'])) {
					$referer = $_SERVER['REQUEST_URI'];
				} elseif (sizeof(db_fetch_assoc_prepared('SELECT realm_id FROM user_auth_realm WHERE realm_id = 8 AND user_id = ?', array($_SESSION['sess_user_id']))) == 0) {
			db_execute_prepared('INSERT IGNORE INTO user_log (username, user_id, result, ip, time) VALUES (?, 0, 0, ?, NOW())', array($username, $_SERVER['REMOTE_ADDR']));
				$user_auth = true;
				$copy_user = true;
				$realm = get_nfilter_request_var('realm');
				cacti_log(""LOGIN: LDAP User '"" . $username . ""' Authenticated from Domain '"" . db_fetch_cell('SELECT domain_name FROM user_domains WHERE domain_id=' . ($realm-1000)) . ""'"", false, 'AUTH');
				$user = db_fetch_row_prepared('SELECT * FROM user_auth WHERE username = ? AND realm = ?', array($username, $realm));
				$template_user = db_fetch_cell_prepared('SELECT user_id FROM user_domains WHERE domain_id = ?', array(get_nfilter_request_var('realm')-1000));
				$template_username = db_fetch_cell_prepared('SELECT username FROM user_auth WHERE id = ?', array($template_user));
				if (!sizeof($user) && $copy_user && $template_user != '0' && $username != '') {
						$user = db_fetch_row_prepared('SELECT * FROM user_auth WHERE username = ? AND realm = ?', array($username, $realm));
	$ld = db_fetch_row_prepared('SELECT * FROM user_domains_ldap WHERE domain_id = ?', array($realm-1000));
		}else{
	$ld = db_fetch_row_prepared('SELECT * FROM user_domains_ldap WHERE domain_id = ?', array($realm-1000));
		}else{
										foreach($realms as $index => $realm) {",a02f223f4e9427b0411a3f25672fd86764e63b54,CVE-2018-10060,a02f223f4e9427b0411a3f25672fd86764e63b54,https://github.com/Cacti/cacti,NVD_GIT_REPOBASED,118,COMPLETED,"$VAR = VAR($VAR['VAR']); } VAR (VAR($VAR['VAR'])) { $VAR = VAR($VAR['VAR']); } VAR (VAR($VAR['VAR'])) { $VAR = VAR($VAR['VAR']); } VAR (!VAR(8)) { VAR('VAR VAR VAR VAR (VAR, VAR, VAR, VAR, VAR) VAR (?, 0, 0, ?, VAR())', VAR($VAR, $VAR['VAR'])); $VAR = VAR; $VAR = VAR; $VAR = VAR('VAR'); $VAR = VAR('VAR VAR VAR VAR VAR VAR = ?', VAR($VAR-1000)); VAR(""VAR: VAR VAR '$VAR' VAR VAR VAR '$VAR'"", VAR, 'VAR'); $VAR = VAR('VAR * VAR VAR VAR VAR = ? VAR VAR = ?', VAR($VAR, $VAR)); $VAR = VAR('VAR VAR VAR VAR VAR VAR = ?', VAR(VAR('VAR')-1000)); $VAR = VAR('VAR VAR VAR VAR VAR VAR = ?', VAR($VAR)); VAR (!VAR($VAR) && $VAR && !VAR($VAR) && $VAR != '') { $VAR = VAR('VAR * VAR VAR VAR VAR = ? VAR VAR = ?', VAR($VAR, $VAR)); $VAR = VAR('VAR * VAR VAR VAR VAR = ?', VAR($VAR-1000)); } VAR { $VAR = VAR('VAR * VAR VAR VAR VAR = ?', VAR($VAR-1000)); } VAR { VAR ($VAR VAR $VAR => $VAR) { $VAR = $VAR['VAR']; } VAR VAR (VAR($VAR['VAR'])) { $VAR = $VAR['VAR']; } VAR VAR (VAR($VAR['VAR'])) { $VAR = $VAR['VAR']; } VAR (VAR(VAR('VAR VAR VAR VAR VAR VAR = 8 VAR VAR = ?', VAR($VAR['VAR']))) == 0) { VAR('VAR VAR VAR VAR (VAR, VAR, VAR, VAR, VAR) VAR (?, 0, 0, ?, VAR())', VAR($VAR, $VAR['VAR'])); $VAR = VAR; $VAR = VAR; $VAR = VAR('VAR'); VAR(""VAR: VAR VAR '"" . $VAR . ""' VAR VAR VAR '"" . VAR('VAR VAR VAR VAR VAR VAR=' . ($VAR-1000)) . ""'"", VAR, 'VAR'); $VAR = VAR('VAR * VAR VAR VAR VAR = ? VAR VAR = ?', VAR($VAR, $VAR)); $VAR = VAR('VAR VAR VAR VAR VAR VAR = ?', VAR(VAR('VAR')-1000)); $VAR = VAR('VAR VAR VAR VAR VAR VAR = ?', VAR($VAR)); VAR (!VAR($VAR) && $VAR && $VAR != '0' && $VAR != '') { $VAR = VAR('VAR * VAR VAR VAR VAR = ? VAR VAR = ?', VAR($VAR, $VAR)); $VAR = VAR('VAR * VAR VAR VAR VAR = ?', VAR($VAR-1000)); }VAR{ $VAR = VAR('VAR * VAR VAR VAR VAR = ?', VAR($VAR-1000)); }VAR{ VAR($VAR VAR $VAR => $VAR) {",14,9
discourse_bfc3132bb22bd5b7e86f428746b89c4d3d7f5a70,spec/models/user_bookmark_list_spec.rb,"  let(:list) { UserBookmarkList.new(user: user, guardian: Guardian.new(user)) }



      list = UserBookmarkList.new(user: user, guardian: Guardian.new(user), per_page: 1000)
","  let(:params) { {} }
  let(:list) { UserBookmarkList.new(user: user, guardian: Guardian.new(user), params: params) }
    let(:params) { { per_page: 1000 } }
",bfc3132bb22bd5b7e86f428746b89c4d3d7f5a70,CVE-2023-38684,bfc3132bb22bd5b7e86f428746b89c4d3d7f5a70,https://github.com/discourse/discourse,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR(:VAR) { VAR.VAR(VAR: VAR, VAR: VAR.VAR(VAR)) } VAR = VAR.VAR(VAR: VAR, VAR: VAR.VAR(VAR), VAR: 1000) VAR(:VAR) { {} } VAR(:VAR) { VAR.VAR(VAR: VAR, VAR: VAR.VAR(VAR), VAR: VAR) } VAR(:VAR) { { VAR: 1000 } }",14,9
discourse_dcc825bda505a344eda403a1b8733f30e784034a,app/models/post.rb,"  validates :edit_reason, length: { maximum: 1000 }",,dcc825bda505a344eda403a1b8733f30e784034a,CVE-2023-37906,dcc825bda505a344eda403a1b8733f30e784034a,https://github.com/discourse/discourse,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR :VAR, VAR: { VAR: 1000 } ",14,9
discourse_dcc825bda505a344eda403a1b8733f30e784034a,spec/models/post_spec.rb,  it { is_expected.to validate_length_of(:edit_reason).is_at_most(1000) },,dcc825bda505a344eda403a1b8733f30e784034a,CVE-2023-37906,dcc825bda505a344eda403a1b8733f30e784034a,https://github.com/discourse/discourse,NVD_DIRECT_COMMIT,1337,COMPLETED,VAR { VAR.VAR VAR(:VAR).VAR(1000) } ,14,9
envoy_5c3c65651743714dfd0edaf65ba806961db2c54c,test/common/grpc/common_test.cc,"  Http::TestRequestHeaderMapImpl small_missing_unit{{""grpc-timeout"", ""1""}};
  EXPECT_EQ(absl::nullopt, Common::getGrpcTimeout(small_missing_unit));

  // Test max 8 digits to prevent millisecond overflow.
  Http::TestRequestHeaderMapImpl value_overflow{{""grpc-timeout"", ""6666666666666H""}};
  EXPECT_EQ(absl::nullopt, Common::getGrpcTimeout(value_overflow));

  // Reject negative values.
  Http::TestRequestHeaderMapImpl value_negative{{""grpc-timeout"", ""-1S""}};
  EXPECT_EQ(absl::nullopt, Common::getGrpcTimeout(value_negative));

  // Allow positive values marked with +.
  Http::TestRequestHeaderMapImpl value_positive{{""grpc-timeout"", ""+1S""}};
  EXPECT_EQ(std::chrono::milliseconds(1000), Common::getGrpcTimeout(value_positive));

  // No leading whitespace are not enforced on decode so we don't test for them.","  // Max 8 digits and no leading whitespace or +- signs are not enforced on decode,
  // so we don't test for them.",5c3c65651743714dfd0edaf65ba806961db2c54c,CVE-2021-28682,5c3c65651743714dfd0edaf65ba806961db2c54c,https://github.com/envoyproxy/envoy,NVD_GIT_REPOBASED,88,COMPLETED,"VAR::VAR VAR{{""VAR-VAR"", ""1""}}; VAR(VAR::VAR, VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""6666666666666H""}}; VAR(VAR::VAR, VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""-1S""}}; VAR(VAR::VAR, VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""+1S""}}; VAR(VAR::VAR::VAR(1000), VAR::VAR(VAR));   ",0,9
envoy_5eba69a1f375413fb93fab4173f9c393ac8c2818,test/common/buffer/owned_impl_test.cc,"    ASSERT_EQ(buffer_list.size(), buffer_slices.size());

  static void expectFirstSlice(std::vector<int> slice_description, OwnedImpl& buffer) {
    const auto& buffer_slices = buffer.describeSlicesForTest();
    ASSERT_LE(1, buffer_slices.size());
    EXPECT_EQ(buffer_slices[0].data, slice_description[0]);
    EXPECT_EQ(buffer_slices[0].reservable, slice_description[1]);
    EXPECT_EQ(buffer_slices[0].capacity, slice_description[2]);
  }
  BufferFragmentImpl frag3(input, 11, [](const void*, size_t, const BufferFragmentImpl*) {});
  buffer.addBufferFragment(frag3);
  EXPECT_EQ(22, buffer.length());

  // Cover case of copying a buffer with an empty fragment.
  Buffer::OwnedImpl buffer2;
  buffer2.add(buffer);

  // Cover copyOut
  std::unique_ptr<char[]> outbuf(new char[buffer.length()]);
  buffer.copyOut(0, buffer.length(), outbuf.get());

  buffer.drain(22);
TEST_F(OwnedImplTest, DrainTracking) {
  testing::InSequence s;

  Buffer::OwnedImpl buffer;
  buffer.add(""a"");

  testing::MockFunction<void()> tracker1;
  testing::MockFunction<void()> tracker2;
  buffer.addDrainTracker(tracker1.AsStdFunction());
  buffer.addDrainTracker(tracker2.AsStdFunction());

  testing::MockFunction<void()> done;
  EXPECT_CALL(tracker1, Call());
  EXPECT_CALL(tracker2, Call());
  EXPECT_CALL(done, Call());
  buffer.drain(buffer.length());
  done.Call();
}

TEST_F(OwnedImplTest, MoveDrainTrackersWhenTransferingSlices) {
  testing::InSequence s;

  Buffer::OwnedImpl buffer1;
  buffer1.add(""a"");

  testing::MockFunction<void()> tracker1;
  buffer1.addDrainTracker(tracker1.AsStdFunction());

  Buffer::OwnedImpl buffer2;
  buffer2.add(""b"");

  testing::MockFunction<void()> tracker2;
  buffer2.addDrainTracker(tracker2.AsStdFunction());

  buffer2.add(std::string(10000, 'c'));
  testing::MockFunction<void()> tracker3;
  buffer2.addDrainTracker(tracker3.AsStdFunction());
  EXPECT_EQ(2, buffer2.getRawSlices().size());

  buffer1.move(buffer2);
  EXPECT_EQ(10002, buffer1.length());
  EXPECT_EQ(0, buffer2.length());
  EXPECT_EQ(3, buffer1.getRawSlices().size());
  EXPECT_EQ(0, buffer2.getRawSlices().size());

  testing::MockFunction<void()> done;
  EXPECT_CALL(tracker1, Call());
  EXPECT_CALL(tracker2, Call());
  EXPECT_CALL(tracker3, Call());
  EXPECT_CALL(done, Call());
  buffer1.drain(buffer1.length());
  done.Call();
}

TEST_F(OwnedImplTest, MoveDrainTrackersWhenCopying) {
  testing::InSequence s;

  Buffer::OwnedImpl buffer1;
  buffer1.add(""a"");

  testing::MockFunction<void()> tracker1;
  buffer1.addDrainTracker(tracker1.AsStdFunction());

  Buffer::OwnedImpl buffer2;
  buffer2.add(""b"");

  testing::MockFunction<void()> tracker2;
  buffer2.addDrainTracker(tracker2.AsStdFunction());

  buffer1.move(buffer2);
  EXPECT_EQ(2, buffer1.length());
  EXPECT_EQ(0, buffer2.length());
  EXPECT_EQ(1, buffer1.getRawSlices().size());
  EXPECT_EQ(0, buffer2.getRawSlices().size());

  buffer1.drain(1);
  testing::MockFunction<void()> done;
  EXPECT_CALL(tracker1, Call());
  EXPECT_CALL(tracker2, Call());
  EXPECT_CALL(done, Call());
  buffer1.drain(1);
  done.Call();
}

TEST_F(OwnedImplTest, PartialMoveDrainTrackers) {
  testing::InSequence s;

  Buffer::OwnedImpl buffer1;
  buffer1.add(""a"");

  testing::MockFunction<void()> tracker1;
  buffer1.addDrainTracker(tracker1.AsStdFunction());

  Buffer::OwnedImpl buffer2;
  buffer2.add(""b"");

  testing::MockFunction<void()> tracker2;
  buffer2.addDrainTracker(tracker2.AsStdFunction());

  buffer2.add(std::string(10000, 'c'));
  testing::MockFunction<void()> tracker3;
  buffer2.addDrainTracker(tracker3.AsStdFunction());
  EXPECT_EQ(2, buffer2.getRawSlices().size());

  // Move the first slice and associated trackers and part of the second slice to buffer1.
  buffer1.move(buffer2, 4999);
  EXPECT_EQ(5000, buffer1.length());
  EXPECT_EQ(5002, buffer2.length());
  EXPECT_EQ(3, buffer1.getRawSlices().size());
  EXPECT_EQ(1, buffer2.getRawSlices().size());

  testing::MockFunction<void()> done;
  EXPECT_CALL(tracker1, Call());
  buffer1.drain(1);

  EXPECT_CALL(tracker2, Call());
  EXPECT_CALL(done, Call());
  buffer1.drain(buffer1.length());
  done.Call();

  // tracker3 remained in buffer2.
  EXPECT_CALL(tracker3, Call());
  buffer2.drain(buffer2.length());
}

TEST_F(OwnedImplTest, DrainTrackingOnDestruction) {
  testing::InSequence s;

  auto buffer = std::make_unique<Buffer::OwnedImpl>();
  buffer->add(""a"");

  testing::MockFunction<void()> tracker;
  buffer->addDrainTracker(tracker.AsStdFunction());

  testing::MockFunction<void()> done;
  EXPECT_CALL(tracker, Call());
  EXPECT_CALL(done, Call());
  buffer.reset();
  done.Call();
}

TEST_F(OwnedImplTest, Linearize) {
  Buffer::OwnedImpl buffer;

  // Unowned slice to track when linearize kicks in.
  std::string input(1000, 'a');
  BufferFragmentImpl frag(
      input.c_str(), input.size(),
      [this](const void*, size_t, const BufferFragmentImpl*) { release_callback_called_ = true; });
  buffer.addBufferFragment(frag);

  // Second slice with more data.
  buffer.add(std::string(1000, 'b'));

  // Linearize does not change the pointer associated with the first slice if requested size is less
  // than or equal to size of the first slice.
  EXPECT_EQ(input.c_str(), buffer.linearize(input.size()));
  EXPECT_FALSE(release_callback_called_);

  constexpr uint64_t LinearizeSize = 2000;
  void* out_ptr = buffer.linearize(LinearizeSize);
  EXPECT_TRUE(release_callback_called_);
  EXPECT_EQ(input + std::string(1000, 'b'),
            absl::string_view(reinterpret_cast<const char*>(out_ptr), LinearizeSize));
}

TEST_F(OwnedImplTest, LinearizeEmptyBuffer) {
  Buffer::OwnedImpl buffer;
  EXPECT_EQ(nullptr, buffer.linearize(0));
}

TEST_F(OwnedImplTest, LinearizeSingleSlice) {
  auto buffer = std::make_unique<Buffer::OwnedImpl>();

  // Unowned slice to track when linearize kicks in.
  std::string input(1000, 'a');
  BufferFragmentImpl frag(
      input.c_str(), input.size(),
      [this](const void*, size_t, const BufferFragmentImpl*) { release_callback_called_ = true; });
  buffer->addBufferFragment(frag);

  EXPECT_EQ(input.c_str(), buffer->linearize(buffer->length()));
  EXPECT_FALSE(release_callback_called_);

  buffer.reset();
  EXPECT_TRUE(release_callback_called_);
}

TEST_F(OwnedImplTest, LinearizeDrainTracking) {
  constexpr uint32_t SmallChunk = 200;
  constexpr uint32_t LargeChunk = 16384 - SmallChunk;
  constexpr uint32_t LinearizeSize = SmallChunk + LargeChunk;

  // Create a buffer with a eclectic combination of buffer OwnedSlice and UnownedSlices that will
  // help us explore the properties of linearize.
  Buffer::OwnedImpl buffer;

  // Large add below the target linearize size.
  testing::MockFunction<void()> tracker1;
  buffer.add(std::string(LargeChunk, 'a'));
  buffer.addDrainTracker(tracker1.AsStdFunction());

  // Unowned slice which causes some fragmentation.
  testing::MockFunction<void()> tracker2;
  testing::MockFunction<void(const void*, size_t, const BufferFragmentImpl*)>
      release_callback_tracker;
  std::string frag_input(2 * SmallChunk, 'b');
  BufferFragmentImpl frag(frag_input.c_str(), frag_input.size(),
                          release_callback_tracker.AsStdFunction());
  buffer.addBufferFragment(frag);
  buffer.addDrainTracker(tracker2.AsStdFunction());

  // And an unowned slice with 0 size, because.
  testing::MockFunction<void()> tracker3;
  testing::MockFunction<void(const void*, size_t, const BufferFragmentImpl*)>
      release_callback_tracker2;
  BufferFragmentImpl frag2(nullptr, 0, release_callback_tracker2.AsStdFunction());
  buffer.addBufferFragment(frag2);
  buffer.addDrainTracker(tracker3.AsStdFunction());

  // Add a very large chunk
  testing::MockFunction<void()> tracker4;
  buffer.add(std::string(LargeChunk + LinearizeSize, 'c'));
  buffer.addDrainTracker(tracker4.AsStdFunction());

  // Small adds that create no gaps.
  testing::MockFunction<void()> tracker5;
  for (int i = 0; i < 105; ++i) {
    buffer.add(std::string(SmallChunk, 'd'));
  }
  buffer.addDrainTracker(tracker5.AsStdFunction());

  expectSlices({{16184, 136, 16320},
                {400, 0, 400},
                {0, 0, 0},
                {32704, 0, 32704},
                {4032, 0, 4032},
                {4032, 0, 4032},
                {4032, 0, 4032},
                {4032, 0, 4032},
                {4032, 0, 4032},
                {704, 3328, 4032}},
               buffer);

  testing::InSequence s;
  testing::MockFunction<void(int, int)> drain_tracker;
  testing::MockFunction<void()> done_tracker;
  EXPECT_CALL(tracker1, Call());
  EXPECT_CALL(release_callback_tracker, Call(_, _, _));
  EXPECT_CALL(tracker2, Call());
  EXPECT_CALL(drain_tracker, Call(3 * LargeChunk + 108 * SmallChunk, 16384));
  EXPECT_CALL(release_callback_tracker2, Call(_, _, _));
  EXPECT_CALL(tracker3, Call());
  EXPECT_CALL(tracker4, Call());
  EXPECT_CALL(drain_tracker, Call(2 * LargeChunk + 107 * SmallChunk, 16384));
  EXPECT_CALL(drain_tracker, Call(LargeChunk + 106 * SmallChunk, 16384));
  EXPECT_CALL(drain_tracker, Call(105 * SmallChunk, 16384));
  EXPECT_CALL(tracker5, Call());
  EXPECT_CALL(drain_tracker, Call(4616, 4616));
  EXPECT_CALL(done_tracker, Call());
  for (auto& expected_first_slice : std::vector<std::vector<int>>{{16584, 3832, 20416},
                                                                  {32904, 3896, 36800},
                                                                  {16520, 3896, 36800},
                                                                  {20296, 120, 20416},
                                                                  {4616, 3512, 8128}}) {
    const uint32_t write_size = std::min<uint32_t>(LinearizeSize, buffer.length());
    buffer.linearize(write_size);
    expectFirstSlice(expected_first_slice, buffer);
    drain_tracker.Call(buffer.length(), write_size);
    buffer.drain(write_size);
  }
  done_tracker.Call();

  expectSlices({}, buffer);
}

    expectSlices({{1, 4031, 4032}}, buffer);
    expectSlices({{1, 4031, 4032}, {0, 4032, 4032}}, buffer);
    expectSlices({{1, 4031, 4032}, {0, 4032, 4032}, {0, 4032, 4032}}, buffer);
    expectSlices({{1, 4031, 4032}}, buffer);
    expectSlices({{1, 4031, 4032}, {12, 0, 12}, {0, 4032, 4032}}, buffer);
  expectSlices({{8000, 4224, 12224}, {0, 12224, 12224}}, buffer);
  expectSlices({{8001, 4223, 12224}}, buffer);
  expectSlices({{8001, 4223, 12224}, {0, 12224, 12224}}, buffer);
  expectSlices({{0, 12224, 12224}, {0, 8128, 8128}}, buffer);
  EXPECT_EQ(12224, iovecs[0].len_);
  expectSlices({{0, 12224, 12224}, {0, 8128, 8128}, {0, 20416, 20416}}, buffer);
  expectSlices({{0, 12224, 12224}, {0, 8128, 8128}, {0, 20416, 20416}}, buffer);
  EXPECT_EQ(12224, iovecs[0].len_);
  expectSlices({{0, 12224, 12224}, {0, 8128, 8128}, {0, 20416, 20416}, {0, 20416, 20416}}, buffer);
  expectSlices({{12224, 0, 12224}, {0, 8128, 8128}, {0, 20416, 20416}, {17776, 2640, 20416}},
  expectSlices({{12224, 0, 12224},
                {0, 8128, 8128},
                {0, 20416, 20416},
                {17776, 2640, 20416},
                {0, 16320, 16320}},
  expectSlices({{12224, 0, 12224},
                {0, 8128, 8128},
                {0, 20416, 20416},
                {20416, 0, 20416},
                {13744, 2576, 16320}},
  expectSlices({{5, 0, 4032}, {1953, 2079, 4032}}, buf);
  expectSlices({{6, 4026, 4032}}, buf);","  buffer.drain(11);
    expectSlices({{1, 4055, 4056}}, buffer);
    expectSlices({{1, 4055, 4056}, {0, 4056, 4056}}, buffer);
    expectSlices({{1, 4055, 4056}, {0, 4056, 4056}, {0, 4056, 4056}}, buffer);
    expectSlices({{1, 4055, 4056}}, buffer);
    expectSlices({{1, 4055, 4056}, {12, 0, 12}, {0, 4056, 4056}}, buffer);
  expectSlices({{8000, 4248, 12248}, {0, 12248, 12248}}, buffer);
  expectSlices({{8001, 4247, 12248}}, buffer);
  expectSlices({{8001, 4247, 12248}, {0, 12248, 12248}}, buffer);
  expectSlices({{0, 12248, 12248}, {0, 8152, 8152}}, buffer);
  EXPECT_EQ(12248, iovecs[0].len_);
  expectSlices({{0, 12248, 12248}, {0, 8152, 8152}, {0, 20440, 20440}}, buffer);
  expectSlices({{0, 12248, 12248}, {0, 8152, 8152}, {0, 20440, 20440}}, buffer);
  EXPECT_EQ(12248, iovecs[0].len_);
  expectSlices({{0, 12248, 12248}, {0, 8152, 8152}, {0, 20440, 20440}, {0, 20440, 20440}}, buffer);
  expectSlices({{12248, 0, 12248}, {0, 8152, 8152}, {0, 20440, 20440}, {17752, 2688, 20440}},
  expectSlices({{12248, 0, 12248},
                {0, 8152, 8152},
                {0, 20440, 20440},
                {17752, 2688, 20440},
                {0, 16344, 16344}},
  expectSlices({{12248, 0, 12248},
                {0, 8152, 8152},
                {0, 20440, 20440},
                {20440, 0, 20440},
                {13696, 2648, 16344}},
  expectSlices({{5, 0, 4056}, {1953, 2103, 4056}}, buf);
  expectSlices({{6, 4050, 4056}}, buf);",5eba69a1f375413fb93fab4173f9c393ac8c2818,CVE-2020-12604,5eba69a1f375413fb93fab4173f9c393ac8c2818,https://github.com/envoyproxy/envoy,NVD_GIT_REPOBASED,68,COMPLETED,"VAR(VAR.VAR(), VAR.VAR()); VAR VAR VAR(VAR::VAR<VAR> VAR, VAR& VAR) { VAR VAR& VAR = VAR.VAR(); VAR(1, VAR.VAR()); VAR(VAR[0].VAR, VAR[0]); VAR(VAR[0].VAR, VAR[1]); VAR(VAR[0].VAR, VAR[2]); } VAR VAR(VAR, 11, [](VAR VAR*, VAR, VAR VAR*) {}); VAR.VAR(VAR); VAR(22, VAR.VAR());  VAR::VAR VAR; VAR.VAR(VAR);  VAR::VAR<VAR[]> VAR(VAR VAR[VAR.VAR()]); VAR.VAR(0, VAR.VAR(), VAR.VAR()); VAR.VAR(22); VAR(VAR, VAR) { VAR::VAR VAR; VAR::VAR VAR; VAR.VAR(""VAR""); VAR::VAR<VAR()> VAR; VAR::VAR<VAR()> VAR; VAR.VAR(VAR.VAR()); VAR.VAR(VAR.VAR()); VAR::VAR<VAR()> VAR; VAR(VAR, VAR()); VAR(VAR, VAR()); VAR(VAR, VAR()); VAR.VAR(VAR.VAR()); VAR.VAR(); } VAR(VAR, VAR) { VAR::VAR VAR; VAR::VAR VAR; VAR.VAR(""VAR""); VAR::VAR<VAR()> VAR; VAR.VAR(VAR.VAR()); VAR::VAR VAR; VAR.VAR(""VAR""); VAR::VAR<VAR()> VAR; VAR.VAR(VAR.VAR()); VAR.VAR(VAR::VAR(10000, 'VAR')); VAR::VAR<VAR()> VAR; VAR.VAR(VAR.VAR()); VAR(2, VAR.VAR().VAR()); VAR.VAR(VAR); VAR(10002, VAR.VAR()); VAR(0, VAR.VAR()); VAR(3, VAR.VAR().VAR()); VAR(0, VAR.VAR().VAR()); VAR::VAR<VAR()> VAR; VAR(VAR, VAR()); VAR(VAR, VAR()); VAR(VAR, VAR()); VAR(VAR, VAR()); VAR.VAR(VAR.VAR()); VAR.VAR(); } VAR(VAR, VAR) { VAR::VAR VAR; VAR::VAR VAR; VAR.VAR(""VAR""); VAR::VAR<VAR()> VAR; VAR.VAR(VAR.VAR()); VAR::VAR VAR; VAR.VAR(""VAR""); VAR::VAR<VAR()> VAR; VAR.VAR(VAR.VAR()); VAR.VAR(VAR); VAR(2, VAR.VAR()); VAR(0, VAR.VAR()); VAR(1, VAR.VAR().VAR()); VAR(0, VAR.VAR().VAR()); VAR.VAR(1); VAR::VAR<VAR()> VAR; VAR(VAR, VAR()); VAR(VAR, VAR()); VAR(VAR, VAR()); VAR.VAR(1); VAR.VAR(); } VAR(VAR, VAR) { VAR::VAR VAR; VAR::VAR VAR; VAR.VAR(""VAR""); VAR::VAR<VAR()> VAR; VAR.VAR(VAR.VAR()); VAR::VAR VAR; VAR.VAR(""VAR""); VAR::VAR<VAR()> VAR; VAR.VAR(VAR.VAR()); VAR.VAR(VAR::VAR(10000, 'VAR')); VAR::VAR<VAR()> VAR; VAR.VAR(VAR.VAR()); VAR(2, VAR.VAR().VAR());  VAR.VAR(VAR, 4999); VAR(5000, VAR.VAR()); VAR(5002, VAR.VAR()); VAR(3, VAR.VAR().VAR()); VAR(1, VAR.VAR().VAR()); VAR::VAR<VAR()> VAR; VAR(VAR, VAR()); VAR.VAR(1); VAR(VAR, VAR()); VAR(VAR, VAR()); VAR.VAR(VAR.VAR()); VAR.VAR();  VAR(VAR, VAR()); VAR.VAR(VAR.VAR()); } VAR(VAR, VAR) { VAR::VAR VAR; VAR VAR = VAR::VAR<VAR::VAR>(); VAR->VAR(""VAR""); VAR::VAR<VAR()> VAR; VAR->VAR(VAR.VAR()); VAR::VAR<VAR()> VAR; VAR(VAR, VAR()); VAR(VAR, VAR()); VAR.VAR(); VAR.VAR(); } VAR(VAR, VAR) { VAR::VAR VAR;  VAR::VAR VAR(1000, 'VAR'); VAR VAR( VAR.VAR(), VAR.VAR(), [VAR](VAR VAR*, VAR, VAR VAR*) { VAR = VAR; }); VAR.VAR(VAR);  VAR.VAR(VAR::VAR(1000, 'VAR'));   VAR(VAR.VAR(), VAR.VAR(VAR.VAR())); VAR(VAR); VAR VAR VAR = 2000; VAR* VAR = VAR.VAR(VAR); VAR(VAR); VAR(VAR + VAR::VAR(1000, 'VAR'), VAR::VAR(VAR<VAR VAR*>(VAR), VAR)); } VAR(VAR, VAR) { VAR::VAR VAR; VAR(VAR, VAR.VAR(0)); } VAR(VAR, VAR) { VAR VAR = VAR::VAR<VAR::VAR>();  VAR::VAR VAR(1000, 'VAR'); VAR VAR( VAR.VAR(), VAR.VAR(), [VAR](VAR VAR*, VAR, VAR VAR*) { VAR = VAR; }); VAR->VAR(VAR); VAR(VAR.VAR(), VAR->VAR(VAR->VAR())); VAR(VAR); VAR.VAR(); VAR(VAR); } VAR(VAR, VAR) { VAR VAR VAR = 200; VAR VAR VAR = 16384 - VAR; VAR VAR VAR = VAR + VAR;   VAR::VAR VAR;  VAR::VAR<VAR()> VAR; VAR.VAR(VAR::VAR(VAR, 'VAR')); VAR.VAR(VAR.VAR());  VAR::VAR<VAR()> VAR; VAR::VAR<VAR(VAR VAR*, VAR, VAR VAR*)> VAR; VAR::VAR VAR(2 * VAR, 'VAR'); VAR VAR(VAR.VAR(), VAR.VAR(), VAR.VAR()); VAR.VAR(VAR); VAR.VAR(VAR.VAR());  VAR::VAR<VAR()> VAR; VAR::VAR<VAR(VAR VAR*, VAR, VAR VAR*)> VAR; VAR VAR(VAR, 0, VAR.VAR()); VAR.VAR(VAR); VAR.VAR(VAR.VAR());  VAR::VAR<VAR()> VAR; VAR.VAR(VAR::VAR(VAR + VAR, 'VAR')); VAR.VAR(VAR.VAR());  VAR::VAR<VAR()> VAR; VAR (VAR VAR = 0; VAR < 105; ++VAR) { VAR.VAR(VAR::VAR(VAR, 'VAR')); } VAR.VAR(VAR.VAR()); VAR({{16184, 136, 16320}, {400, 0, 400}, {0, 0, 0}, {32704, 0, 32704}, {4032, 0, 4032}, {4032, 0, 4032}, {4032, 0, 4032}, {4032, 0, 4032}, {4032, 0, 4032}, {704, 3328, 4032}}, VAR); VAR::VAR VAR; VAR::VAR<VAR(VAR, VAR)> VAR; VAR::VAR<VAR()> VAR; VAR(VAR, VAR()); VAR(VAR, VAR(VAR, VAR, VAR)); VAR(VAR, VAR()); VAR(VAR, VAR(3 * VAR + 108 * VAR, 16384)); VAR(VAR, VAR(VAR, VAR, VAR)); VAR(VAR, VAR()); VAR(VAR, VAR()); VAR(VAR, VAR(2 * VAR + 107 * VAR, 16384)); VAR(VAR, VAR(VAR + 106 * VAR, 16384)); VAR(VAR, VAR(105 * VAR, 16384)); VAR(VAR, VAR()); VAR(VAR, VAR(4616, 4616)); VAR(VAR, VAR()); VAR (VAR& VAR : VAR::VAR<VAR::VAR<VAR>>{{16584, 3832, 20416}, {32904, 3896, 36800}, {16520, 3896, 36800}, {20296, 120, 20416}, {4616, 3512, 8128}}) { VAR VAR VAR = VAR::VAR<VAR>(VAR, VAR.VAR()); VAR.VAR(VAR); VAR(VAR, VAR); VAR.VAR(VAR.VAR(), VAR); VAR.VAR(VAR); } VAR.VAR(); VAR({}, VAR); } VAR({{1, 4031, 4032}}, VAR); VAR({{1, 4031, 4032}, {0, 4032, 4032}}, VAR); VAR({{1, 4031, 4032}, {0, 4032, 4032}, {0, 4032, 4032}}, VAR); VAR({{1, 4031, 4032}}, VAR); VAR({{1, 4031, 4032}, {12, 0, 12}, {0, 4032, 4032}}, VAR); VAR({{8000, 4224, 12224}, {0, 12224, 12224}}, VAR); VAR({{8001, 4223, 12224}}, VAR); VAR({{8001, 4223, 12224}, {0, 12224, 12224}}, VAR); VAR({{0, 12224, 12224}, {0, 8128, 8128}}, VAR); VAR(12224, VAR[0].VAR); VAR({{0, 12224, 12224}, {0, 8128, 8128}, {0, 20416, 20416}}, VAR); VAR({{0, 12224, 12224}, {0, 8128, 8128}, {0, 20416, 20416}}, VAR); VAR(12224, VAR[0].VAR); VAR({{0, 12224, 12224}, {0, 8128, 8128}, {0, 20416, 20416}, {0, 20416, 20416}}, VAR); VAR({{12224, 0, 12224}, {0, 8128, 8128}, {0, 20416, 20416}, {17776, 2640, 20416}}, VAR({{12224, 0, 12224}, {0, 8128, 8128}, {0, 20416, 20416}, {17776, 2640, 20416}, {0, 16320, 16320}}, VAR({{12224, 0, 12224}, {0, 8128, 8128}, {0, 20416, 20416}, {20416, 0, 20416}, {13744, 2576, 16320}}, VAR({{5, 0, 4032}, {1953, 2079, 4032}}, VAR); VAR({{6, 4026, 4032}}, VAR); VAR.VAR(11); VAR({{1, 4055, 4056}}, VAR); VAR({{1, 4055, 4056}, {0, 4056, 4056}}, VAR); VAR({{1, 4055, 4056}, {0, 4056, 4056}, {0, 4056, 4056}}, VAR); VAR({{1, 4055, 4056}}, VAR); VAR({{1, 4055, 4056}, {12, 0, 12}, {0, 4056, 4056}}, VAR); VAR({{8000, 4248, 12248}, {0, 12248, 12248}}, VAR); VAR({{8001, 4247, 12248}}, VAR); VAR({{8001, 4247, 12248}, {0, 12248, 12248}}, VAR); VAR({{0, 12248, 12248}, {0, 8152, 8152}}, VAR); VAR(12248, VAR[0].VAR); VAR({{0, 12248, 12248}, {0, 8152, 8152}, {0, 20440, 20440}}, VAR); VAR({{0, 12248, 12248}, {0, 8152, 8152}, {0, 20440, 20440}}, VAR); VAR(12248, VAR[0].VAR); VAR({{0, 12248, 12248}, {0, 8152, 8152}, {0, 20440, 20440}, {0, 20440, 20440}}, VAR); VAR({{12248, 0, 12248}, {0, 8152, 8152}, {0, 20440, 20440}, {17752, 2688, 20440}}, VAR({{12248, 0, 12248}, {0, 8152, 8152}, {0, 20440, 20440}, {17752, 2688, 20440}, {0, 16344, 16344}}, VAR({{12248, 0, 12248}, {0, 8152, 8152}, {0, 20440, 20440}, {20440, 0, 20440}, {13696, 2648, 16344}}, VAR({{5, 0, 4056}, {1953, 2103, 4056}}, VAR); VAR({{6, 4050, 4056}}, VAR);",14,9
envoy_8d8603a69dc7a3b627819420d4005ed6b0a34a30,test/common/grpc/common_test.cc,"  Http::TestRequestHeaderMapImpl small_missing_unit{{""grpc-timeout"", ""1""}};
  EXPECT_EQ(std::chrono::milliseconds(0), Common::getGrpcTimeout(small_missing_unit));

  Http::TestRequestHeaderMapImpl value_overflow{{""grpc-timeout"", ""6666666666666H""}};
  EXPECT_EQ(std::chrono::milliseconds(0), Common::getGrpcTimeout(value_overflow));

  // Reject negative values.
  Http::TestRequestHeaderMapImpl value_negative{{""grpc-timeout"", ""-1S""}};
  EXPECT_EQ(std::chrono::milliseconds(0), Common::getGrpcTimeout(value_negative));

  // Allow positive values marked with +.
  Http::TestRequestHeaderMapImpl value_positive{{""grpc-timeout"", ""+1S""}};
  EXPECT_EQ(std::chrono::milliseconds(1000), Common::getGrpcTimeout(value_positive));

  // No leading whitespace are not enforced on decode so we don't test for them.","  // Max 8 digits and no leading whitespace or +- signs are not enforced on decode,
  // so we don't test for them.",8d8603a69dc7a3b627819420d4005ed6b0a34a30,CVE-2021-28682,8d8603a69dc7a3b627819420d4005ed6b0a34a30,https://github.com/envoyproxy/envoy,NVD_GIT_REPOBASED,90,COMPLETED,"VAR::VAR VAR{{""VAR-VAR"", ""1""}}; VAR(VAR::VAR::VAR(0), VAR::VAR(VAR)); VAR::VAR VAR{{""VAR-VAR"", ""6666666666666H""}}; VAR(VAR::VAR::VAR(0), VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""-1S""}}; VAR(VAR::VAR::VAR(0), VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""+1S""}}; VAR(VAR::VAR::VAR(1000), VAR::VAR(VAR));   ",0,9
envoy_96e889dd8a0446ab2ecf379338f44bed205771a0,test/common/grpc/common_test.cc,"  Http::TestRequestHeaderMapImpl small_missing_unit{{""grpc-timeout"", ""1""}};
  EXPECT_EQ(std::chrono::milliseconds(0), Common::getGrpcTimeout(small_missing_unit));

  Http::TestRequestHeaderMapImpl value_overflow{{""grpc-timeout"", ""6666666666666H""}};
  EXPECT_EQ(std::chrono::milliseconds(0), Common::getGrpcTimeout(value_overflow));

  // Reject negative values.
  Http::TestRequestHeaderMapImpl value_negative{{""grpc-timeout"", ""-1S""}};
  EXPECT_EQ(std::chrono::milliseconds(0), Common::getGrpcTimeout(value_negative));

  // Allow positive values marked with +.
  Http::TestRequestHeaderMapImpl value_positive{{""grpc-timeout"", ""+1S""}};
  EXPECT_EQ(std::chrono::milliseconds(1000), Common::getGrpcTimeout(value_positive));

  // No leading whitespace are not enforced on decode so we don't test for them.","  // Max 8 digits and no leading whitespace or +- signs are not enforced on decode,
  // so we don't test for them.",96e889dd8a0446ab2ecf379338f44bed205771a0,CVE-2021-28682,96e889dd8a0446ab2ecf379338f44bed205771a0,https://github.com/envoyproxy/envoy,NVD_GIT_REPOBASED,90,COMPLETED,"VAR::VAR VAR{{""VAR-VAR"", ""1""}}; VAR(VAR::VAR::VAR(0), VAR::VAR(VAR)); VAR::VAR VAR{{""VAR-VAR"", ""6666666666666H""}}; VAR(VAR::VAR::VAR(0), VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""-1S""}}; VAR(VAR::VAR::VAR(0), VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""+1S""}}; VAR(VAR::VAR::VAR(1000), VAR::VAR(VAR));   ",0,9
envoy_cb4ef0b09200c720dfdb07e097092dd105450343,test/extensions/compression/gzip/decompressor/zlib_decompressor_impl_test.cc,"// Detect excessive compression ratio by compressing a long whitespace string
// into a very small chunk of data and decompressing it again.
TEST_F(ZlibDecompressorImplTest, DetectExcessiveCompressionRatio) {
  const absl::string_view ten_whitespaces = ""          "";
  Buffer::OwnedImpl buffer;
  Extensions::Compression::Gzip::Compressor::ZlibCompressorImpl compressor;
  compressor.init(
      Extensions::Compression::Gzip::Compressor::ZlibCompressorImpl::CompressionLevel::Standard,
      Extensions::Compression::Gzip::Compressor::ZlibCompressorImpl::CompressionStrategy::Standard,
      gzip_window_bits, memory_level);

  for (int i = 0; i < 1000; i++) {
    buffer.add(ten_whitespaces);
  }

  compressor.compress(buffer, Envoy::Compression::Compressor::State::Finish);

  Buffer::OwnedImpl output_buffer;
  Stats::IsolatedStoreImpl stats_store{};
  ZlibDecompressorImpl decompressor{stats_store, ""test.""};
  decompressor.init(gzip_window_bits);
  decompressor.decompress(buffer, output_buffer);
  ASSERT_EQ(stats_store.counterFromString(""test.zlib_data_error"").value(), 1);
}
",,cb4ef0b09200c720dfdb07e097092dd105450343,CVE-2022-29225,cb4ef0b09200c720dfdb07e097092dd105450343,https://github.com/envoyproxy/envoy,NVD_DIRECT_COMMIT,1337,COMPLETED,"  VAR(VAR, VAR) { VAR VAR::VAR VAR = "" ""; VAR::VAR VAR; VAR::VAR::VAR::VAR::VAR VAR; VAR.VAR( VAR::VAR::VAR::VAR::VAR::VAR::VAR, VAR::VAR::VAR::VAR::VAR::VAR::VAR, VAR, VAR); VAR (VAR VAR = 0; VAR < 1000; VAR++) { VAR.VAR(VAR); } VAR.VAR(VAR, VAR::VAR::VAR::VAR::VAR); VAR::VAR VAR; VAR::VAR VAR{}; VAR VAR{VAR, ""VAR.""}; VAR.VAR(VAR); VAR.VAR(VAR, VAR); VAR(VAR.VAR(""VAR.VAR"").VAR(), 1); } ",14,9
envoy_e383908bf29ce78d196b2f6cd4f41e7862acf6e8,test/common/grpc/common_test.cc,"  Http::TestRequestHeaderMapImpl small_missing_unit{{""grpc-timeout"", ""1""}};
  EXPECT_EQ(absl::nullopt, Common::getGrpcTimeout(small_missing_unit));

  // Test max 8 digits to prevent millisecond overflow.
  Http::TestRequestHeaderMapImpl value_overflow{{""grpc-timeout"", ""6666666666666H""}};
  EXPECT_EQ(absl::nullopt, Common::getGrpcTimeout(value_overflow));

  // Reject negative values.
  Http::TestRequestHeaderMapImpl value_negative{{""grpc-timeout"", ""-1S""}};
  EXPECT_EQ(absl::nullopt, Common::getGrpcTimeout(value_negative));

  // Allow positive values marked with +.
  Http::TestRequestHeaderMapImpl value_positive{{""grpc-timeout"", ""+1S""}};
  EXPECT_EQ(std::chrono::milliseconds(1000), Common::getGrpcTimeout(value_positive));

  // No leading whitespace are not enforced on decode so we don't test for them.","  // Max 8 digits and no leading whitespace or +- signs are not enforced on decode,
  // so we don't test for them.",e383908bf29ce78d196b2f6cd4f41e7862acf6e8,CVE-2021-28682,e383908bf29ce78d196b2f6cd4f41e7862acf6e8,https://github.com/envoyproxy/envoy,NVD_GIT_REPOBASED,90,COMPLETED,"VAR::VAR VAR{{""VAR-VAR"", ""1""}}; VAR(VAR::VAR, VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""6666666666666H""}}; VAR(VAR::VAR, VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""-1S""}}; VAR(VAR::VAR, VAR::VAR(VAR));  VAR::VAR VAR{{""VAR-VAR"", ""+1S""}}; VAR(VAR::VAR::VAR(1000), VAR::VAR(VAR));   ",0,9
FFmpeg_7ba100d3e6e8b1e5d5342feb960a7f081d6e15af,doc/demuxers.texi,"@section hls

HLS demuxer

It accepts the following options:

@table @option
@item live_start_index
segment index to start live streams at (negative values are from the end).

@item allowed_extensions
',' separated list of file extensions that hls is allowed to access.

@item max_reload
Maximum number of times a insufficient list is attempted to be reloaded.
Default value is 1000.
@end table
",,7ba100d3e6e8b1e5d5342feb960a7f081d6e15af,CVE-2017-14058,7ba100d3e6e8b1e5d5342feb960a7f081d6e15af,https://github.com/FFmpeg/FFmpeg,NVD_DIRECT_COMMIT,1337,COMPLETED,"@VAR VAR VAR VAR VAR VAR VAR VAR VAR: @VAR @VAR @VAR VAR VAR VAR VAR VAR VAR VAR VAR (VAR VAR VAR VAR VAR VAR). @VAR VAR ',' VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. @VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR 1000. @VAR VAR ",14,9
FFmpeg_7ba100d3e6e8b1e5d5342feb960a7f081d6e15af,libavformat/hls.c,"    int max_reload;
    int reload_count = 0;
        reload_count++;
        if (reload_count > c->max_reload)
            return AVERROR_EOF;
    {""max_reload"", ""Maximum number of times a insufficient list is attempted to be reloaded"",
        OFFSET(max_reload), AV_OPT_TYPE_INT, {.i64 = 1000}, 0, INT_MAX, FLAGS},",,7ba100d3e6e8b1e5d5342feb960a7f081d6e15af,CVE-2017-14058,7ba100d3e6e8b1e5d5342feb960a7f081d6e15af,https://github.com/FFmpeg/FFmpeg,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR VAR; VAR VAR = 0; VAR++; VAR (VAR > VAR->VAR) VAR VAR; {""VAR"", ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"", VAR(VAR), VAR, {.VAR = 1000}, 0, VAR, VAR}, ",14,9
FFmpeg_7ec414892ddcad88313848494b6fc5f437c9ca4a,doc/demuxers.texi,"@section hls

HLS demuxer

It accepts the following options:

@table @option
@item live_start_index
segment index to start live streams at (negative values are from the end).

@item allowed_extensions
',' separated list of file extensions that hls is allowed to access.

@item max_reload
Maximum number of times a insufficient list is attempted to be reloaded.
Default value is 1000.
@end table
",,7ec414892ddcad88313848494b6fc5f437c9ca4a,CVE-2017-14058,7ec414892ddcad88313848494b6fc5f437c9ca4a,https://github.com/FFmpeg/FFmpeg,NVD_DIRECT_COMMIT,1337,COMPLETED,"@VAR VAR VAR VAR VAR VAR VAR VAR VAR: @VAR @VAR @VAR VAR VAR VAR VAR VAR VAR VAR VAR (VAR VAR VAR VAR VAR VAR). @VAR VAR ',' VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. @VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR 1000. @VAR VAR ",14,9
FFmpeg_7ec414892ddcad88313848494b6fc5f437c9ca4a,libavformat/hls.c,"    int max_reload;
    int reload_count = 0;
        reload_count++;
        if (reload_count > c->max_reload)
            return AVERROR_EOF;
    {""max_reload"", ""Maximum number of times a insufficient list is attempted to be reloaded"",
        OFFSET(max_reload), AV_OPT_TYPE_INT, {.i64 = 1000}, 0, INT_MAX, FLAGS},",,7ec414892ddcad88313848494b6fc5f437c9ca4a,CVE-2017-14058,7ec414892ddcad88313848494b6fc5f437c9ca4a,https://github.com/FFmpeg/FFmpeg,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR VAR; VAR VAR = 0; VAR++; VAR (VAR > VAR->VAR) VAR VAR; {""VAR"", ""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR"", VAR(VAR), VAR, {.VAR = 1000}, 0, VAR, VAR}, ",14,9
FFmpeg_d6af26c55c1ea30f85a7d9edbc373f53be1743ee,libavutil/lzo.c,"        while (!(x = get_byte(c))) {
            if (cnt >= INT_MAX - 1000) {
                c->error |= AV_LZO_ERROR;
                break;
            }
        }",        while (!(x = get_byte(c))),d6af26c55c1ea30f85a7d9edbc373f53be1743ee,CVE-2014-4610,d6af26c55c1ea30f85a7d9edbc373f53be1743ee,https://github.com/FFmpeg/FFmpeg,CPE_GIT_REPOBASED,82,COMPLETED,VAR (!(VAR = VAR(VAR))) { VAR (VAR >= VAR - 1000) { VAR->VAR |= VAR; VAR; } } VAR (!(VAR = VAR(VAR))),14,9
go_1e43cfa15b4b618812e85c00c9e92c2615b324c8,src/mime/multipart/formdata.go,"	""strconv""
var (
	multipartFiles    = godebug.New(""multipartfiles"")
	multipartMaxParts = godebug.New(""multipartmaxparts"")
)
	combineFiles := true
	if multipartFiles.Value() == ""distinct"" {
		combineFiles = false
		multipartFiles.IncNonDefault()
	}
	maxParts := 1000
	if s := multipartMaxParts.Value(); s != """" {
		if v, err := strconv.Atoi(s); err == nil && v >= 0 {
			maxParts = v
			multipartMaxParts.IncNonDefault()
		}
	}
	maxHeaders := maxMIMEHeaders()

		p, err := r.nextPart(false, maxMemoryBytes, maxHeaders)
		if maxParts <= 0 {
			return nil, ErrMessageTooLarge
		}
		maxParts--
		for _, v := range p.Header {
			maxHeaders -= int64(len(v))
		}","var multipartFiles = godebug.New(""multipartfiles"")
	combineFiles := multipartFiles.Value() != ""distinct""
		p, err := r.nextPart(false, maxMemoryBytes)",1e43cfa15b4b618812e85c00c9e92c2615b324c8,CVE-2023-24536,1e43cfa15b4b618812e85c00c9e92c2615b324c8,https://github.com/golang/go,CPE_GIT_REPOBASED,134,COMPLETED,"""VAR"" VAR ( VAR = VAR.VAR(""VAR"") VAR = VAR.VAR(""VAR"") ) VAR := VAR VAR VAR.VAR() == ""VAR"" { VAR = VAR VAR.VAR() } VAR := 1000 VAR VAR := VAR.VAR(); VAR != """" { VAR VAR, VAR := VAR.VAR(VAR); VAR == VAR && VAR >= 0 { VAR = VAR VAR.VAR() } } VAR := VAR() VAR, VAR := VAR.VAR(VAR, VAR, VAR) VAR VAR <= 0 { VAR VAR, VAR } VAR-- VAR VAR, VAR := VAR VAR.VAR { VAR -= VAR(VAR(VAR)) } VAR VAR = VAR.VAR(""VAR"") VAR := VAR.VAR() != ""VAR"" VAR, VAR := VAR.VAR(VAR, VAR)",14,9
go_3991f6c41c7dfd167e889234c0cf1d840475e93c,src/net/textproto/reader.go,"	hint := r.upcomingHeaderKeys()
		if hint > 1000 {
			hint = 1000 // set a cap to avoid overallocation
		}
// upcomingHeaderKeys returns an approximation of the number of keys
func (r *Reader) upcomingHeaderKeys() (n int) {
	for len(peek) > 0 && n < 1000 {
		var line []byte
		line, peek, _ = bytes.Cut(peek, nl)
		if len(line) == 0 || (len(line) == 1 && line[0] == '\r') {
			// Blank line separating headers from the body.
			break
		}
		if line[0] == ' ' || line[0] == '\t' {
			// Folded continuation of the previous line.
			continue
		}
		n++
	}
	return n","	hint := r.upcomingHeaderNewlines()
// upcomingHeaderNewlines returns an approximation of the number of newlines
func (r *Reader) upcomingHeaderNewlines() (n int) {
	return bytes.Count(peek, nl)",3991f6c41c7dfd167e889234c0cf1d840475e93c,CVE-2023-24534,3991f6c41c7dfd167e889234c0cf1d840475e93c,https://github.com/golang/go,CPE_GIT_REPOBASED,218,COMPLETED,"VAR := VAR.VAR() VAR VAR > 1000 { VAR = 1000  }  VAR (VAR *VAR) VAR() (VAR VAR) { VAR VAR(VAR) > 0 && VAR < 1000 { VAR VAR []VAR VAR, VAR, VAR = VAR.VAR(VAR, VAR) VAR VAR(VAR) == 0 || (VAR(VAR) == 1 && VAR[0] == '\VAR') {  VAR } VAR VAR[0] == ' ' || VAR[0] == '\VAR' {  VAR } VAR++ } VAR VAR VAR := VAR.VAR()  VAR (VAR *VAR) VAR() (VAR VAR) { VAR VAR.VAR(VAR, VAR)",14,9
go_66ae75ff86950ae55ca1add47fa95b5576717be0,src/net/textproto/reader.go,"	hint := r.upcomingHeaderKeys()
		if hint > 1000 {
			hint = 1000 // set a cap to avoid overallocation
		}
// upcomingHeaderKeys returns an approximation of the number of keys
func (r *Reader) upcomingHeaderKeys() (n int) {
	for len(peek) > 0 && n < 1000 {
		var line []byte
		line, peek, _ = bytes.Cut(peek, nl)
		if len(line) == 0 || (len(line) == 1 && line[0] == '\r') {
			// Blank line separating headers from the body.
			break
		}
		if line[0] == ' ' || line[0] == '\t' {
			// Folded continuation of the previous line.
			continue
		}
		n++
	}
	return n","	hint := r.upcomingHeaderNewlines()
// upcomingHeaderNewlines returns an approximation of the number of newlines
func (r *Reader) upcomingHeaderNewlines() (n int) {
	return bytes.Count(peek, nl)",66ae75ff86950ae55ca1add47fa95b5576717be0,CVE-2023-24534,66ae75ff86950ae55ca1add47fa95b5576717be0,https://github.com/golang/go,CPE_GIT_REPOBASED,118,COMPLETED,"VAR := VAR.VAR() VAR VAR > 1000 { VAR = 1000  }  VAR (VAR *VAR) VAR() (VAR VAR) { VAR VAR(VAR) > 0 && VAR < 1000 { VAR VAR []VAR VAR, VAR, VAR = VAR.VAR(VAR, VAR) VAR VAR(VAR) == 0 || (VAR(VAR) == 1 && VAR[0] == '\VAR') {  VAR } VAR VAR[0] == ' ' || VAR[0] == '\VAR' {  VAR } VAR++ } VAR VAR VAR := VAR.VAR()  VAR (VAR *VAR) VAR() (VAR VAR) { VAR VAR.VAR(VAR, VAR)",14,9
go_7917b5f31204528ea72e0629f0b7d52b35b27538,src/mime/multipart/formdata.go,"	""strconv""
	maxParts := 1000
	multipartMaxParts := godebug.Get(""multipartmaxparts"")
	if multipartMaxParts != """" {
		if v, err := strconv.Atoi(multipartMaxParts); err == nil && v >= 0 {
			maxParts = v
		}
	}
	maxHeaders := maxMIMEHeaders()

		p, err := r.nextPart(false, maxMemoryBytes, maxHeaders)
		if maxParts <= 0 {
			return nil, ErrMessageTooLarge
		}
		maxParts--
		for _, v := range p.Header {
			maxHeaders -= int64(len(v))
		}","		p, err := r.nextPart(false, maxMemoryBytes)",7917b5f31204528ea72e0629f0b7d52b35b27538,CVE-2023-24536,7917b5f31204528ea72e0629f0b7d52b35b27538,https://github.com/golang/go,CPE_GIT_REPOBASED,166,COMPLETED,"""VAR"" VAR := 1000 VAR := VAR.VAR(""VAR"") VAR VAR != """" { VAR VAR, VAR := VAR.VAR(VAR); VAR == VAR && VAR >= 0 { VAR = VAR } } VAR := VAR() VAR, VAR := VAR.VAR(VAR, VAR, VAR) VAR VAR <= 0 { VAR VAR, VAR } VAR-- VAR VAR, VAR := VAR VAR.VAR { VAR -= VAR(VAR(VAR)) } VAR, VAR := VAR.VAR(VAR, VAR)",14,9
go_bf8c7c575c8a552d9d79deb29e80854dc88528d0,src/mime/multipart/formdata.go,"	""strconv""
var (
	multipartFiles    = godebug.New(""multipartfiles"")
	multipartMaxParts = godebug.New(""multipartmaxparts"")
)
	combineFiles := true
	if multipartFiles.Value() == ""distinct"" {
		combineFiles = false
	}
	maxParts := 1000
	if s := multipartMaxParts.Value(); s != """" {
		if v, err := strconv.Atoi(s); err == nil && v >= 0 {
			maxParts = v
		}
	}
	maxHeaders := maxMIMEHeaders()

		p, err := r.nextPart(false, maxMemoryBytes, maxHeaders)
		if maxParts <= 0 {
			return nil, ErrMessageTooLarge
		}
		maxParts--
		for _, v := range p.Header {
			maxHeaders -= int64(len(v))
		}","var multipartFiles = godebug.New(""multipartfiles"")
	combineFiles := multipartFiles.Value() != ""distinct""
		p, err := r.nextPart(false, maxMemoryBytes)",bf8c7c575c8a552d9d79deb29e80854dc88528d0,CVE-2023-24536,bf8c7c575c8a552d9d79deb29e80854dc88528d0,https://github.com/golang/go,CPE_GIT_REPOBASED,230,COMPLETED,"""VAR"" VAR ( VAR = VAR.VAR(""VAR"") VAR = VAR.VAR(""VAR"") ) VAR := VAR VAR VAR.VAR() == ""VAR"" { VAR = VAR } VAR := 1000 VAR VAR := VAR.VAR(); VAR != """" { VAR VAR, VAR := VAR.VAR(VAR); VAR == VAR && VAR >= 0 { VAR = VAR } } VAR := VAR() VAR, VAR := VAR.VAR(VAR, VAR, VAR) VAR VAR <= 0 { VAR VAR, VAR } VAR-- VAR VAR, VAR := VAR VAR.VAR { VAR -= VAR(VAR(VAR)) } VAR VAR = VAR.VAR(""VAR"") VAR := VAR.VAR() != ""VAR"" VAR, VAR := VAR.VAR(VAR, VAR)",14,9
go_d6759e7a059f4208f07aa781402841d7ddaaef96,src/net/textproto/reader.go,"	hint := r.upcomingHeaderKeys()
		if hint > 1000 {
			hint = 1000 // set a cap to avoid overallocation
		}
// upcomingHeaderKeys returns an approximation of the number of keys
func (r *Reader) upcomingHeaderKeys() (n int) {
	for len(peek) > 0 && n < 1000 {
		var line []byte
		line, peek, _ = bytes.Cut(peek, nl)
		if len(line) == 0 || (len(line) == 1 && line[0] == '\r') {
			// Blank line separating headers from the body.
			break
		}
		if line[0] == ' ' || line[0] == '\t' {
			// Folded continuation of the previous line.
			continue
		}
		n++
	}
	return n","	hint := r.upcomingHeaderNewlines()
// upcomingHeaderNewlines returns an approximation of the number of newlines
func (r *Reader) upcomingHeaderNewlines() (n int) {
	return bytes.Count(peek, nl)",d6759e7a059f4208f07aa781402841d7ddaaef96,CVE-2023-24534,d6759e7a059f4208f07aa781402841d7ddaaef96,https://github.com/golang/go,CPE_GIT_REPOBASED,214,COMPLETED,"VAR := VAR.VAR() VAR VAR > 1000 { VAR = 1000  }  VAR (VAR *VAR) VAR() (VAR VAR) { VAR VAR(VAR) > 0 && VAR < 1000 { VAR VAR []VAR VAR, VAR, VAR = VAR.VAR(VAR, VAR) VAR VAR(VAR) == 0 || (VAR(VAR) == 1 && VAR[0] == '\VAR') {  VAR } VAR VAR[0] == ' ' || VAR[0] == '\VAR' {  VAR } VAR++ } VAR VAR VAR := VAR.VAR()  VAR (VAR *VAR) VAR() (VAR VAR) { VAR VAR.VAR(VAR, VAR)",14,9
gpac_47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,src/bifs/com_dec.c,"void gf_bifs_dec_name(GF_BitStream *bs, char *name, u32 size)
	Bool error = GF_FALSE;
		char c = gf_bs_read_int(bs, 8);
		if (i<size)
			name[i] = c;
		else {
			error = GF_TRUE;
		}
		if (!c) break;
	if (error) {
		name[size-1] = 0;
		GF_LOG(GF_LOG_ERROR, GF_LOG_CODING, (""[BIFS] name too long %d bytes but max size %d, truncating\n"", i, size));
	}
			gf_bifs_dec_name(bs, name, 1000);
				gf_bifs_dec_name(bs, name, 1000);
		if (codec->UseName) gf_bifs_dec_name(bs, name, 1000);","void gf_bifs_dec_name(GF_BitStream *bs, char *name)
		name[i] = gf_bs_read_int(bs, 8);
		if (!name[i]) break;
			gf_bifs_dec_name(bs, name);
				gf_bifs_dec_name(bs, name);
		if (codec->UseName) gf_bifs_dec_name(bs, name);",47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,CVE-2021-45258,47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,https://github.com/gpac/gpac,NVD_GIT_REPOBASED,106,COMPLETED,"VAR VAR(VAR *VAR, VAR *VAR, VAR VAR) VAR VAR = VAR; VAR VAR = VAR(VAR, 8); VAR (VAR<VAR) VAR[VAR] = VAR; VAR { VAR = VAR; } VAR (!VAR) VAR; VAR (VAR) { VAR[VAR-1] = 0; VAR(VAR, VAR, (""[VAR] VAR VAR VAR %VAR VAR VAR VAR VAR %VAR, VAR\VAR"", VAR, VAR)); } VAR(VAR, VAR, 1000); VAR(VAR, VAR, 1000); VAR (VAR->VAR) VAR(VAR, VAR, 1000); VAR VAR(VAR *VAR, VAR *VAR) VAR[VAR] = VAR(VAR, 8); VAR (!VAR[VAR]) VAR; VAR(VAR, VAR); VAR(VAR, VAR); VAR (VAR->VAR) VAR(VAR, VAR);",14,9
gpac_47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,src/bifs/field_decode.c,"			if (codec->UseName) gf_bifs_dec_name(bs, name, 1000);
		if (codec->UseName) gf_bifs_dec_name(bs, name, 1000);","			if (codec->UseName) gf_bifs_dec_name(bs, name);
		if (codec->UseName) gf_bifs_dec_name(bs, name);",47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,CVE-2021-45258,47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,https://github.com/gpac/gpac,NVD_GIT_REPOBASED,106,COMPLETED,"VAR (VAR->VAR) VAR(VAR, VAR, 1000); VAR (VAR->VAR) VAR(VAR, VAR, 1000); VAR (VAR->VAR) VAR(VAR, VAR); VAR (VAR->VAR) VAR(VAR, VAR);",14,9
gpac_47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,src/bifs/memory_decoder.c,"		if (codec->UseName) gf_bifs_dec_name(bs, name, 1000);","		if (codec->UseName) gf_bifs_dec_name(bs, name);",47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,CVE-2021-45258,47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,https://github.com/gpac/gpac,NVD_GIT_REPOBASED,106,COMPLETED,"VAR (VAR->VAR) VAR(VAR, VAR, 1000); VAR (VAR->VAR) VAR(VAR, VAR);",14,9
gpac_47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,src/bifs/script_dec.c,"	gf_bifs_dec_name(parser->bs, name, 1000);
		gf_bifs_dec_name(parser->bs, name, 500);
	gf_bifs_dec_name(parser->bs, name, 1000);","	gf_bifs_dec_name(parser->bs, name);
		gf_bifs_dec_name(parser->bs, name);
	gf_bifs_dec_name(parser->bs, name);",47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,CVE-2021-45258,47a26a32c9a2cd630c48517c3e6ab2fa5f6a26ad,https://github.com/gpac/gpac,NVD_GIT_REPOBASED,106,COMPLETED,"VAR(VAR->VAR, VAR, 1000); VAR(VAR->VAR, VAR, 500); VAR(VAR->VAR, VAR, 1000); VAR(VAR->VAR, VAR); VAR(VAR->VAR, VAR); VAR(VAR->VAR, VAR);",14,9
gpac_6c51ddec7a1d8b9881e633188c88f73aae36a3ee,src/filters/in_sock.c,"	u32 init_time;
	u32 last_rcv_time;
	u32 last_timeout_sec;
	strcpy(ctx->sock_c.address, ""unknown"");
	gf_sk_get_remote_address(ctx->sock_c.socket, ctx->sock_c.address);


	ctx->sock_c.init_time = gf_sys_clock();


	if (!sock_c->nb_bytes) {
		GF_LOG(GF_LOG_INFO, GF_LOG_NETWORK, (""[SockIn] Reception started after %u ms\n"", gf_sys_clock() - sock_c->init_time));
	}

	u32 now;
	now = gf_sys_clock();
	if (now - ctx->last_rcv_time < ctx->timeout) {
		u32 tout = (ctx->timeout - (now - ctx->last_rcv_time)) / 1000;
		if (tout != ctx->last_timeout_sec) {
			ctx->last_timeout_sec = tout;
			GF_LOG(GF_LOG_INFO, GF_LOG_NETWORK, (""[SockIn] Waiting for %u seconds\r"", tout));
		}
	if (!ctx->sock_c.done) {
		if (ctx->sock_c.pid)
			gf_filter_pid_set_eos(ctx->sock_c.pid);
		if (ctx->sock_c.nb_bytes) {
			GF_LOG(GF_LOG_INFO, GF_LOG_NETWORK, (""[SockIn] No data received for %d ms, assuming end of stream\n"", ctx->timeout));
		} else {
			GF_LOG(GF_LOG_WARNING, GF_LOG_NETWORK, (""[SockIn] No data received after %d ms, aborting\n"", ctx->timeout));
		}
				sc->init_time = gf_sys_clock();","
	u64 last_rcv_time;

	u64 now;
	now = gf_sys_clock_high_res();
	if (now - ctx->last_rcv_time < ctx->timeout*1000) {
	if (ctx->sock_c.pid && !ctx->sock_c.done) {
		gf_filter_pid_set_eos(ctx->sock_c.pid);",6c51ddec7a1d8b9881e633188c88f73aae36a3ee,CVE-2022-26967,6c51ddec7a1d8b9881e633188c88f73aae36a3ee,https://github.com/gpac/gpac,NVD_GIT_REPOBASED,66,COMPLETED,"VAR VAR; VAR VAR; VAR VAR; VAR(VAR->VAR.VAR, ""VAR""); VAR(VAR->VAR.VAR, VAR->VAR.VAR); VAR->VAR.VAR = VAR(); VAR (!VAR->VAR) { VAR(VAR, VAR, (""[VAR] VAR VAR VAR %VAR VAR\VAR"", VAR() - VAR->VAR)); } VAR VAR; VAR = VAR(); VAR (VAR - VAR->VAR < VAR->VAR) { VAR VAR = (VAR->VAR - (VAR - VAR->VAR)) / 1000; VAR (VAR != VAR->VAR) { VAR->VAR = VAR; VAR(VAR, VAR, (""[VAR] VAR VAR %VAR VAR\VAR"", VAR)); } VAR (!VAR->VAR.VAR) { VAR (VAR->VAR.VAR) VAR(VAR->VAR.VAR); VAR (VAR->VAR.VAR) { VAR(VAR, VAR, (""[VAR] VAR VAR VAR VAR %VAR VAR, VAR VAR VAR VAR\VAR"", VAR->VAR)); } VAR { VAR(VAR, VAR, (""[VAR] VAR VAR VAR VAR %VAR VAR, VAR\VAR"", VAR->VAR)); } VAR->VAR = VAR(); VAR VAR; VAR VAR; VAR = VAR(); VAR (VAR - VAR->VAR < VAR->VAR*1000) { VAR (VAR->VAR.VAR && !VAR->VAR.VAR) { VAR(VAR->VAR.VAR);",14,9
gpac_ae2828284f2fc0381548aaa991958f1eb9b90619,src/filters/dmx_nhml.c,"	char *media_file;
	char *ext, *init_name=NULL, szXmlFrom[1000], szXmlHeaderEnd[1000];
		if (base) gf_dynstrcat(&init_name, base, NULL);
		gf_dynstrcat(&init_name, ctx->src_url, NULL);
	if (!init_name) return GF_OUT_OF_MEM;
	ext = gf_file_ext_start(init_name);
	if (ctx->media_file) gf_free(ctx->media_file);
	ctx->media_file = gf_strdup(init_name);
	gf_dynstrcat(&ctx->media_file, "".media"", NULL);
	gf_dynstrcat(&init_name, "".info"", NULL);
		if (init_name) gf_free(init_name);
		if (init_name) gf_free(init_name);
		if (init_name) gf_free(init_name);
			if (ctx->media_file) gf_free(ctx->media_file);
			ctx->media_file = url;
			if (init_name) gf_free(init_name);
			init_name = url;
	if (gf_file_exists_ex(ctx->media_file, ctx->src_url))
		ctx->mdia = gf_fopen_ex(ctx->media_file, ctx->src_url, ""rb"");
	if (gf_file_exists_ex(init_name, ctx->src_url))
		finfo = gf_fopen_ex(init_name, ctx->src_url, ""rb"");
		if (e) {
			if (init_name) gf_free(init_name);
			return e;
		}
		e = nhml_sample_from_xml(ctx, ctx->media_file, szXmlFrom, szXmlHeaderEnd);
			if (init_name) gf_free(init_name);
			if (init_name) gf_free(init_name);
	gf_filter_pid_set_property(ctx->opid, GF_PROP_PID_FILEPATH, & PROP_STRING(ctx->media_file));
	if (init_name) gf_free(init_name);
			else xml_file = ctx->media_file;
					GF_LOG(GF_LOG_ERROR, GF_LOG_PARSER, (""[NHMLDmx] import failure in sample %d: file %s not found"", ctx->sample_num, close ? szMediaTemp : ctx->media_file));
	if (ctx->media_file) gf_free(ctx->media_file);","	char szMedia[GF_MAX_PATH];
	char *ext, szName[1000], szInfo[GF_MAX_PATH], szXmlFrom[1000], szXmlHeaderEnd[1000];
	szName[0] = 0;
		if (base) strcpy(szName, base);
		strcpy(szName, ctx->src_url);
	ext = gf_file_ext_start(szName);
	strcpy(ctx->szMedia, szName);
	strcpy(szInfo, szName);
	strcat(ctx->szMedia, "".media"");
	strcat(szInfo, "".info"");
			strcpy(ctx->szMedia, url ? url : att->value);
			if (url) gf_free(url);
			strcpy(szInfo, url ? url : att->value);
			if (url) gf_free(url);
	if (gf_file_exists_ex(ctx->szMedia, ctx->src_url))
		ctx->mdia = gf_fopen_ex(ctx->szMedia, ctx->src_url, ""rb"");
	if (gf_file_exists_ex(szInfo, ctx->src_url))
		finfo = gf_fopen_ex(szInfo, ctx->src_url, ""rb"");
		if (e) return e;
		e = nhml_sample_from_xml(ctx, ctx->szMedia, szXmlFrom, szXmlHeaderEnd);
	gf_filter_pid_set_property(ctx->opid, GF_PROP_PID_FILEPATH, & PROP_STRING(ctx->szMedia));
			else xml_file = ctx->szMedia;
					GF_LOG(GF_LOG_ERROR, GF_LOG_PARSER, (""[NHMLDmx] import failure in sample %d: file %s not found"", ctx->sample_num, close ? szMediaTemp : ctx->szMedia));",ae2828284f2fc0381548aaa991958f1eb9b90619,CVE-2021-41457,ae2828284f2fc0381548aaa991958f1eb9b90619,https://github.com/gpac/gpac,NVD_GIT_REPOBASED,118,COMPLETED,"VAR *VAR; VAR *VAR, *VAR=VAR, VAR[1000], VAR[1000]; VAR (VAR) VAR(&VAR, VAR, VAR); VAR(&VAR, VAR->VAR, VAR); VAR (!VAR) VAR VAR; VAR = VAR(VAR); VAR (VAR->VAR) VAR(VAR->VAR); VAR->VAR = VAR(VAR); VAR(&VAR->VAR, "".VAR"", VAR); VAR(&VAR, "".VAR"", VAR); VAR (VAR) VAR(VAR); VAR (VAR) VAR(VAR); VAR (VAR) VAR(VAR); VAR (VAR->VAR) VAR(VAR->VAR); VAR->VAR = VAR; VAR (VAR) VAR(VAR); VAR = VAR; VAR (VAR(VAR->VAR, VAR->VAR)) VAR->VAR = VAR(VAR->VAR, VAR->VAR, ""VAR""); VAR (VAR(VAR, VAR->VAR)) VAR = VAR(VAR, VAR->VAR, ""VAR""); VAR (VAR) { VAR (VAR) VAR(VAR); VAR VAR; } VAR = VAR(VAR, VAR->VAR, VAR, VAR); VAR (VAR) VAR(VAR); VAR (VAR) VAR(VAR); VAR(VAR->VAR, VAR, & VAR(VAR->VAR)); VAR (VAR) VAR(VAR); VAR VAR = VAR->VAR; VAR(VAR, VAR, (""[VAR] VAR VAR VAR VAR %VAR: VAR %VAR VAR VAR"", VAR->VAR, VAR ? VAR : VAR->VAR)); VAR (VAR->VAR) VAR(VAR->VAR); VAR VAR[VAR]; VAR *VAR, VAR[1000], VAR[VAR], VAR[1000], VAR[1000]; VAR[0] = 0; VAR (VAR) VAR(VAR, VAR); VAR(VAR, VAR->VAR); VAR = VAR(VAR); VAR(VAR->VAR, VAR); VAR(VAR, VAR); VAR(VAR->VAR, "".VAR""); VAR(VAR, "".VAR""); VAR(VAR->VAR, VAR ? VAR : VAR->VAR); VAR (VAR) VAR(VAR); VAR(VAR, VAR ? VAR : VAR->VAR); VAR (VAR) VAR(VAR); VAR (VAR(VAR->VAR, VAR->VAR)) VAR->VAR = VAR(VAR->VAR, VAR->VAR, ""VAR""); VAR (VAR(VAR, VAR->VAR)) VAR = VAR(VAR, VAR->VAR, ""VAR""); VAR (VAR) VAR VAR; VAR = VAR(VAR, VAR->VAR, VAR, VAR); VAR(VAR->VAR, VAR, & VAR(VAR->VAR)); VAR VAR = VAR->VAR; VAR(VAR, VAR, (""[VAR] VAR VAR VAR VAR %VAR: VAR %VAR VAR VAR"", VAR->VAR, VAR ? VAR : VAR->VAR));",14,9
gpac_da37ec8582266983d0ec4b7550ec907401ec441e,applications/mp4box/fileimport.c,"	char *ext, *final_name=NULL, *handler_name, *rvc_config, *chapter_name;
	final_name = gf_strdup(inName);
	if ( (final_name[0]=='/') && (final_name[2]=='/')) {
		final_name[0] = final_name[1];
		final_name[1] = ':';
	ext_start = gf_file_ext_start(final_name);
	ext = strrchr(ext_start ? ext_start : final_name, '#');
	if (!ext) ext = gf_url_colon_suffix(final_name);
 	if (!strlen(final_name) || !strcmp(final_name, ""self"")) {
	if (gf_isom_probe_file(final_name))
	ext = gf_url_colon_suffix(final_name);
	ext = strrchr(final_name, '%');
	ext_start = gf_file_ext_start(final_name);
	ext = strrchr(ext_start ? ext_start : final_name, '#');
		import.in_name = final_name;
		import.in_name = final_name;
	if (final_name) gf_free(final_name);","	char *ext, szName[1000], *handler_name, *rvc_config, *chapter_name;
	strcpy(szName, inName);
	if ( (szName[0]=='/') && (szName[2]=='/')) {
		szName[0] = szName[1];
		szName[1] = ':';
	ext_start = gf_file_ext_start(szName);
	ext = strrchr(ext_start ? ext_start : szName, '#');
	if (!ext) ext = gf_url_colon_suffix(szName);
 	if (!strlen(szName) || !strcmp(szName, ""self"")) {
	if (gf_isom_probe_file(szName))
	ext = gf_url_colon_suffix(szName);
	ext = strrchr(szName, '%');
	ext_start = gf_file_ext_start(szName);
	ext = strrchr(ext_start ? ext_start : szName, '#');
		import.in_name = szName;
		import.in_name = szName;",da37ec8582266983d0ec4b7550ec907401ec441e,CVE-2021-40942,da37ec8582266983d0ec4b7550ec907401ec441e,https://github.com/gpac/gpac,NVD_GIT_REPOBASED,118,COMPLETED,"VAR *VAR, *VAR=VAR, *VAR, *VAR, *VAR; VAR = VAR(VAR); VAR ( (VAR[0]=='/') && (VAR[2]=='/')) { VAR[0] = VAR[1]; VAR[1] = ':'; VAR = VAR(VAR); VAR = VAR(VAR ? VAR : VAR, ' VAR (!VAR) VAR = VAR(VAR); VAR (!VAR(VAR) || !VAR(VAR, ""VAR"")) { VAR (VAR(VAR)) VAR = VAR(VAR); VAR = VAR(VAR, '%'); VAR = VAR(VAR); VAR = VAR(VAR ? VAR : VAR, ' VAR.VAR = VAR; VAR.VAR = VAR; VAR (VAR) VAR(VAR); VAR *VAR, VAR[1000], *VAR, *VAR, *VAR; VAR(VAR, VAR); VAR ( (VAR[0]=='/') && (VAR[2]=='/')) { VAR[0] = VAR[1]; VAR[1] = ':'; VAR = VAR(VAR); VAR = VAR(VAR ? VAR : VAR, ' VAR (!VAR) VAR = VAR(VAR); VAR (!VAR(VAR) || !VAR(VAR, ""VAR"")) { VAR (VAR(VAR)) VAR = VAR(VAR); VAR = VAR(VAR, '%'); VAR = VAR(VAR); VAR = VAR(VAR ? VAR : VAR, ' VAR.VAR = VAR; VAR.VAR = VAR;",14,9
jackson-databind_fcfc4998ec23f0b1f7f8a9521c2b317b6c25892b,src/main/java/com/fasterxml/jackson/databind/deser/std/UntypedObjectDeserializer.java,"        // Arbitrarily chosen.
        // Introduced to resolve CVE-2020-36518 and as a temporary hotfix for #2816
        private static final int MAX_DEPTH = 1000;

        public Object deserialize(JsonParser p, DeserializationContext ctxt) throws IOException {
            return deserialize(p, ctxt, 0);
        }

        private Object deserialize(JsonParser p, DeserializationContext ctxt, int depth) throws IOException
                case JsonTokenId.ID_START_OBJECT: {
                        return new LinkedHashMap<String, Object>(2);
                case JsonTokenId.ID_FIELD_NAME:
                    if (depth > MAX_DEPTH) {
                        throw new JsonParseException(p, ""JSON is too deeply nested."");
                    }

                    return mapObject(p, ctxt, depth);
                case JsonTokenId.ID_START_ARRAY: {
                        if (ctxt.isEnabled(
                            DeserializationFeature.USE_JAVA_ARRAY_FOR_JSON_ARRAY)) {
                if (depth > MAX_DEPTH) {
                    throw new JsonParseException(p, ""JSON is too deeply nested."");
                if (ctxt.isEnabled(DeserializationFeature.USE_JAVA_ARRAY_FOR_JSON_ARRAY)) {
                    return mapArrayToArray(p, ctxt, depth);
                return mapArray(p, ctxt, depth);
                case JsonTokenId.ID_EMBEDDED_OBJECT:
                    return p.getEmbeddedObject();
                case JsonTokenId.ID_STRING:
                    return p.getText();

                case JsonTokenId.ID_NUMBER_INT:
                    if (ctxt.hasSomeOfFeatures(F_MASK_INT_COERCIONS)) {
                        return _coerceIntegral(p, ctxt);
                    }
                    return p.getNumberValue(); // should be optimal, whatever it is
                case JsonTokenId.ID_NUMBER_FLOAT:
                    if (ctxt.isEnabled(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS)) {
                        return p.getDecimalValue();
                    }
                    return p.getNumberValue();
                case JsonTokenId.ID_TRUE:
                    return Boolean.TRUE;
                case JsonTokenId.ID_FALSE:
                    return Boolean.FALSE;
                case JsonTokenId.ID_END_OBJECT:
                    // 28-Oct-2015, tatu: [databind#989] We may also be given END_OBJECT (similar to FIELD_NAME),
                    //    if caller has advanced to the first token of Object, but for empty Object
                    return new LinkedHashMap<String, Object>(2);
                case JsonTokenId.ID_NULL: // 08-Nov-2016, tatu: yes, occurs
                    return null;

                //case JsonTokenId.ID_END_ARRAY: // invalid
                default:
        protected Object mapArray(JsonParser p, DeserializationContext ctxt, int depth) throws IOException
            Object value = deserialize(p, ctxt, depth + 1);
            Object value2 = deserialize(p, ctxt, depth + 1);
                value = deserialize(p, ctxt, depth + 1);
        protected Object[] mapArrayToArray(JsonParser p, DeserializationContext ctxt, int depth) throws IOException {
                Object value = deserialize(p, ctxt, depth + 1);
        protected Object mapObject(JsonParser p, DeserializationContext ctxt, int depth) throws IOException
            Object value1 = deserialize(p, ctxt, depth + 1);
            Object value2 = deserialize(p, ctxt, depth + 1);
                final Object newValue = deserialize(p, ctxt, depth + 1);","        public Object deserialize(JsonParser p, DeserializationContext ctxt) throws IOException
            case JsonTokenId.ID_START_OBJECT:
                {
                        return new LinkedHashMap<String,Object>(2);
            case JsonTokenId.ID_FIELD_NAME:
                return mapObject(p, ctxt);
            case JsonTokenId.ID_START_ARRAY:
                {
                        if (ctxt.isEnabled(DeserializationFeature.USE_JAVA_ARRAY_FOR_JSON_ARRAY)) {
                if (ctxt.isEnabled(DeserializationFeature.USE_JAVA_ARRAY_FOR_JSON_ARRAY)) {
                    return mapArrayToArray(p, ctxt);
                }
                return mapArray(p, ctxt);
            case JsonTokenId.ID_EMBEDDED_OBJECT:
                return p.getEmbeddedObject();
            case JsonTokenId.ID_STRING:
                return p.getText();
            case JsonTokenId.ID_NUMBER_INT:
                if (ctxt.hasSomeOfFeatures(F_MASK_INT_COERCIONS)) {
                    return _coerceIntegral(p, ctxt);
                return p.getNumberValue(); // should be optimal, whatever it is
            case JsonTokenId.ID_NUMBER_FLOAT:
                if (ctxt.isEnabled(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS)) {
                    return p.getDecimalValue();
                return p.getNumberValue();
            case JsonTokenId.ID_TRUE:
                return Boolean.TRUE;
            case JsonTokenId.ID_FALSE:
                return Boolean.FALSE;
            case JsonTokenId.ID_END_OBJECT:
                // 28-Oct-2015, tatu: [databind#989] We may also be given END_OBJECT (similar to FIELD_NAME),
                //    if caller has advanced to the first token of Object, but for empty Object
                return new LinkedHashMap<String,Object>(2);
            case JsonTokenId.ID_NULL: // 08-Nov-2016, tatu: yes, occurs
                return null;
            //case JsonTokenId.ID_END_ARRAY: // invalid
            default:
        protected Object mapArray(JsonParser p, DeserializationContext ctxt) throws IOException
            Object value = deserialize(p, ctxt);
            Object value2 = deserialize(p, ctxt);
                value = deserialize(p, ctxt);
        protected Object[] mapArrayToArray(JsonParser p, DeserializationContext ctxt) throws IOException {
                Object value = deserialize(p, ctxt);
        protected Object mapObject(JsonParser p, DeserializationContext ctxt) throws IOException
            Object value1 = deserialize(p, ctxt);
            Object value2 = deserialize(p, ctxt);
                final Object newValue = deserialize(p, ctxt);",fcfc4998ec23f0b1f7f8a9521c2b317b6c25892b,CVE-2020-36518,fcfc4998ec23f0b1f7f8a9521c2b317b6c25892b,https://github.com/FasterXML/jackson-databind,GHSD_DIRECT_COMMIT,1337,COMPLETED,"  VAR VAR VAR VAR VAR = 1000; VAR VAR VAR(VAR VAR, VAR VAR) VAR VAR { VAR VAR(VAR, VAR, 0); } VAR VAR VAR(VAR VAR, VAR VAR, VAR VAR) VAR VAR VAR VAR.VAR: { VAR VAR VAR<VAR, VAR>(2); VAR VAR.VAR: VAR (VAR > VAR) { VAR VAR VAR(VAR, ""VAR VAR VAR VAR VAR.""); } VAR VAR(VAR, VAR, VAR); VAR VAR.VAR: { VAR (VAR.VAR( VAR.VAR)) { VAR (VAR > VAR) { VAR VAR VAR(VAR, ""VAR VAR VAR VAR VAR.""); VAR (VAR.VAR(VAR.VAR)) { VAR VAR(VAR, VAR, VAR); VAR VAR(VAR, VAR, VAR); VAR VAR.VAR: VAR VAR.VAR(); VAR VAR.VAR: VAR VAR.VAR(); VAR VAR.VAR: VAR (VAR.VAR(VAR)) { VAR VAR(VAR, VAR); } VAR VAR.VAR();  VAR VAR.VAR: VAR (VAR.VAR(VAR.VAR)) { VAR VAR.VAR(); } VAR VAR.VAR(); VAR VAR.VAR: VAR VAR.VAR; VAR VAR.VAR: VAR VAR.VAR; VAR VAR.VAR:   VAR VAR VAR<VAR, VAR>(2); VAR VAR.VAR:  VAR VAR;  VAR: VAR VAR VAR(VAR VAR, VAR VAR, VAR VAR) VAR VAR VAR VAR = VAR(VAR, VAR, VAR + 1); VAR VAR = VAR(VAR, VAR, VAR + 1); VAR = VAR(VAR, VAR, VAR + 1); VAR VAR[] VAR(VAR VAR, VAR VAR, VAR VAR) VAR VAR { VAR VAR = VAR(VAR, VAR, VAR + 1); VAR VAR VAR(VAR VAR, VAR VAR, VAR VAR) VAR VAR VAR VAR = VAR(VAR, VAR, VAR + 1); VAR VAR = VAR(VAR, VAR, VAR + 1); VAR VAR VAR = VAR(VAR, VAR, VAR + 1); VAR VAR VAR(VAR VAR, VAR VAR) VAR VAR VAR VAR.VAR: { VAR VAR VAR<VAR,VAR>(2); VAR VAR.VAR: VAR VAR(VAR, VAR); VAR VAR.VAR: { VAR (VAR.VAR(VAR.VAR)) { VAR (VAR.VAR(VAR.VAR)) { VAR VAR(VAR, VAR); } VAR VAR(VAR, VAR); VAR VAR.VAR: VAR VAR.VAR(); VAR VAR.VAR: VAR VAR.VAR(); VAR VAR.VAR: VAR (VAR.VAR(VAR)) { VAR VAR(VAR, VAR); VAR VAR.VAR();  VAR VAR.VAR: VAR (VAR.VAR(VAR.VAR)) { VAR VAR.VAR(); VAR VAR.VAR(); VAR VAR.VAR: VAR VAR.VAR; VAR VAR.VAR: VAR VAR.VAR; VAR VAR.VAR:   VAR VAR VAR<VAR,VAR>(2); VAR VAR.VAR:  VAR VAR;  VAR: VAR VAR VAR(VAR VAR, VAR VAR) VAR VAR VAR VAR = VAR(VAR, VAR); VAR VAR = VAR(VAR, VAR); VAR = VAR(VAR, VAR); VAR VAR[] VAR(VAR VAR, VAR VAR) VAR VAR { VAR VAR = VAR(VAR, VAR); VAR VAR VAR(VAR VAR, VAR VAR) VAR VAR VAR VAR = VAR(VAR, VAR); VAR VAR = VAR(VAR, VAR); VAR VAR VAR = VAR(VAR, VAR);",14,9
joplin_9e90d9016daf79b5414646a93fd369aedb035071,packages/app-cli/tests/md_to_html/sanitize_16.html,"<map name=""test"" class=""jop-noMdConv""><area coords=""0,0,1000,1000"" href=""#"" class=""jop-noMdConv""/></map><img usemap=""#test"" src=""https://github.com/Ry0taK.png"" class=""jop-noMdConv""/>",,9e90d9016daf79b5414646a93fd369aedb035071,CVE-2023-37299,9e90d9016daf79b5414646a93fd369aedb035071,https://github.com/laurent22/joplin,NVD_DIRECT_COMMIT,1337,COMPLETED,"<VAR VAR=""VAR"" VAR=""VAR-VAR""><VAR VAR=""0,0,1000,1000"" VAR="" ",14,9
joplin_9e90d9016daf79b5414646a93fd369aedb035071,packages/app-cli/tests/md_to_html/sanitize_16.md,"<map name=""test""><area coords=""0,0,1000,1000"" href=""javascript:top.require(`child_process`).execSync(`calc.exe`)""></map><img usemap=""#test"" src=""https://github.com/Ry0taK.png"">",,9e90d9016daf79b5414646a93fd369aedb035071,CVE-2023-37299,9e90d9016daf79b5414646a93fd369aedb035071,https://github.com/laurent22/joplin,NVD_DIRECT_COMMIT,1337,COMPLETED,"<VAR VAR=""VAR""><VAR VAR=""0,0,1000,1000"" VAR=""VAR:VAR.VAR(`VAR`).VAR(`VAR.VAR`)""></VAR><VAR VAR="" ",14,9
libarchive_39fc59391b7cf2a007bffce280c1e3e66674258f,libarchive/archive_read_support_format_iso9660.c,"static const char *build_pathname(struct archive_string *, struct file_info *, int);
			return (ARCHIVE_FATAL);
		const char *path = build_pathname(&iso9660->pathname, file, 0);
		if (path == NULL) {
			archive_set_error(&a->archive,
			    ARCHIVE_ERRNO_FILE_FORMAT,
			    ""Pathname is too long"");
			return (ARCHIVE_FATAL);
		} else {
			archive_string_empty(&iso9660->pathname);
			archive_entry_set_pathname(entry, path);
		}
build_pathname(struct archive_string *as, struct file_info *file, int depth)
	// Plain ISO9660 only allows 8 dir levels; if we get
	// to 1000, then something is very, very wrong.
	if (depth > 1000) {
		return NULL;
	}
		if (build_pathname(as, file->parent, depth + 1) == NULL) {
			return NULL;
		}","static const char *build_pathname(struct archive_string *, struct file_info *);
		archive_string_empty(&iso9660->pathname);
		archive_entry_set_pathname(entry,
		    build_pathname(&iso9660->pathname, file));
build_pathname(struct archive_string *as, struct file_info *file)
		build_pathname(as, file->parent);",39fc59391b7cf2a007bffce280c1e3e66674258f,CVE-2015-8930,39fc59391b7cf2a007bffce280c1e3e66674258f,https://github.com/libarchive/libarchive,NVD_GIT_REPOBASED,118,COMPLETED,"VAR VAR VAR *VAR(VAR VAR *, VAR VAR *, VAR); VAR (VAR); VAR VAR *VAR = VAR(&VAR->VAR, VAR, 0); VAR (VAR == VAR) { VAR(&VAR->VAR, VAR, ""VAR VAR VAR VAR""); VAR (VAR); } VAR { VAR(&VAR->VAR); VAR(VAR, VAR); } VAR(VAR VAR *VAR, VAR VAR *VAR, VAR VAR)   VAR (VAR > 1000) { VAR VAR; } VAR (VAR(VAR, VAR->VAR, VAR + 1) == VAR) { VAR VAR; } VAR VAR VAR *VAR(VAR VAR *, VAR VAR *); VAR(&VAR->VAR); VAR(VAR, VAR(&VAR->VAR, VAR)); VAR(VAR VAR *VAR, VAR VAR *VAR) VAR(VAR, VAR->VAR);",14,9
linux_0c4df39e504bf925ab666132ac3c98d6cbbe380b,drivers/media/usdvb-ustechnisat-usb2.c,"	u8 *buf = state->buf;
	int i, ret;
	debug_dump(buf + 1, ret, deb_rc);
	for (i = 1; i < ARRAY_SIZE(state->buf); i++) {
		if (buf[i] == 0xff) {

		ev.pulse = !ev.pulse;
		ev.duration = (buf[i] * FIRMWARE_CLOCK_DIVISOR *
			       FIRMWARE_CLOCK_TICK) / 1000;
		ir_raw_event_store(d->rc_dev, &ev);","	u8 *buf = state->buf;
	u8 *b;
	int ret;
	b = buf+1;
	debug_dump(b, ret, deb_rc);
	while (1) {
		ev.pulse = !ev.pulse;
		ev.duration = (*b * FIRMWARE_CLOCK_DIVISOR * FIRMWARE_CLOCK_TICK) / 1000;
		ir_raw_event_store(d->rc_dev, &ev);

		b++;
		if (*b == 0xff) {",0c4df39e504bf925ab666132ac3c98d6cbbe380b,CVE-2019-15505,0c4df39e504bf925ab666132ac3c98d6cbbe380b,https://github.com/torvalds/linux,CPE_DIRECT_COMMIT,88,COMPLETED,"VAR *VAR = VAR->VAR; VAR VAR, VAR; VAR(VAR + 1, VAR, VAR); VAR (VAR = 1; VAR < VAR(VAR->VAR); VAR++) { VAR (VAR[VAR] == 0xff) { VAR.VAR = !VAR.VAR; VAR.VAR = (VAR[VAR] * VAR * VAR) / 1000; VAR(VAR->VAR, &VAR); VAR *VAR = VAR->VAR; VAR *VAR; VAR VAR; VAR = VAR+1; VAR(VAR, VAR, VAR); VAR (1) { VAR.VAR = !VAR.VAR; VAR.VAR = (*VAR * VAR * VAR) / 1000; VAR(VAR->VAR, &VAR); VAR++; VAR (*VAR == 0xff) {",14,9
linux_9816144313d33974bbd4599d4e8e554c11cae27e,drivers/uscdns3/ep0.c,			mdelay(1);,"			usleep_range(1000, 2000);",9816144313d33974bbd4599d4e8e554c11cae27e,CVE-2019-19523,9816144313d33974bbd4599d4e8e554c11cae27e,https://github.com/torvalds/linux,CPE_DIRECT_COMMIT,72,COMPLETED,"VAR(1); VAR(1000, 2000);",14,9
linux_dfb4357da6ddbdf57d583ba64361c9d792b0e0b1,kernel/time/timer_stats.c,,"/*
 * kernel/time/timer_stats.c
 *
 * Collect timer usage statistics.
 *
 * Copyright(C) 2006, Red Hat, Inc., Ingo Molnar
 * Copyright(C) 2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>
 *
 * timer_stats is based on timer_top, a similar functionality which was part of
 * Con Kolivas dyntick patch set. It was developed by Daniel Petrini at the
 * Instituto Nokia de Tecnologia - INdT - Manaus. timer_top's design was based
 * on dynamic allocation of the statistics entries and linear search based
 * lookup combined with a global lock, rather than the static array, hash
 * and per-CPU locking which is used by timer_stats. It was written for the
 * pre hrtimer kernel code and therefore did not take hrtimers into account.
 * Nevertheless it provided the base for the timer_stats implementation and
 * was a helpful source of inspiration. Kudos to Daniel and the Nokia folks
 * for this effort.
 *
 * timer_top.c is
 *	Copyright (C) 2005 Instituto Nokia de Tecnologia - INdT - Manaus
 *	Written by Daniel Petrini <d.pensator@gmail.com>
 *	timer_top.c was released under the GNU General Public License version 2
 *
 * We export the addresses and counting of timer functions being called,
 * the pid and cmdline from the owner process if applicable.
 *
 * Start/stop data collection:
 * # echo [1|0] >/proc/timer_stats
 *
 * Display the information collected so far:
 * # cat /proc/timer_stats
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */

#include <linux/proc_fs.h>
#include <linux/module.h>
#include <linux/spinlock.h>
#include <linux/sched.h>
#include <linux/seq_file.h>
#include <linux/kallsyms.h>

#include <linux/uaccess.h>

/*
 * This is our basic unit of interest: a timer expiry event identified
 * by the timer, its start/expire functions and the PID of the task that
 * started the timer. We count the number of times an event happens:
 */
struct entry {
	/*
	 * Hash list:
	 */
	struct entry		*next;

	/*
	 * Hash keys:
	 */
	void			*timer;
	void			*start_func;
	void			*expire_func;
	pid_t			pid;

	/*
	 * Number of timeout events:
	 */
	unsigned long		count;
	u32			flags;

	/*
	 * We save the command-line string to preserve
	 * this information past task exit:
	 */
	char			comm[TASK_COMM_LEN + 1];

} ____cacheline_aligned_in_smp;

/*
 * Spinlock protecting the tables - not taken during lookup:
 */
static DEFINE_RAW_SPINLOCK(table_lock);

/*
 * Per-CPU lookup locks for fast hash lookup:
 */
static DEFINE_PER_CPU(raw_spinlock_t, tstats_lookup_lock);

/*
 * Mutex to serialize state changes with show-stats activities:
 */
static DEFINE_MUTEX(show_mutex);

/*
 * Collection status, active/inactive:
 */
int __read_mostly timer_stats_active;

/*
 * Beginning/end timestamps of measurement:
 */
static ktime_t time_start, time_stop;

/*
 * tstat entry structs only get allocated while collection is
 * active and never freed during that time - this simplifies
 * things quite a bit.
 *
 * They get freed when a new collection period is started.
 */
#define MAX_ENTRIES_BITS	10
#define MAX_ENTRIES		(1UL << MAX_ENTRIES_BITS)

static unsigned long nr_entries;
static struct entry entries[MAX_ENTRIES];

static atomic_t overflow_count;

/*
 * The entries are in a hash-table, for fast lookup:
 */
#define TSTAT_HASH_BITS		(MAX_ENTRIES_BITS - 1)
#define TSTAT_HASH_SIZE		(1UL << TSTAT_HASH_BITS)
#define TSTAT_HASH_MASK		(TSTAT_HASH_SIZE - 1)

#define __tstat_hashfn(entry)						\
	(((unsigned long)(entry)->timer       ^				\
	  (unsigned long)(entry)->start_func  ^				\
	  (unsigned long)(entry)->expire_func ^				\
	  (unsigned long)(entry)->pid		) & TSTAT_HASH_MASK)

#define tstat_hashentry(entry)	(tstat_hash_table + __tstat_hashfn(entry))

static struct entry *tstat_hash_table[TSTAT_HASH_SIZE] __read_mostly;

static void reset_entries(void)
{
	nr_entries = 0;
	memset(entries, 0, sizeof(entries));
	memset(tstat_hash_table, 0, sizeof(tstat_hash_table));
	atomic_set(&overflow_count, 0);
}

static struct entry *alloc_entry(void)
{
	if (nr_entries >= MAX_ENTRIES)
		return NULL;

	return entries + nr_entries++;
}

static int match_entries(struct entry *entry1, struct entry *entry2)
{
	return entry1->timer       == entry2->timer	  &&
	       entry1->start_func  == entry2->start_func  &&
	       entry1->expire_func == entry2->expire_func &&
	       entry1->pid	   == entry2->pid;
}

/*
 * Look up whether an entry matching this item is present
 * in the hash already. Must be called with irqs off and the
 * lookup lock held:
 */
static struct entry *tstat_lookup(struct entry *entry, char *comm)
{
	struct entry **head, *curr, *prev;

	head = tstat_hashentry(entry);
	curr = *head;

	/*
	 * The fastpath is when the entry is already hashed,
	 * we do this with the lookup lock held, but with the
	 * table lock not held:
	 */
	while (curr) {
		if (match_entries(curr, entry))
			return curr;

		curr = curr->next;
	}
	/*
	 * Slowpath: allocate, set up and link a new hash entry:
	 */
	prev = NULL;
	curr = *head;

	raw_spin_lock(&table_lock);
	/*
	 * Make sure we have not raced with another CPU:
	 */
	while (curr) {
		if (match_entries(curr, entry))
			goto out_unlock;

		prev = curr;
		curr = curr->next;
	}

	curr = alloc_entry();
	if (curr) {
		*curr = *entry;
		curr->count = 0;
		curr->next = NULL;
		memcpy(curr->comm, comm, TASK_COMM_LEN);

		smp_mb(); /* Ensure that curr is initialized before insert */

		if (prev)
			prev->next = curr;
		else
			*head = curr;
	}
 out_unlock:
	raw_spin_unlock(&table_lock);

	return curr;
}

/**
 * timer_stats_update_stats - Update the statistics for a timer.
 * @timer:	pointer to either a timer_list or a hrtimer
 * @pid:	the pid of the task which set up the timer
 * @startf:	pointer to the function which did the timer setup
 * @timerf:	pointer to the timer callback function of the timer
 * @comm:	name of the process which set up the timer
 * @tflags:	The flags field of the timer
 *
 * When the timer is already registered, then the event counter is
 * incremented. Otherwise the timer is registered in a free slot.
 */
void timer_stats_update_stats(void *timer, pid_t pid, void *startf,
			      void *timerf, char *comm, u32 tflags)
{
	/*
	 * It doesn't matter which lock we take:
	 */
	raw_spinlock_t *lock;
	struct entry *entry, input;
	unsigned long flags;

	if (likely(!timer_stats_active))
		return;

	lock = &per_cpu(tstats_lookup_lock, raw_smp_processor_id());

	input.timer = timer;
	input.start_func = startf;
	input.expire_func = timerf;
	input.pid = pid;
	input.flags = tflags;

	raw_spin_lock_irqsave(lock, flags);
	if (!timer_stats_active)
		goto out_unlock;

	entry = tstat_lookup(&input, comm);
	if (likely(entry))
		entry->count++;
	else
		atomic_inc(&overflow_count);

 out_unlock:
	raw_spin_unlock_irqrestore(lock, flags);
}

static void print_name_offset(struct seq_file *m, unsigned long addr)
{
	char symname[KSYM_NAME_LEN];

	if (lookup_symbol_name(addr, symname) < 0)
		seq_printf(m, ""<%p>"", (void *)addr);
	else
		seq_printf(m, ""%s"", symname);
}

static int tstats_show(struct seq_file *m, void *v)
{
	struct timespec64 period;
	struct entry *entry;
	unsigned long ms;
	long events = 0;
	ktime_t time;
	int i;

	mutex_lock(&show_mutex);
	/*
	 * If still active then calculate up to now:
	 */
	if (timer_stats_active)
		time_stop = ktime_get();

	time = ktime_sub(time_stop, time_start);

	period = ktime_to_timespec64(time);
	ms = period.tv_nsec / 1000000;

	seq_puts(m, ""Timer Stats Version: v0.3\n"");
	seq_printf(m, ""Sample period: %ld.%03ld s\n"", (long)period.tv_sec, ms);
	if (atomic_read(&overflow_count))
		seq_printf(m, ""Overflow: %d entries\n"", atomic_read(&overflow_count));
	seq_printf(m, ""Collection: %s\n"", timer_stats_active ? ""active"" : ""inactive"");

	for (i = 0; i < nr_entries; i++) {
		entry = entries + i;
		if (entry->flags & TIMER_DEFERRABLE) {
			seq_printf(m, ""%4luD, %5d %-16s "",
				entry->count, entry->pid, entry->comm);
		} else {
			seq_printf(m, "" %4lu, %5d %-16s "",
				entry->count, entry->pid, entry->comm);
		}

		print_name_offset(m, (unsigned long)entry->start_func);
		seq_puts(m, "" ("");
		print_name_offset(m, (unsigned long)entry->expire_func);
		seq_puts(m, "")\n"");

		events += entry->count;
	}

	ms += period.tv_sec * 1000;
	if (!ms)
		ms = 1;

	if (events && period.tv_sec)
		seq_printf(m, ""%ld total events, %ld.%03ld events/sec\n"",
			   events, events * 1000 / ms,
			   (events * 1000000 / ms) % 1000);
	else
		seq_printf(m, ""%ld total events\n"", events);

	mutex_unlock(&show_mutex);

	return 0;
}

/*
 * After a state change, make sure all concurrent lookup/update
 * activities have stopped:
 */
static void sync_access(void)
{
	unsigned long flags;
	int cpu;

	for_each_online_cpu(cpu) {
		raw_spinlock_t *lock = &per_cpu(tstats_lookup_lock, cpu);

		raw_spin_lock_irqsave(lock, flags);
		/* nothing */
		raw_spin_unlock_irqrestore(lock, flags);
	}
}

static ssize_t tstats_write(struct file *file, const char __user *buf,
			    size_t count, loff_t *offs)
{
	char ctl[2];

	if (count != 2 || *offs)
		return -EINVAL;

	if (copy_from_user(ctl, buf, count))
		return -EFAULT;

	mutex_lock(&show_mutex);
	switch (ctl[0]) {
	case '0':
		if (timer_stats_active) {
			timer_stats_active = 0;
			time_stop = ktime_get();
			sync_access();
		}
		break;
	case '1':
		if (!timer_stats_active) {
			reset_entries();
			time_start = ktime_get();
			smp_mb();
			timer_stats_active = 1;
		}
		break;
	default:
		count = -EINVAL;
	}
	mutex_unlock(&show_mutex);

	return count;
}

static int tstats_open(struct inode *inode, struct file *filp)
{
	return single_open(filp, tstats_show, NULL);
}

static const struct file_operations tstats_fops = {
	.open		= tstats_open,
	.read		= seq_read,
	.write		= tstats_write,
	.llseek		= seq_lseek,
	.release	= single_release,
};

void __init init_timer_stats(void)
{
	int cpu;

	for_each_possible_cpu(cpu)
		raw_spin_lock_init(&per_cpu(tstats_lookup_lock, cpu));
}

static int __init init_tstats_procfs(void)
{
	struct proc_dir_entry *pe;

	pe = proc_create(""timer_stats"", 0644, NULL, &tstats_fops);
	if (!pe)
		return -ENOMEM;
	return 0;
}
__initcall(init_tstats_procfs);",dfb4357da6ddbdf57d583ba64361c9d792b0e0b1,CVE-2017-5967,dfb4357da6ddbdf57d583ba64361c9d792b0e0b1,https://github.com/torvalds/linux,CPE_DIRECT_COMMIT,98,COMPLETED," /* * VAR/VAR/VAR.VAR * * VAR VAR VAR VAR. * * VAR(VAR) 2006, VAR VAR, VAR., VAR VAR * VAR(VAR) 2006 VAR VAR., VAR VAR <VAR@VAR.VAR> * * VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR VAR - VAR - VAR. VAR'VAR VAR VAR VAR * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR, VAR * VAR VAR-VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR. * * VAR.VAR VAR * VAR (VAR) 2005 VAR VAR VAR VAR - VAR - VAR * VAR VAR VAR VAR <VAR.VAR@VAR.VAR> * VAR.VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR 2 * * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR, * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. * * VAR/VAR VAR VAR: *  * * VAR VAR VAR VAR VAR VAR: *  * * VAR VAR VAR VAR VAR; VAR VAR VAR VAR VAR/VAR VAR * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR 2 VAR * VAR VAR VAR VAR VAR VAR. */        /* * VAR VAR VAR VAR VAR VAR VAR: VAR VAR VAR VAR VAR * VAR VAR VAR, VAR VAR/VAR VAR VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR: */ VAR VAR { /* * VAR VAR: */ VAR VAR *VAR; /* * VAR VAR: */ VAR *VAR; VAR *VAR; VAR *VAR; VAR VAR; /* * VAR VAR VAR VAR: */ VAR VAR VAR; VAR VAR; /* * VAR VAR VAR VAR-VAR VAR VAR VAR * VAR VAR VAR VAR VAR: */ VAR VAR[VAR + 1]; } VAR; /* * VAR VAR VAR VAR - VAR VAR VAR VAR: */ VAR VAR(VAR); /* * VAR-VAR VAR VAR VAR VAR VAR VAR: */ VAR VAR(VAR, VAR); /* * VAR VAR VAR VAR VAR VAR VAR-VAR VAR: */ VAR VAR(VAR); /* * VAR VAR, VAR/VAR: */ VAR VAR VAR; /* * VAR/VAR VAR VAR VAR: */ VAR VAR VAR, VAR; /* * VAR VAR VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR VAR VAR VAR VAR - VAR VAR * VAR VAR VAR VAR. * * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. */   VAR VAR VAR VAR; VAR VAR VAR VAR[VAR]; VAR VAR VAR; /* * VAR VAR VAR VAR VAR VAR-VAR, VAR VAR VAR: */     (((VAR VAR)(VAR)->VAR ^ \ (VAR VAR)(VAR)->VAR ^ \ (VAR VAR)(VAR)->VAR ^ \ (VAR VAR)(VAR)->VAR ) & VAR)  VAR VAR VAR *VAR[VAR] VAR; VAR VAR VAR(VAR) { VAR = 0; VAR(VAR, 0, VAR(VAR)); VAR(VAR, 0, VAR(VAR)); VAR(&VAR, 0); } VAR VAR VAR *VAR(VAR) { VAR (VAR >= VAR) VAR VAR; VAR VAR + VAR++; } VAR VAR VAR(VAR VAR *VAR, VAR VAR *VAR) { VAR VAR->VAR == VAR->VAR && VAR->VAR == VAR->VAR && VAR->VAR == VAR->VAR && VAR->VAR == VAR->VAR; } /* * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR: */ VAR VAR VAR *VAR(VAR VAR *VAR, VAR *VAR) { VAR VAR **VAR, *VAR, *VAR; VAR = VAR(VAR); VAR = *VAR; /* * VAR VAR VAR VAR VAR VAR VAR VAR VAR, * VAR VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR * VAR VAR VAR VAR: */ VAR (VAR) { VAR (VAR(VAR, VAR)) VAR VAR; VAR = VAR->VAR; } /* * VAR: VAR, VAR VAR VAR VAR VAR VAR VAR VAR: */ VAR = VAR; VAR = *VAR; VAR(&VAR); /* * VAR VAR VAR VAR VAR VAR VAR VAR VAR: */ VAR (VAR) { VAR (VAR(VAR, VAR)) VAR VAR; VAR = VAR; VAR = VAR->VAR; } VAR = VAR(); VAR (VAR) { *VAR = *VAR; VAR->VAR = 0; VAR->VAR = VAR; VAR(VAR->VAR, VAR, VAR); VAR();  VAR (VAR) VAR->VAR = VAR; VAR *VAR = VAR; } VAR: VAR(&VAR); VAR VAR; } /** * VAR - VAR VAR VAR VAR VAR VAR. * @VAR: VAR VAR VAR VAR VAR VAR VAR VAR * @VAR: VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR * @VAR: VAR VAR VAR VAR VAR VAR VAR VAR VAR * @VAR: VAR VAR VAR VAR VAR VAR VAR VAR VAR * @VAR: VAR VAR VAR VAR VAR VAR VAR VAR VAR * @VAR: VAR VAR VAR VAR VAR VAR * * VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR * VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR. */ VAR VAR(VAR *VAR, VAR VAR, VAR *VAR, VAR *VAR, VAR *VAR, VAR VAR) { /* * VAR VAR'VAR VAR VAR VAR VAR VAR: */ VAR *VAR; VAR VAR *VAR, VAR; VAR VAR VAR; VAR (VAR(!VAR)) VAR; VAR = &VAR(VAR, VAR()); VAR.VAR = VAR; VAR.VAR = VAR; VAR.VAR = VAR; VAR.VAR = VAR; VAR.VAR = VAR; VAR(VAR, VAR); VAR (!VAR) VAR VAR; VAR = VAR(&VAR, VAR); VAR (VAR(VAR)) VAR->VAR++; VAR VAR(&VAR); VAR: VAR(VAR, VAR); } VAR VAR VAR(VAR VAR *VAR, VAR VAR VAR) { VAR VAR[VAR]; VAR (VAR(VAR, VAR) < 0) VAR(VAR, ""<%VAR>"", (VAR *)VAR); VAR VAR(VAR, ""%VAR"", VAR); } VAR VAR VAR(VAR VAR *VAR, VAR *VAR) { VAR VAR VAR; VAR VAR *VAR; VAR VAR VAR; VAR VAR = 0; VAR VAR; VAR VAR; VAR(&VAR); /* * VAR VAR VAR VAR VAR VAR VAR VAR: */ VAR (VAR) VAR = VAR(); VAR = VAR(VAR, VAR); VAR = VAR(VAR); VAR = VAR.VAR / 1000000; VAR(VAR, ""VAR VAR VAR: VAR.3\VAR""); VAR(VAR, ""VAR VAR: %VAR.%03ld VAR\VAR"", (VAR)VAR.VAR, VAR); VAR (VAR(&VAR)) VAR(VAR, ""VAR: %VAR VAR\VAR"", VAR(&VAR)); VAR(VAR, ""VAR: %VAR\VAR"", VAR ? ""VAR"" : ""VAR""); VAR (VAR = 0; VAR < VAR; VAR++) { VAR = VAR + VAR; VAR (VAR->VAR & VAR) { VAR(VAR, ""%4luD, %5d %-16s "", VAR->VAR, VAR->VAR, VAR->VAR); } VAR { VAR(VAR, "" %4lu, %5d %-16s "", VAR->VAR, VAR->VAR, VAR->VAR); } VAR(VAR, (VAR VAR)VAR->VAR); VAR(VAR, "" (""); VAR(VAR, (VAR VAR)VAR->VAR); VAR(VAR, "")\VAR""); VAR += VAR->VAR; } VAR += VAR.VAR * 1000; VAR (!VAR) VAR = 1; VAR (VAR && VAR.VAR) VAR(VAR, ""%VAR VAR VAR, %VAR.%03ld VAR/VAR\VAR"", VAR, VAR * 1000 / VAR, (VAR * 1000000 / VAR) % 1000); VAR VAR(VAR, ""%VAR VAR VAR\VAR"", VAR); VAR(&VAR); VAR 0; } /* * VAR VAR VAR VAR, VAR VAR VAR VAR VAR/VAR * VAR VAR VAR: */ VAR VAR VAR(VAR) { VAR VAR VAR; VAR VAR; VAR(VAR) { VAR *VAR = &VAR(VAR, VAR); VAR(VAR, VAR);  VAR(VAR, VAR); } } VAR VAR VAR(VAR VAR *VAR, VAR VAR VAR *VAR, VAR VAR, VAR *VAR) { VAR VAR[2]; VAR (VAR != 2 || *VAR) VAR -VAR; VAR (VAR(VAR, VAR, VAR)) VAR -VAR; VAR(&VAR); VAR (VAR[0]) { VAR '0': VAR (VAR) { VAR = 0; VAR = VAR(); VAR(); } VAR; VAR '1': VAR (!VAR) { VAR(); VAR = VAR(); VAR(); VAR = 1; } VAR; VAR: VAR = -VAR; } VAR(&VAR); VAR VAR; } VAR VAR VAR(VAR VAR *VAR, VAR VAR *VAR) { VAR VAR(VAR, VAR, VAR); } VAR VAR VAR VAR VAR = { .VAR = VAR, .VAR = VAR, .VAR = VAR, .VAR = VAR, .VAR = VAR, }; VAR VAR VAR(VAR) { VAR VAR; VAR(VAR) VAR(&VAR(VAR, VAR)); } VAR VAR VAR VAR(VAR) { VAR VAR *VAR; VAR = VAR(""VAR"", 0644, VAR, &VAR); VAR (!VAR) VAR -VAR; VAR 0; } VAR(VAR);",14,9
openssl_879f7080d7e141f415c79eaa3a8ac4a3dad0348b,crypto/x509v3/pcy_tree.c,"/*
 * If the maximum number of nodes in the policy tree isn't defined, set it to
 * a generous default of 1000 nodes.
 *
 * Defining this to be zero means unlimited policy tree growth which opens the
 * door on CVE-2023-0464.
 */

#ifndef OPENSSL_POLICY_TREE_NODES_MAX
# define OPENSSL_POLICY_TREE_NODES_MAX 1000
#endif

    /* Limit the growth of the tree to mitigate CVE-2023-0464 */
    tree->node_maximum = OPENSSL_POLICY_TREE_NODES_MAX;

    if (level_add_node(level, data, NULL, tree, 1) == NULL) {
                                    X509_POLICY_DATA *data,
                                    X509_POLICY_TREE *tree)
            if (level_add_node(curr, data, node, tree, 0) == NULL)
        if (level_add_node(curr, data, last->anyPolicy, tree, 0) == NULL)
                           const X509_POLICY_CACHE *cache,
                           X509_POLICY_TREE *tree)
        if (!tree_link_matching_nodes(curr, data, tree))
    if (level_add_node(curr, data, node, tree, 1) == NULL) {
        level_add_node(curr, cache->anyPolicy, last->anyPolicy, tree, 0) == NULL)
            node = level_add_node(NULL, extra, anyPolicy->parent, tree, 1);
        if (!tree_link_nodes(curr, cache, tree))","    if (level_add_node(level, data, NULL, tree) == NULL) {
                                    X509_POLICY_DATA *data)
            if (level_add_node(curr, data, node, NULL) == NULL)
        if (level_add_node(curr, data, last->anyPolicy, NULL) == NULL)
                           const X509_POLICY_CACHE *cache)
        if (!tree_link_matching_nodes(curr, data))
    if (level_add_node(curr, data, node, tree) == NULL) {
        level_add_node(curr, cache->anyPolicy, last->anyPolicy, NULL) == NULL)
            node = level_add_node(NULL, extra, anyPolicy->parent, tree);
        if (!tree_link_nodes(curr, cache))",879f7080d7e141f415c79eaa3a8ac4a3dad0348b,CVE-2023-0464,879f7080d7e141f415c79eaa3a8ac4a3dad0348b,https://github.com/openssl/openssl,CPE_GIT_REPOBASED,146,COMPLETED,"/* * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR'VAR VAR, VAR VAR VAR * VAR VAR VAR VAR 1000 VAR. * * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR * VAR VAR VAR-2023-0464. */     VAR->VAR = VAR; VAR (VAR(VAR, VAR, VAR, VAR, 1) == VAR) { VAR *VAR, VAR *VAR) VAR (VAR(VAR, VAR, VAR, VAR, 0) == VAR) VAR (VAR(VAR, VAR, VAR->VAR, VAR, 0) == VAR) VAR VAR *VAR, VAR *VAR) VAR (!VAR(VAR, VAR, VAR)) VAR (VAR(VAR, VAR, VAR, VAR, 1) == VAR) { VAR(VAR, VAR->VAR, VAR->VAR, VAR, 0) == VAR) VAR = VAR(VAR, VAR, VAR->VAR, VAR, 1); VAR (!VAR(VAR, VAR, VAR)) VAR (VAR(VAR, VAR, VAR, VAR) == VAR) { VAR *VAR) VAR (VAR(VAR, VAR, VAR, VAR) == VAR) VAR (VAR(VAR, VAR, VAR->VAR, VAR) == VAR) VAR VAR *VAR) VAR (!VAR(VAR, VAR)) VAR (VAR(VAR, VAR, VAR, VAR) == VAR) { VAR(VAR, VAR->VAR, VAR->VAR, VAR) == VAR) VAR = VAR(VAR, VAR, VAR->VAR, VAR); VAR (!VAR(VAR, VAR))",14,9
parse-server_78b59fb26b1c36e3cdbd42ba9fec025003267f58,src/LiveQuery/ParseLiveQueryServer.js,"  constructor(server: any, config: any = {}, parseServerConfig: any = {}) {
    this.config = config;
    this.cacheController = getCacheController(parseServerConfig);

    config.cacheTimeout = config.cacheTimeout || 5 * 1000; // 5s
      maxAge: config.cacheTimeout,
            this.config.cacheTimeout","  constructor(server: any, config: any = {}) {
    this.cacheController = getCacheController(config);
      maxAge: 60 * 60 * 1000, // 1h
          // Store a resolved promise with the error for 10 minutes
            60 * 10 * 1000",78b59fb26b1c36e3cdbd42ba9fec025003267f58,CVE-2020-15270,78b59fb26b1c36e3cdbd42ba9fec025003267f58,https://github.com/parse-community/parse-server,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR(VAR: VAR, VAR: VAR = {}, VAR: VAR = {}) { VAR.VAR = VAR; VAR.VAR = VAR(VAR); VAR.VAR = VAR.VAR || 5 * 1000;  VAR: VAR.VAR, VAR.VAR.VAR VAR(VAR: VAR, VAR: VAR = {}) { VAR.VAR = VAR(VAR); VAR: 60 * 60 * 1000,   60 * 10 * 1000",14,9
parse-server_78b59fb26b1c36e3cdbd42ba9fec025003267f58,src/Options/Definitions.js,"      ""Number in milliseconds. When clients provide the sessionToken to the LiveQuery server, the LiveQuery server will try to fetch its ParseUser's objectId from parse server and store it in the cache. The value defines the duration of the cache. Check the following Security section and our protocol specification for details, defaults to 5 * 1000 ms (5 seconds)."",","      ""Number in milliseconds. When clients provide the sessionToken to the LiveQuery server, the LiveQuery server will try to fetch its ParseUser's objectId from parse server and store it in the cache. The value defines the duration of the cache. Check the following Security section and our protocol specification for details, defaults to 30 * 24 * 60 * 60 * 1000 ms (~30 days)."",",78b59fb26b1c36e3cdbd42ba9fec025003267f58,CVE-2020-15270,78b59fb26b1c36e3cdbd42ba9fec025003267f58,https://github.com/parse-community/parse-server,NVD_DIRECT_COMMIT,1337,COMPLETED,"""VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR VAR VAR VAR'VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR, VAR VAR 5 * 1000 VAR (5 VAR)."", ""VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR VAR VAR VAR'VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR, VAR VAR 30 * 24 * 60 * 60 * 1000 VAR (~30 VAR)."",",14,9
parse-server_78b59fb26b1c36e3cdbd42ba9fec025003267f58,src/Options/docs.js," * @property {Number} cacheTimeout Number in milliseconds. When clients provide the sessionToken to the LiveQuery server, the LiveQuery server will try to fetch its ParseUser's objectId from parse server and store it in the cache. The value defines the duration of the cache. Check the following Security section and our protocol specification for details, defaults to 5 * 1000 ms (5 seconds)."," * @property {Number} cacheTimeout Number in milliseconds. When clients provide the sessionToken to the LiveQuery server, the LiveQuery server will try to fetch its ParseUser's objectId from parse server and store it in the cache. The value defines the duration of the cache. Check the following Security section and our protocol specification for details, defaults to 30 * 24 * 60 * 60 * 1000 ms (~30 days).",78b59fb26b1c36e3cdbd42ba9fec025003267f58,CVE-2020-15270,78b59fb26b1c36e3cdbd42ba9fec025003267f58,https://github.com/parse-community/parse-server,NVD_DIRECT_COMMIT,1337,COMPLETED,"* @VAR {VAR} VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR VAR VAR VAR'VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR, VAR VAR 5 * 1000 VAR (5 VAR). * @VAR {VAR} VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR VAR VAR VAR'VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR, VAR VAR 30 * 24 * 60 * 60 * 1000 VAR (~30 VAR).",14,9
urllib3_0aa3e24fcd75f1bb59ab159e9f8adb44055b2271,src/urllib3/util/wait.py,"import errno
from functools import partial
import select
import sys
try:
    from time import monotonic
except ImportError:
    from time import time as monotonic

__all__ = [""NoWayToWaitForSocketError"", ""wait_for_read"", ""wait_for_write""]


class NoWayToWaitForSocketError(Exception):
    pass


# How should we wait on sockets?
#
# There are two types of APIs you can use for waiting on sockets: the fancy
# modern stateful APIs like epoll/kqueue, and the older stateless APIs like
# select/poll. The stateful APIs are more efficient when you have a lots of
# sockets to keep track of, because you can set them up once and then use them
# lots of times. But we only ever want to wait on a single socket at a time
# and don't want to keep track of state, so the stateless APIs are actually
# more efficient. So we want to use select() or poll().
#
# Now, how do we choose between select() and poll()? On traditional Unixes,
# select() has a strange calling convention that makes it slow, or fail
# altogether, for high-numbered file descriptors. The point of poll() is to fix
# that, so on Unixes, we prefer poll().
#
# On Windows, there is no poll() (or at least Python doesn't provide a wrapper
# for it), but that's OK, because on Windows, select() doesn't have this
# strange calling convention; plain select() works fine.
#
# So: on Windows we use select(), and everywhere else we use poll(). We also
# fall back to select() in case poll() is somehow broken or missing.

if sys.version_info >= (3, 5):
    # Modern Python, that retries syscalls by default
    def _retry_on_intr(fn, timeout):
        return fn(timeout)
else:
    # Old and broken Pythons.
    def _retry_on_intr(fn, timeout):
        if timeout is None:
            deadline = float(""inf"")
        else:
            deadline = monotonic() + timeout

        while True:
            try:
                return fn(timeout)
            # OSError for 3 <= pyver < 3.5, select.error for pyver <= 2.7
            except (OSError, select.error) as e:
                # 'e.args[0]' incantation works for both OSError and select.error
                if e.args[0] != errno.EINTR:
                    raise
                else:
                    timeout = deadline - monotonic()
                    if timeout < 0:
                        timeout = 0
                    if timeout == float(""inf""):
                        timeout = None
                    continue


def select_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError(""must specify at least one of read=True, write=True"")
    rcheck = []
    wcheck = []
    if read:
        rcheck.append(sock)
    if write:
        wcheck.append(sock)
    # When doing a non-blocking connect, most systems signal success by
    # marking the socket writable. Windows, though, signals success by marked
    # it as ""exceptional"". We paper over the difference by checking the write
    # sockets for both conditions. (The stdlib selectors module does the same
    # thing.)
    fn = partial(select.select, rcheck, wcheck, wcheck)
    rready, wready, xready = _retry_on_intr(fn, timeout)
    return bool(rready or wready or xready)


def poll_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError(""must specify at least one of read=True, write=True"")
    mask = 0
    if read:
        mask |= select.POLLIN
    if write:
        mask |= select.POLLOUT
    poll_obj = select.poll()
    poll_obj.register(sock, mask)

    # For some reason, poll() takes timeout in milliseconds
    def do_poll(t):
        if t is not None:
            t *= 1000
        return poll_obj.poll(t)

    return bool(_retry_on_intr(do_poll, timeout))


def null_wait_for_socket(*args, **kwargs):
    raise NoWayToWaitForSocketError(""no select-equivalent available"")


def _have_working_poll():
    # Apparently some systems have a select.poll that fails as soon as you try
    # to use it, either due to strange configuration or broken monkeypatching
    # from libraries like eventlet/greenlet.
    try:
        poll_obj = select.poll()
        _retry_on_intr(poll_obj.poll, 0)
    except (AttributeError, OSError):
        return False
    else:
        return True


def wait_for_socket(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global wait_for_socket
    if _have_working_poll():
        wait_for_socket = poll_wait_for_socket
    elif hasattr(select, ""select""):
        wait_for_socket = select_wait_for_socket
    else:  # Platform-specific: Appengine.
        wait_for_socket = null_wait_for_socket
    return wait_for_socket(*args, **kwargs)


def wait_for_read(sock, timeout=None):
    """""" Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, read=True, timeout=timeout)


def wait_for_write(sock, timeout=None):
    """""" Waits for writing to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, write=True, timeout=timeout)",,0aa3e24fcd75f1bb59ab159e9f8adb44055b2271,CVE-2019-11236,0aa3e24fcd75f1bb59ab159e9f8adb44055b2271,https://github.com/urllib3/urllib3,NVD_GIT_REPOBASED,98,COMPLETED,"VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR: VAR VAR VAR VAR VAR VAR: VAR VAR VAR VAR VAR VAR VAR = [""VAR"", ""VAR"", ""VAR""] VAR VAR(VAR): VAR                      VAR VAR.VAR >= (3, 5):  VAR VAR(VAR, VAR): VAR VAR(VAR) VAR:  VAR VAR(VAR, VAR): VAR VAR VAR VAR: VAR = VAR(""VAR"") VAR: VAR = VAR() + VAR VAR VAR: VAR: VAR VAR(VAR)  VAR (VAR, VAR.VAR) VAR VAR:  VAR VAR.VAR[0] != VAR.VAR: VAR VAR: VAR = VAR - VAR() VAR VAR < 0: VAR = 0 VAR VAR == VAR(""VAR""): VAR = VAR VAR VAR VAR(VAR, VAR=VAR, VAR=VAR, VAR=VAR): VAR VAR VAR VAR VAR VAR: VAR VAR(""VAR VAR VAR VAR VAR VAR VAR=VAR, VAR=VAR"") VAR = [] VAR = [] VAR VAR: VAR.VAR(VAR) VAR VAR: VAR.VAR(VAR)      VAR = VAR(VAR.VAR, VAR, VAR, VAR) VAR, VAR, VAR = VAR(VAR, VAR) VAR VAR(VAR VAR VAR VAR VAR) VAR VAR(VAR, VAR=VAR, VAR=VAR, VAR=VAR): VAR VAR VAR VAR VAR VAR: VAR VAR(""VAR VAR VAR VAR VAR VAR VAR=VAR, VAR=VAR"") VAR = 0 VAR VAR: VAR |= VAR.VAR VAR VAR: VAR |= VAR.VAR VAR = VAR.VAR() VAR.VAR(VAR, VAR)  VAR VAR(VAR): VAR VAR VAR VAR VAR: VAR *= 1000 VAR VAR.VAR(VAR) VAR VAR(VAR(VAR, VAR)) VAR VAR(*VAR, **VAR): VAR VAR(""VAR VAR-VAR VAR"") VAR VAR():    VAR: VAR = VAR.VAR() VAR(VAR.VAR, 0) VAR (VAR, VAR): VAR VAR VAR: VAR VAR VAR VAR(*VAR, **VAR):     VAR VAR VAR VAR(): VAR = VAR VAR VAR(VAR, ""VAR""): VAR = VAR VAR:  VAR = VAR VAR VAR(*VAR, **VAR) VAR VAR(VAR, VAR=VAR): """""" VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR. """""" VAR VAR(VAR, VAR=VAR, VAR=VAR) VAR VAR(VAR, VAR=VAR): """""" VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR. """""" VAR VAR(VAR, VAR=VAR, VAR=VAR) ",14,9
urllib3_2d4a3fee6de2fa45eb82169361918f759269b4ec,src/urllib3/util/wait.py,"import errno
import select
import sys
from functools import partial

try:
    from time import monotonic
except ImportError:
    from time import time as monotonic

__all__ = [""NoWayToWaitForSocketError"", ""wait_for_read"", ""wait_for_write""]


class NoWayToWaitForSocketError(Exception):
    pass


# How should we wait on sockets?
#
# There are two types of APIs you can use for waiting on sockets: the fancy
# modern stateful APIs like epoll/kqueue, and the older stateless APIs like
# select/poll. The stateful APIs are more efficient when you have a lots of
# sockets to keep track of, because you can set them up once and then use them
# lots of times. But we only ever want to wait on a single socket at a time
# and don't want to keep track of state, so the stateless APIs are actually
# more efficient. So we want to use select() or poll().
#
# Now, how do we choose between select() and poll()? On traditional Unixes,
# select() has a strange calling convention that makes it slow, or fail
# altogether, for high-numbered file descriptors. The point of poll() is to fix
# that, so on Unixes, we prefer poll().
#
# On Windows, there is no poll() (or at least Python doesn't provide a wrapper
# for it), but that's OK, because on Windows, select() doesn't have this
# strange calling convention; plain select() works fine.
#
# So: on Windows we use select(), and everywhere else we use poll(). We also
# fall back to select() in case poll() is somehow broken or missing.

if sys.version_info >= (3, 5):
    # Modern Python, that retries syscalls by default
    def _retry_on_intr(fn, timeout):
        return fn(timeout)


else:
    # Old and broken Pythons.
    def _retry_on_intr(fn, timeout):
        if timeout is None:
            deadline = float(""inf"")
        else:
            deadline = monotonic() + timeout

        while True:
            try:
                return fn(timeout)
            # OSError for 3 <= pyver < 3.5, select.error for pyver <= 2.7
            except (OSError, select.error) as e:
                # 'e.args[0]' incantation works for both OSError and select.error
                if e.args[0] != errno.EINTR:
                    raise
                else:
                    timeout = deadline - monotonic()
                    if timeout < 0:
                        timeout = 0
                    if timeout == float(""inf""):
                        timeout = None
                    continue


def select_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError(""must specify at least one of read=True, write=True"")
    rcheck = []
    wcheck = []
    if read:
        rcheck.append(sock)
    if write:
        wcheck.append(sock)
    # When doing a non-blocking connect, most systems signal success by
    # marking the socket writable. Windows, though, signals success by marked
    # it as ""exceptional"". We paper over the difference by checking the write
    # sockets for both conditions. (The stdlib selectors module does the same
    # thing.)
    fn = partial(select.select, rcheck, wcheck, wcheck)
    rready, wready, xready = _retry_on_intr(fn, timeout)
    return bool(rready or wready or xready)


def poll_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError(""must specify at least one of read=True, write=True"")
    mask = 0
    if read:
        mask |= select.POLLIN
    if write:
        mask |= select.POLLOUT
    poll_obj = select.poll()
    poll_obj.register(sock, mask)

    # For some reason, poll() takes timeout in milliseconds
    def do_poll(t):
        if t is not None:
            t *= 1000
        return poll_obj.poll(t)

    return bool(_retry_on_intr(do_poll, timeout))


def null_wait_for_socket(*args, **kwargs):
    raise NoWayToWaitForSocketError(""no select-equivalent available"")


def _have_working_poll():
    # Apparently some systems have a select.poll that fails as soon as you try
    # to use it, either due to strange configuration or broken monkeypatching
    # from libraries like eventlet/greenlet.
    try:
        poll_obj = select.poll()
        _retry_on_intr(poll_obj.poll, 0)
    except (AttributeError, OSError):
        return False
    else:
        return True


def wait_for_socket(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global wait_for_socket
    if _have_working_poll():
        wait_for_socket = poll_wait_for_socket
    elif hasattr(select, ""select""):
        wait_for_socket = select_wait_for_socket
    else:  # Platform-specific: Appengine.
        wait_for_socket = null_wait_for_socket
    return wait_for_socket(*args, **kwargs)


def wait_for_read(sock, timeout=None):
    """"""Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, read=True, timeout=timeout)


def wait_for_write(sock, timeout=None):
    """"""Waits for writing to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, write=True, timeout=timeout)",,2d4a3fee6de2fa45eb82169361918f759269b4ec,CVE-2021-33503,2d4a3fee6de2fa45eb82169361918f759269b4ec,https://github.com/urllib3/urllib3,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR: VAR VAR VAR VAR VAR VAR: VAR VAR VAR VAR VAR VAR VAR = [""VAR"", ""VAR"", ""VAR""] VAR VAR(VAR): VAR                      VAR VAR.VAR >= (3, 5):  VAR VAR(VAR, VAR): VAR VAR(VAR) VAR:  VAR VAR(VAR, VAR): VAR VAR VAR VAR: VAR = VAR(""VAR"") VAR: VAR = VAR() + VAR VAR VAR: VAR: VAR VAR(VAR)  VAR (VAR, VAR.VAR) VAR VAR:  VAR VAR.VAR[0] != VAR.VAR: VAR VAR: VAR = VAR - VAR() VAR VAR < 0: VAR = 0 VAR VAR == VAR(""VAR""): VAR = VAR VAR VAR VAR(VAR, VAR=VAR, VAR=VAR, VAR=VAR): VAR VAR VAR VAR VAR VAR: VAR VAR(""VAR VAR VAR VAR VAR VAR VAR=VAR, VAR=VAR"") VAR = [] VAR = [] VAR VAR: VAR.VAR(VAR) VAR VAR: VAR.VAR(VAR)      VAR = VAR(VAR.VAR, VAR, VAR, VAR) VAR, VAR, VAR = VAR(VAR, VAR) VAR VAR(VAR VAR VAR VAR VAR) VAR VAR(VAR, VAR=VAR, VAR=VAR, VAR=VAR): VAR VAR VAR VAR VAR VAR: VAR VAR(""VAR VAR VAR VAR VAR VAR VAR=VAR, VAR=VAR"") VAR = 0 VAR VAR: VAR |= VAR.VAR VAR VAR: VAR |= VAR.VAR VAR = VAR.VAR() VAR.VAR(VAR, VAR)  VAR VAR(VAR): VAR VAR VAR VAR VAR: VAR *= 1000 VAR VAR.VAR(VAR) VAR VAR(VAR(VAR, VAR)) VAR VAR(*VAR, **VAR): VAR VAR(""VAR VAR-VAR VAR"") VAR VAR():    VAR: VAR = VAR.VAR() VAR(VAR.VAR, 0) VAR (VAR, VAR): VAR VAR VAR: VAR VAR VAR VAR(*VAR, **VAR):     VAR VAR VAR VAR(): VAR = VAR VAR VAR(VAR, ""VAR""): VAR = VAR VAR:  VAR = VAR VAR VAR(*VAR, **VAR) VAR VAR(VAR, VAR=VAR): """"""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR. """""" VAR VAR(VAR, VAR=VAR, VAR=VAR) VAR VAR(VAR, VAR=VAR): """"""VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR. """""" VAR VAR(VAR, VAR=VAR, VAR=VAR) ",14,9
urllib3_9b76785331243689a9d52cef3db05ef7462cb02d,src/urllib3/util/wait.py,"import errno
from functools import partial
import select
import sys
try:
    from time import monotonic
except ImportError:
    from time import time as monotonic

__all__ = [""NoWayToWaitForSocketError"", ""wait_for_read"", ""wait_for_write""]


class NoWayToWaitForSocketError(Exception):
    pass


# How should we wait on sockets?
#
# There are two types of APIs you can use for waiting on sockets: the fancy
# modern stateful APIs like epoll/kqueue, and the older stateless APIs like
# select/poll. The stateful APIs are more efficient when you have a lots of
# sockets to keep track of, because you can set them up once and then use them
# lots of times. But we only ever want to wait on a single socket at a time
# and don't want to keep track of state, so the stateless APIs are actually
# more efficient. So we want to use select() or poll().
#
# Now, how do we choose between select() and poll()? On traditional Unixes,
# select() has a strange calling convention that makes it slow, or fail
# altogether, for high-numbered file descriptors. The point of poll() is to fix
# that, so on Unixes, we prefer poll().
#
# On Windows, there is no poll() (or at least Python doesn't provide a wrapper
# for it), but that's OK, because on Windows, select() doesn't have this
# strange calling convention; plain select() works fine.
#
# So: on Windows we use select(), and everywhere else we use poll(). We also
# fall back to select() in case poll() is somehow broken or missing.

if sys.version_info >= (3, 5):
    # Modern Python, that retries syscalls by default
    def _retry_on_intr(fn, timeout):
        return fn(timeout)
else:
    # Old and broken Pythons.
    def _retry_on_intr(fn, timeout):
        if timeout is None:
            deadline = float(""inf"")
        else:
            deadline = monotonic() + timeout

        while True:
            try:
                return fn(timeout)
            # OSError for 3 <= pyver < 3.5, select.error for pyver <= 2.7
            except (OSError, select.error) as e:
                # 'e.args[0]' incantation works for both OSError and select.error
                if e.args[0] != errno.EINTR:
                    raise
                else:
                    timeout = deadline - monotonic()
                    if timeout < 0:
                        timeout = 0
                    if timeout == float(""inf""):
                        timeout = None
                    continue


def select_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError(""must specify at least one of read=True, write=True"")
    rcheck = []
    wcheck = []
    if read:
        rcheck.append(sock)
    if write:
        wcheck.append(sock)
    # When doing a non-blocking connect, most systems signal success by
    # marking the socket writable. Windows, though, signals success by marked
    # it as ""exceptional"". We paper over the difference by checking the write
    # sockets for both conditions. (The stdlib selectors module does the same
    # thing.)
    fn = partial(select.select, rcheck, wcheck, wcheck)
    rready, wready, xready = _retry_on_intr(fn, timeout)
    return bool(rready or wready or xready)


def poll_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError(""must specify at least one of read=True, write=True"")
    mask = 0
    if read:
        mask |= select.POLLIN
    if write:
        mask |= select.POLLOUT
    poll_obj = select.poll()
    poll_obj.register(sock, mask)

    # For some reason, poll() takes timeout in milliseconds
    def do_poll(t):
        if t is not None:
            t *= 1000
        return poll_obj.poll(t)

    return bool(_retry_on_intr(do_poll, timeout))


def null_wait_for_socket(*args, **kwargs):
    raise NoWayToWaitForSocketError(""no select-equivalent available"")


def _have_working_poll():
    # Apparently some systems have a select.poll that fails as soon as you try
    # to use it, either due to strange configuration or broken monkeypatching
    # from libraries like eventlet/greenlet.
    try:
        poll_obj = select.poll()
        _retry_on_intr(poll_obj.poll, 0)
    except (AttributeError, OSError):
        return False
    else:
        return True


def wait_for_socket(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global wait_for_socket
    if _have_working_poll():
        wait_for_socket = poll_wait_for_socket
    elif hasattr(select, ""select""):
        wait_for_socket = select_wait_for_socket
    else:  # Platform-specific: Appengine.
        wait_for_socket = null_wait_for_socket
    return wait_for_socket(*args, **kwargs)


def wait_for_read(sock, timeout=None):
    """""" Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, read=True, timeout=timeout)


def wait_for_write(sock, timeout=None):
    """""" Waits for writing to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, write=True, timeout=timeout)",,9b76785331243689a9d52cef3db05ef7462cb02d,CVE-2019-11236,9b76785331243689a9d52cef3db05ef7462cb02d,https://github.com/urllib3/urllib3,NVD_GIT_REPOBASED,70,COMPLETED,"VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR: VAR VAR VAR VAR VAR VAR: VAR VAR VAR VAR VAR VAR VAR = [""VAR"", ""VAR"", ""VAR""] VAR VAR(VAR): VAR                      VAR VAR.VAR >= (3, 5):  VAR VAR(VAR, VAR): VAR VAR(VAR) VAR:  VAR VAR(VAR, VAR): VAR VAR VAR VAR: VAR = VAR(""VAR"") VAR: VAR = VAR() + VAR VAR VAR: VAR: VAR VAR(VAR)  VAR (VAR, VAR.VAR) VAR VAR:  VAR VAR.VAR[0] != VAR.VAR: VAR VAR: VAR = VAR - VAR() VAR VAR < 0: VAR = 0 VAR VAR == VAR(""VAR""): VAR = VAR VAR VAR VAR(VAR, VAR=VAR, VAR=VAR, VAR=VAR): VAR VAR VAR VAR VAR VAR: VAR VAR(""VAR VAR VAR VAR VAR VAR VAR=VAR, VAR=VAR"") VAR = [] VAR = [] VAR VAR: VAR.VAR(VAR) VAR VAR: VAR.VAR(VAR)      VAR = VAR(VAR.VAR, VAR, VAR, VAR) VAR, VAR, VAR = VAR(VAR, VAR) VAR VAR(VAR VAR VAR VAR VAR) VAR VAR(VAR, VAR=VAR, VAR=VAR, VAR=VAR): VAR VAR VAR VAR VAR VAR: VAR VAR(""VAR VAR VAR VAR VAR VAR VAR=VAR, VAR=VAR"") VAR = 0 VAR VAR: VAR |= VAR.VAR VAR VAR: VAR |= VAR.VAR VAR = VAR.VAR() VAR.VAR(VAR, VAR)  VAR VAR(VAR): VAR VAR VAR VAR VAR: VAR *= 1000 VAR VAR.VAR(VAR) VAR VAR(VAR(VAR, VAR)) VAR VAR(*VAR, **VAR): VAR VAR(""VAR VAR-VAR VAR"") VAR VAR():    VAR: VAR = VAR.VAR() VAR(VAR.VAR, 0) VAR (VAR, VAR): VAR VAR VAR: VAR VAR VAR VAR(*VAR, **VAR):     VAR VAR VAR VAR(): VAR = VAR VAR VAR(VAR, ""VAR""): VAR = VAR VAR:  VAR = VAR VAR VAR(*VAR, **VAR) VAR VAR(VAR, VAR=VAR): """""" VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR. """""" VAR VAR(VAR, VAR=VAR, VAR=VAR) VAR VAR(VAR, VAR=VAR): """""" VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR. """""" VAR VAR(VAR, VAR=VAR, VAR=VAR) ",14,9
urllib3_a74c9cfbaed9f811e7563cfc3dce894928e0221a,src/urllib3/util/wait.py,"import errno
from functools import partial
import select
import sys
try:
    from time import monotonic
except ImportError:
    from time import time as monotonic

__all__ = [""NoWayToWaitForSocketError"", ""wait_for_read"", ""wait_for_write""]


class NoWayToWaitForSocketError(Exception):
    pass


# How should we wait on sockets?
#
# There are two types of APIs you can use for waiting on sockets: the fancy
# modern stateful APIs like epoll/kqueue, and the older stateless APIs like
# select/poll. The stateful APIs are more efficient when you have a lots of
# sockets to keep track of, because you can set them up once and then use them
# lots of times. But we only ever want to wait on a single socket at a time
# and don't want to keep track of state, so the stateless APIs are actually
# more efficient. So we want to use select() or poll().
#
# Now, how do we choose between select() and poll()? On traditional Unixes,
# select() has a strange calling convention that makes it slow, or fail
# altogether, for high-numbered file descriptors. The point of poll() is to fix
# that, so on Unixes, we prefer poll().
#
# On Windows, there is no poll() (or at least Python doesn't provide a wrapper
# for it), but that's OK, because on Windows, select() doesn't have this
# strange calling convention; plain select() works fine.
#
# So: on Windows we use select(), and everywhere else we use poll(). We also
# fall back to select() in case poll() is somehow broken or missing.

if sys.version_info >= (3, 5):
    # Modern Python, that retries syscalls by default
    def _retry_on_intr(fn, timeout):
        return fn(timeout)
else:
    # Old and broken Pythons.
    def _retry_on_intr(fn, timeout):
        if timeout is None:
            deadline = float(""inf"")
        else:
            deadline = monotonic() + timeout

        while True:
            try:
                return fn(timeout)
            # OSError for 3 <= pyver < 3.5, select.error for pyver <= 2.7
            except (OSError, select.error) as e:
                # 'e.args[0]' incantation works for both OSError and select.error
                if e.args[0] != errno.EINTR:
                    raise
                else:
                    timeout = deadline - monotonic()
                    if timeout < 0:
                        timeout = 0
                    if timeout == float(""inf""):
                        timeout = None
                    continue


def select_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError(""must specify at least one of read=True, write=True"")
    rcheck = []
    wcheck = []
    if read:
        rcheck.append(sock)
    if write:
        wcheck.append(sock)
    # When doing a non-blocking connect, most systems signal success by
    # marking the socket writable. Windows, though, signals success by marked
    # it as ""exceptional"". We paper over the difference by checking the write
    # sockets for both conditions. (The stdlib selectors module does the same
    # thing.)
    fn = partial(select.select, rcheck, wcheck, wcheck)
    rready, wready, xready = _retry_on_intr(fn, timeout)
    return bool(rready or wready or xready)


def poll_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError(""must specify at least one of read=True, write=True"")
    mask = 0
    if read:
        mask |= select.POLLIN
    if write:
        mask |= select.POLLOUT
    poll_obj = select.poll()
    poll_obj.register(sock, mask)

    # For some reason, poll() takes timeout in milliseconds
    def do_poll(t):
        if t is not None:
            t *= 1000
        return poll_obj.poll(t)

    return bool(_retry_on_intr(do_poll, timeout))


def null_wait_for_socket(*args, **kwargs):
    raise NoWayToWaitForSocketError(""no select-equivalent available"")


def _have_working_poll():
    # Apparently some systems have a select.poll that fails as soon as you try
    # to use it, either due to strange configuration or broken monkeypatching
    # from libraries like eventlet/greenlet.
    try:
        poll_obj = select.poll()
        _retry_on_intr(poll_obj.poll, 0)
    except (AttributeError, OSError):
        return False
    else:
        return True


def wait_for_socket(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global wait_for_socket
    if _have_working_poll():
        wait_for_socket = poll_wait_for_socket
    elif hasattr(select, ""select""):
        wait_for_socket = select_wait_for_socket
    else:  # Platform-specific: Appengine.
        wait_for_socket = null_wait_for_socket
    return wait_for_socket(*args, **kwargs)


def wait_for_read(sock, timeout=None):
    """""" Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, read=True, timeout=timeout)


def wait_for_write(sock, timeout=None):
    """""" Waits for writing to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """"""
    return wait_for_socket(sock, write=True, timeout=timeout)",,a74c9cfbaed9f811e7563cfc3dce894928e0221a,CVE-2020-7212,a74c9cfbaed9f811e7563cfc3dce894928e0221a,https://github.com/urllib3/urllib3,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR: VAR VAR VAR VAR VAR VAR: VAR VAR VAR VAR VAR VAR VAR = [""VAR"", ""VAR"", ""VAR""] VAR VAR(VAR): VAR                      VAR VAR.VAR >= (3, 5):  VAR VAR(VAR, VAR): VAR VAR(VAR) VAR:  VAR VAR(VAR, VAR): VAR VAR VAR VAR: VAR = VAR(""VAR"") VAR: VAR = VAR() + VAR VAR VAR: VAR: VAR VAR(VAR)  VAR (VAR, VAR.VAR) VAR VAR:  VAR VAR.VAR[0] != VAR.VAR: VAR VAR: VAR = VAR - VAR() VAR VAR < 0: VAR = 0 VAR VAR == VAR(""VAR""): VAR = VAR VAR VAR VAR(VAR, VAR=VAR, VAR=VAR, VAR=VAR): VAR VAR VAR VAR VAR VAR: VAR VAR(""VAR VAR VAR VAR VAR VAR VAR=VAR, VAR=VAR"") VAR = [] VAR = [] VAR VAR: VAR.VAR(VAR) VAR VAR: VAR.VAR(VAR)      VAR = VAR(VAR.VAR, VAR, VAR, VAR) VAR, VAR, VAR = VAR(VAR, VAR) VAR VAR(VAR VAR VAR VAR VAR) VAR VAR(VAR, VAR=VAR, VAR=VAR, VAR=VAR): VAR VAR VAR VAR VAR VAR: VAR VAR(""VAR VAR VAR VAR VAR VAR VAR=VAR, VAR=VAR"") VAR = 0 VAR VAR: VAR |= VAR.VAR VAR VAR: VAR |= VAR.VAR VAR = VAR.VAR() VAR.VAR(VAR, VAR)  VAR VAR(VAR): VAR VAR VAR VAR VAR: VAR *= 1000 VAR VAR.VAR(VAR) VAR VAR(VAR(VAR, VAR)) VAR VAR(*VAR, **VAR): VAR VAR(""VAR VAR-VAR VAR"") VAR VAR():    VAR: VAR = VAR.VAR() VAR(VAR.VAR, 0) VAR (VAR, VAR): VAR VAR VAR: VAR VAR VAR VAR(*VAR, **VAR):     VAR VAR VAR VAR(): VAR = VAR VAR VAR(VAR, ""VAR""): VAR = VAR VAR:  VAR = VAR VAR VAR(*VAR, **VAR) VAR VAR(VAR, VAR=VAR): """""" VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR. """""" VAR VAR(VAR, VAR=VAR, VAR=VAR) VAR VAR(VAR, VAR=VAR): """""" VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. VAR VAR VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR. """""" VAR VAR(VAR, VAR=VAR, VAR=VAR) ",14,9
vim_fe6fb267e6ee5c5da2f41889e4e0e0ac5bf4b89d,src/eval.c,"    static	int recurse = 0;
    // Limit recursion to 1000 levels.  At least at 10000 we run out of stack
    // and crash.
    if (recurse == 1000)
    {
	semsg(_(e_expression_too_recursive_str), *arg);
	return FAIL;
    }
    ++recurse;


    --recurse;",,fe6fb267e6ee5c5da2f41889e4e0e0ac5bf4b89d,CVE-2022-0351,fe6fb267e6ee5c5da2f41889e4e0e0ac5bf4b89d,https://github.com/vim/vim,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR VAR VAR = 0;   VAR (VAR == 1000) { VAR(VAR(VAR), *VAR); VAR VAR; } ++VAR; --VAR; ",14,9
xwiki-platform_bff0203e739b6e3eb90af5736f04278c73c2a8bb,xwiki-platform-core/xwiki-platform-diff/xwiki-platform-diff-xml/src/main/java/org/xwiki/diff/xml/internal/DefaultDataURIConverter.java,"import org.xwiki.cache.eviction.EntryEvictionConfiguration;
import org.xwiki.component.phase.Disposable;
import org.xwiki.diff.xml.XMLDiffDataURIConverterConfiguration;
import org.xwiki.url.URLSecurityManager;
import org.xwiki.user.CurrentUserReference;
import org.xwiki.user.UserReferenceSerializer;
import com.xpn.xwiki.XWiki;
import com.xpn.xwiki.XWikiException;
 * Default Implementation of {@link DataURIConverter} that uses an HTTP client to embed images.
 *
public class DefaultDataURIConverter implements Initializable, Disposable, DataURIConverter
    @Inject
    private URLSecurityManager urlSecurityManager;

    @Inject
    private UserReferenceSerializer<String> userReferenceSerializer;

    @Inject
    private ImageDownloader imageDownloader;

    @Inject
    private XMLDiffDataURIConverterConfiguration configuration;

    private Cache<DiffException> failureCache;

        if (!this.configuration.isEnabled()) {
            return;
        }

        cacheConfig.put(EntryEvictionConfiguration.CONFIGURATIONID, lru);

        CacheConfiguration failureCacheConfiguration = new CacheConfiguration();
        failureCacheConfiguration.setConfigurationId(""diff.html.dataURIFailureCache"");
        LRUEvictionConfiguration failureLRU = new LRUEvictionConfiguration();
        failureLRU.setMaxEntries(1000);
        // Cache failures for an hour. This is to avoid hammering the server with requests for images that don't
        // exist or are inaccessible or too large.
        failureLRU.setLifespan(3600);
        failureCacheConfiguration.put(EntryEvictionConfiguration.CONFIGURATIONID, failureLRU);

            this.failureCache = this.cacheManager.createNewCache(failureCacheConfiguration);
            // Dispose the cache if it has been created.
            if (this.cache != null) {
                this.cache.dispose();
            }
    public void dispose()
        if (this.cache != null) {
            this.cache.dispose();
        }
        if (this.failureCache != null) {
            this.failureCache.dispose();
    }
    /**
     * Convert the given URL to an absolute URL using the request URL from the given context.
     *
     * @param url the URL to convert
     * @param xcontext the XWiki context
     * @return the absolute URL
     * @throws DiffException if the URL cannot be converted due to being malformed
     */
    protected URL getAbsoluteURL(String url, XWikiContext xcontext) throws DiffException
    {
        URL absoluteURL;
        try {
            if (xcontext.getRequest() != null) {
                URL requestURL = XWiki.getRequestURL(xcontext.getRequest());
                absoluteURL = new URL(requestURL, url);
            } else {
                absoluteURL = new URL(url);
        } catch (MalformedURLException | XWikiException e) {
            throw new DiffException(String.format(""Failed to resolve [%s] to an absolute URL."", url), e);
        return absoluteURL;
    /**
     * Get a data URI for the given content and content type.
     *
     * @param contentType the content type
     * @param content the content
     * @return the data URI
     */
    protected static String getDataURI(String contentType, byte[] content)
        return String.format(""data:%s;base64,%s"", contentType, Base64.getEncoder().encodeToString(content));
    /**
     * Compute a cache key based on the current user and the URL.
     *
     * @param url the url
     * @return the cache key
     */
    private String getCacheKey(URL url)
        String userPart = this.userReferenceSerializer.serialize(CurrentUserReference.INSTANCE);
        // Prepend the length of the user part to avoid any kind of confusion between user and URL.
        return String.format(""%d:%s:%s"", userPart.length(), userPart, url.toString());
    @Override
    public String convert(String url) throws DiffException
        if (url.startsWith(""data:"") || !this.configuration.isEnabled()) {
            // Already data URI.
            return url;
        }

        // Convert URL to absolute URL to avoid issues with relative URLs that might reference different images
        // in different subwikis.
        URL absoluteURL = getAbsoluteURL(url, this.xcontextProvider.get());
        String cacheKey = getCacheKey(absoluteURL);
        try {
            String dataURI = this.cache.get(cacheKey);

            if (dataURI == null) {
                DiffException failure = this.failureCache.get(cacheKey);

                if (failure != null) {
                    throw failure;
                }

                dataURI = convert(absoluteURL);
                this.cache.set(cacheKey, dataURI);
            }

            return dataURI;
        } catch (IOException | URISyntaxException e) {
            DiffException diffException = new DiffException(""Failed to convert ["" + url + ""] to data URI."", e);
            this.failureCache.set(cacheKey, diffException);
            throw diffException;
    }
    private String convert(URL url) throws IOException, URISyntaxException
    {
        if (!this.urlSecurityManager.isDomainTrusted(url)) {
            throw new IOException(String.format(""The URL [%s] is not trusted."", url));

        ImageDownloader.DownloadResult downloadResult = this.imageDownloader.download(url.toURI());

        return getDataURI(downloadResult.getContentType(), downloadResult.getData());","import java.net.URI;
import org.apache.commons.io.IOUtils;
import org.apache.commons.lang3.StringUtils;
import org.apache.http.HttpEntity;
import org.apache.http.HttpStatus;
import org.apache.http.StatusLine;
import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClientBuilder;
import com.xpn.xwiki.web.XWikiRequest;
 * Default implementation of {@link DataURIConverter}.
 *
public class DefaultDataURIConverter implements DataURIConverter, Initializable
    private static final String HEADER_COOKIE = ""Cookie"";

        cacheConfig.put(LRUEvictionConfiguration.CONFIGURATIONID, lru);
    public String convert(String url) throws DiffException
        if (url.startsWith(""data:"")) {
            // Already data URI.
            return url;
        String cachedDataURI = this.cache.get(url);
        if (cachedDataURI == null) {
            try {
                cachedDataURI = convert(getAbsoluteURI(url));
                this.cache.set(url, cachedDataURI);
            } catch (IOException | URISyntaxException e) {
                throw new DiffException(""Failed to convert ["" + url + ""] to data URI."", e);

        return cachedDataURI;
    private URL getAbsoluteURI(String relativeURL) throws MalformedURLException
        XWikiContext xcontext = this.xcontextProvider.get();
        URL baseURL = xcontext.getURLFactory().getServerURL(xcontext);
        return new URL(baseURL, relativeURL);
    private String convert(URL url) throws IOException, URISyntaxException
        HttpEntity entity = fetch(url.toURI());
        // Remove the content type parameters, such as the charset, so they don't influence the diff.
        String contentType = StringUtils.substringBefore(entity.getContentType().getValue(), "";"");
        byte[] content = IOUtils.toByteArray(entity.getContent());
        return String.format(""data:%s;base64,%s"", contentType, Base64.getEncoder().encodeToString(content));
    private HttpEntity fetch(URI uri) throws IOException
        HttpClientBuilder httpClientBuilder = HttpClientBuilder.create();
        httpClientBuilder.useSystemProperties();
        httpClientBuilder.setUserAgent(""XWikiHTMLDiff"");
        CloseableHttpClient httpClient = httpClientBuilder.build();
        HttpGet getMethod = new HttpGet(uri);
        XWikiRequest request = this.xcontextProvider.get().getRequest();
        if (request != null) {
            // Copy the cookies from the current request.
            getMethod.setHeader(HEADER_COOKIE, request.getHeader(HEADER_COOKIE));
        CloseableHttpResponse response = httpClient.execute(getMethod);
        StatusLine statusLine = response.getStatusLine();
        if (statusLine.getStatusCode() == HttpStatus.SC_OK) {
            return response.getEntity();
        } else {
            throw new IOException(statusLine.getStatusCode() + "" "" + statusLine.getReasonPhrase());",bff0203e739b6e3eb90af5736f04278c73c2a8bb,CVE-2023-48240,bff0203e739b6e3eb90af5736f04278c73c2a8bb,https://github.com/xwiki/xwiki-platform,NVD_DIRECT_COMMIT,1337,COMPLETED,"VAR VAR.VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR; * VAR VAR VAR {@VAR VAR} VAR VAR VAR VAR VAR VAR VAR VAR. * VAR VAR VAR VAR VAR, VAR, VAR @VAR VAR VAR VAR; @VAR VAR VAR<VAR> VAR; @VAR VAR VAR VAR; @VAR VAR VAR VAR; VAR VAR<VAR> VAR; VAR (!VAR.VAR.VAR()) { VAR; } VAR.VAR(VAR.VAR, VAR); VAR VAR = VAR VAR(); VAR.VAR(""VAR.VAR.VAR""); VAR VAR = VAR VAR(); VAR.VAR(1000);   VAR.VAR(3600); VAR.VAR(VAR.VAR, VAR); VAR.VAR = VAR.VAR.VAR(VAR);  VAR (VAR.VAR != VAR) { VAR.VAR.VAR(); } VAR VAR VAR() VAR (VAR.VAR != VAR) { VAR.VAR.VAR(); } VAR (VAR.VAR != VAR) { VAR.VAR.VAR(); } /** * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. * * @VAR VAR VAR VAR VAR VAR * @VAR VAR VAR VAR VAR * @VAR VAR VAR VAR * @VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR */ VAR VAR VAR(VAR VAR, VAR VAR) VAR VAR { VAR VAR; VAR { VAR (VAR.VAR() != VAR) { VAR VAR = VAR.VAR(VAR.VAR()); VAR = VAR VAR(VAR, VAR); } VAR { VAR = VAR VAR(VAR); } VAR (VAR | VAR VAR) { VAR VAR VAR(VAR.VAR(""VAR VAR VAR [%VAR] VAR VAR VAR VAR."", VAR), VAR); VAR VAR; /** * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. * * @VAR VAR VAR VAR VAR * @VAR VAR VAR VAR * @VAR VAR VAR VAR */ VAR VAR VAR VAR(VAR VAR, VAR[] VAR) VAR VAR.VAR(""VAR:%VAR;VAR,%VAR"", VAR, VAR.VAR().VAR(VAR)); /** * VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR VAR. * * @VAR VAR VAR VAR * @VAR VAR VAR VAR */ VAR VAR VAR(VAR VAR) VAR VAR = VAR.VAR.VAR(VAR.VAR);  VAR VAR.VAR(""%VAR:%VAR:%VAR"", VAR.VAR(), VAR, VAR.VAR()); @VAR VAR VAR VAR(VAR VAR) VAR VAR VAR (VAR.VAR(""VAR:"") || !VAR.VAR.VAR()) {  VAR VAR; }   VAR VAR = VAR(VAR, VAR.VAR.VAR()); VAR VAR = VAR(VAR); VAR { VAR VAR = VAR.VAR.VAR(VAR); VAR (VAR == VAR) { VAR VAR = VAR.VAR.VAR(VAR); VAR (VAR != VAR) { VAR VAR; } VAR = VAR(VAR); VAR.VAR.VAR(VAR, VAR); } VAR VAR; } VAR (VAR | VAR VAR) { VAR VAR = VAR VAR(""VAR VAR VAR ["" + VAR + ""] VAR VAR VAR."", VAR); VAR.VAR.VAR(VAR, VAR); VAR VAR; } VAR VAR VAR(VAR VAR) VAR VAR, VAR { VAR (!VAR.VAR.VAR(VAR)) { VAR VAR VAR(VAR.VAR(""VAR VAR [%VAR] VAR VAR VAR."", VAR)); VAR.VAR VAR = VAR.VAR.VAR(VAR.VAR()); VAR VAR(VAR.VAR(), VAR.VAR()); VAR VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR.VAR.VAR; VAR VAR.VAR.VAR.VAR.VAR; * VAR VAR VAR {@VAR VAR}. * VAR VAR VAR VAR VAR, VAR VAR VAR VAR VAR VAR = ""VAR""; VAR.VAR(VAR.VAR, VAR); VAR VAR VAR(VAR VAR) VAR VAR VAR (VAR.VAR(""VAR:"")) {  VAR VAR; VAR VAR = VAR.VAR.VAR(VAR); VAR (VAR == VAR) { VAR { VAR = VAR(VAR(VAR)); VAR.VAR.VAR(VAR, VAR); } VAR (VAR | VAR VAR) { VAR VAR VAR(""VAR VAR VAR ["" + VAR + ""] VAR VAR VAR."", VAR); VAR VAR; VAR VAR VAR(VAR VAR) VAR VAR VAR VAR = VAR.VAR.VAR(); VAR VAR = VAR.VAR().VAR(VAR); VAR VAR VAR(VAR, VAR); VAR VAR VAR(VAR VAR) VAR VAR, VAR VAR VAR = VAR(VAR.VAR());  VAR VAR = VAR.VAR(VAR.VAR().VAR(), "";""); VAR[] VAR = VAR.VAR(VAR.VAR()); VAR VAR.VAR(""VAR:%VAR;VAR,%VAR"", VAR, VAR.VAR().VAR(VAR)); VAR VAR VAR(VAR VAR) VAR VAR VAR VAR = VAR.VAR(); VAR.VAR(); VAR.VAR(""VAR""); VAR VAR = VAR.VAR(); VAR VAR = VAR VAR(VAR); VAR VAR = VAR.VAR.VAR().VAR(); VAR (VAR != VAR) {  VAR.VAR(VAR, VAR.VAR(VAR)); VAR VAR = VAR.VAR(VAR); VAR VAR = VAR.VAR(); VAR (VAR.VAR() == VAR.VAR) { VAR VAR.VAR(); } VAR { VAR VAR VAR(VAR.VAR() + "" "" + VAR.VAR());",14,9
